{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "paperback-source",
   "metadata": {},
   "source": [
    "# 30. 딥네트워크, 서로 뭐가 다른 거죠?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-taste",
   "metadata": {},
   "source": [
    "## 1. 딥네트워크, 서로 뭐가 다른 거죠?\n",
    "\n",
    "요즘 핫한 __'딥러닝'__은 이름에서도 볼 수 있듯, _깊게_ 구성된 신경망이다. 이렇게 신경망을 깊게 쌓는 데에도 많은 연구와 기법이 필요하다.\n",
    "\n",
    "연구자들이 더 좋은 성능을 내는 딥네트워크를 만들기 위해서 다양한 방법을 적용하면서, 많은 종류의 네트워크들이 탄생했다. 그중 몇 가지 사전학습된 네트워크(Pre-trained network)들은 Tensorflow나 PyTorch 등 프레임워크 차원에서 지원하고 있다. 아래 사진에 보이는 모델만 20가지가 넘는다. 많아서 복잡해 보일 수 있지만, 그만큼 활발한 분야고 가져다 쓸 수 있는 네트워크들이 많다는 점을 확인할 수 있습다.\n",
    "\n",
    "이런 네트워크들은 딥러닝을 하다 보면 자주 접하게 된다. 오늘은 딥러닝 네트워크들이 어떻게 생겨났고 어떤 시도로 만들어진 것인지 알아보겠다.\n",
    "\n",
    "<img src=\"./image/pretrain.png\" alt=\"pretrained\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-manner",
   "metadata": {},
   "source": [
    "### 목표\n",
    "---\n",
    "1. 딥러닝 네트워크들을 동물원 구경하듯 둘러본다. (Model Zoo)\n",
    "2. 딥러닝이 어떻게 시작된 것인지 알아본다.\n",
    "3. ResNet과 VGG는 무엇을 시도했는지 알아본다.\n",
    "\n",
    "[TensorFlow-Slim image classification model library](https://github.com/tensorflow/models/tree/master/research/slim) 에서 Pre-trained Model을 한번 둘러보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-sandwich",
   "metadata": {},
   "source": [
    "## 2. ImageNet Challenge\n",
    "\n",
    "<img src=\"./image/imagenet.jpg\" alt=\"ImageNet Challenge\" />\n",
    "\n",
    "이미지넷(ImageNet)은 비전(vision) 관련 딥러닝을 하다 보면 필연적으로 마주치게 되는 이름이다. 이미지넷은 2010년 ILSVRC2010를 시작으로 대량의 이미지 데이터를 포함하는 데이터셋이다. ILSVRC2010의 소개에 따르면, 이미지넷은 1만 개가 넘는 카테고리에 대해 100만 장 규모의 이미지를 가지고 있다고 한다.<br>\n",
    "이미지 데이터를 수집할 당시 10억 장의 이미지에서 167개국에서 모인 5만 명의 작업자가 라벨링에 참여했다고 한요. 재밌는 건 고양이 사진이 무려 6만 2천 장 가량 나왔다.\n",
    "\n",
    "이 데이터셋은 모으는 데서 멈추지 않고 챌린지를 위한 데이터셋으로 제공됐다. 그중 가장 유명한 테스크가 바로 우리가 많이 봐왔던 이미지 분류기(Image Classification Task)이다.\n",
    "\n",
    "2010년 이 테스크에는 11개의 팀이 참가했다. 그 중 NEC-UIUC팀이 __Descriptor Coding__과 __SVM__을 결합한 방식을 사용해 오류율 28%로 1등을 달성한다. 이듬해인 2011년에는 Xerox Research Centre Europe이 오류율 26%로 1등을 달성하게 된다.\n",
    "\n",
    "그리고 바로 그 다음 해에는 Geoffrey Hinton 교수님이 이끄는 토론토 대학의 SuperVision팀이 오류율 16%로 1등을 달성한다. 이전 1등들이 민망할 만큼 놀라운 차이를 보여주는데, 과연 이 팀은 어떤 방법을 썼을까?\n",
    "\n",
    "* [bskyvision님의 이미지 분류 모델 평가에 사용되는 top-5 error와 top-1 error](https://bskyvision.com/422)\n",
    "* [Stackoverflow Evaluation & Calculate Top-N Accuracy: Top 1 and Top 5](https://stackoverflow.com/questions/37668902/evaluation-calculate-top-n-accuracy-top-1-and-top-5)\n",
    "\n",
    "__Top-1 Accuracy와 Top-5 Accuracy__\n",
    "* Top-1 accuracy는 예측값이 일반적으로 생각하는 정답을 맞춘 정확도이다.\n",
    "* Top-5 accruacy는 예측한 확률이 높은 순서로 5개 내에서 정답이 있을 경우 맞춘 것으로 간주한 정확도이다.\n",
    "\n",
    "ImageNet이 만들어진 이야기는 페이페이 교수님의 TED에서 들어볼 수 있다. 길지만 한국어 자막도 있으니 봐두면 좋을 것 같다.\n",
    "\n",
    "[![TED feifei li](http://img.youtube.com/vi/40riCqvRoMs/0.jpg)](https://youtu.be/40riCqvRoMs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-fraction",
   "metadata": {},
   "source": [
    "## 3. 딥네트워크의 시작\n",
    "\n",
    "<img src=\"./image/alexnet.png\" alt=\"alexnet\" />\n",
    "\n",
    "SuperVision 팀이 전년도 대비 10%의 오류율을 낮추면서 1등을 할 수 있었던 비법은 네트워크를 깊게 쌓았기 때문이다. __AlexNet__이라는 이름은 논문의 1저자인 Alex Khrizevsky의 이름에서 따왔다고 한다.\n",
    "\n",
    "위의 그림에서 잘려진 부분은 GPU의 병렬 연산을 표현하기 위한 것으로 보면 된다. 레이어가 많이 생긴 정도이지 MNIST에 CNN과 클래스가 많이 있는 네트워크로 볼 수 있다. 여기에 더해진 것들은 ReLU 활성화 함수와 드롭아웃(Dropout) 그리고 오버래핑 풀링(Overlapping pooling) 등이 적용되었다. 방금 언급한 기법에 대한 내용은 아래 사이트와 논문에서 더 자세히 확인할 수 있다.\n",
    "\n",
    "* [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "* [AlexNet의 구조](https://bskyvision.com/421)\n",
    "\n",
    "__AlexNet에서 가장 깊은 레이어__\n",
    "\n",
    "* AlextNet에서 가장 깊게 실험된 조건은 7개의 CNN과 2개의 FCN을 사용했다.\n",
    "* 논문의 Table 2.에서 해당 조건을 확인할 수 있는데 7개의 CNN 모델에는 pre-training을 적용했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-antibody",
   "metadata": {},
   "source": [
    "## 4. CNN을 잘쓰자\n",
    "\n",
    "<img src=\"./image/cnn.png\" alt=\"CNN\" />\n",
    "\n",
    "다음으로 소개해드릴 네트워크 모델은 __VGG__이다. VGG는 AlexNet 같이 이미지넷 챌린지에서 공개된 모델이다. 이 모델은 2014년 이미지넷 챌린지 준우승을 거두었다.\n",
    "\n",
    "우승모델이 아닌 준우승 모델을 소개하는 이유는 간결한 구조로 많은 활용이 이루어졌기 때문이다. 이전에 우승한 네트워크들이 10개가 안 되는 CNN층을 가진 반면, VGG16과 VGG19라는 이름 뒤의 숫자로 볼 수 있듯이, VGG는 16개, 19개의 층으로 이뤄진다.\n",
    "\n",
    "그럼 VGG에서는 어떤 방식으로 이런 레이어를 쌓았을까? CNN을 만들 때 우리는 커널 크기(kernel size)를 조절한다. VGG에서는 3x3 커널을 사용해서 더 많은 레이어를 쌓고 이미지의 비선형적 특성을 더 잘 잡아낼 수 있게 만들었다.\n",
    "\n",
    "레이어가 많아지면 걱정되는 것이 연산이 많아지는 점인데, 5x5와 7x7 레이어를 3x3 레이어와 비교해보자. 입/출력 채널의 수를 각각 C라고 했을 때 커널 크기가 7x7인 CNN 레이어를 한 개 쌓으면 (7x7)×C 2 번 연산을 해야 한다. 그리고 커널 크기가 5x5인 CNN은 (5x5)×C 2의 연산을 필요로 한다. 그렇다면 3x3은 어떻게 될까? 레이어가 N개 일 때, Nx(3x3)xC 2개의 연산으로 레이어가 3개더라도 5x5인 레이어 1개와 비슷한 연산 수를 유지하게 된다.\n",
    "\n",
    "아래 링크를 통해 자세한 설명과 논문을 확인할 수 있다.\n",
    "\n",
    "* [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "* [bskyvision님의 [CNN 알고리즘들] VGGNet의 구조 (VGG16)](https://bskyvision.com/504)\n",
    "* [라온피플 머신러닝 아카데미 GoogleNet](https://m.blog.naver.com/laonple/220686328027)\n",
    "* [ratsgo님의 CNN 주요 모델들](https://ratsgo.github.io/deep%20learning/2017/10/09/CNNs/)\n",
    "\n",
    "__GoogleNet과 VGG 비교__\n",
    "* 우선 레이어의 개수에서 차이가 난다. GoogleNet은 22개의 Layer 그리고 VGG16은 16개의 레이어를 갖는다.\n",
    "* VGG는 간결하게 레이어를 쌓아나간 반면 GoogleNet에서는 Inception block을 도입해서 다양한 기법을 사용해 볼 수 있도록 사고를 확장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-progress",
   "metadata": {},
   "source": [
    "## 5. 멀리 있으면 잘 안 들려요\n",
    "\n",
    "<img src=\"./image/vanishing.png\" alt=\"vanishing\" />\n",
    "\n",
    "* [cbjsena님의 경사소실 문제 해결](http://cbjsena.blogspot.com/2018/12/blog-post_25.html)\n",
    "\n",
    "앞의 내용을 보면 네트워크를 깊게 쌓기가 생각보다 어려운 문제라는 것을 엿볼 수 있는데, 그 원인이 무엇일까?\n",
    "\n",
    "멀리서 말하는 사람의 목소리가 잘 안 들리듯, 모델이 깊어질수록 모델의 학습을 위한 Gradient가 사라지는 현상이 발생한다. 조금 더 자세히 설명하자면, 우리의 네트워크는 Gradient descent를 통해서 기울기를 학습하는데 깊은 레이어에는 데이터에 따른 차이가 충분하게 반영되지 못한다. 이렇게 Gradient가 매우 작아져서 레이어를 학습시키기 위해 충분한 값을 표현하지 못할 경우를 Vanshing 했다고 하여 __기울기 소실(경사소실, Gradient vanishing)__이라고 한다.\n",
    "\n",
    "Gradient vanishing 또는 exploding의 문제가 발생하는 원인은, 레이어가 깊어지면서 Gradient가 매우 커지거나 작아지기 때문이다. 레이어의 가중치가 반복되서 곱해지면, 1보다 작을 때에는 0에 너무 가까워져 버리고, 1보다 클 때에는 그 값이 기하급수적으로 커지게 된다. 이 내용은 Andrew Ng 교수님의 Vanishing/Exploding Gradients 영상을 보도록 하자.\n",
    "\n",
    "[![Vanishing/Exploding Gradients](http://img.youtube.com/vi/qhXZsFVxGKo/0.jpg)](https://youtu.be/qhXZsFVxGKo) \n",
    "\n",
    "이렇게 Gradient가 사라져서 깊은 레이어가 적절히 학습되지 않는 문제를 해결하기 위해서 ResNet은 새로운 방법을 도입했다.\n",
    "\n",
    "__Gradient Vanishing 해결방법__\n",
    "* 활성화 함수를 변경하거나 가중치 초기화 방법을 통해서 Gradient vanishing 문제를 완화할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-amendment",
   "metadata": {},
   "source": [
    "## 6. 지름길을 만들어주자\n",
    "\n",
    "<img src=\"./image/resnet.png\" alt=\"resnet\" />\n",
    "\n",
    "레이어를 깊게 쌓으면서 생기는 gradient exploding, vanishing 문제를 해결하기 위해서 __ResNet__은 생각보다 간단한 방법을 사용했다.\n",
    "\n",
    "위의 그래프는 이미지넷 챌린지의 역대 모델들의 레이어 수와 오류율(error rate)의 그래프이다. ResNet의 레이어 수는 무려 152개를 넘어간다.\n",
    "\n",
    "그럼 ResNet은 어떤 방식을 도입해서 이렇게 깊은 레이어를 쌓을 수 있을까?\n",
    "\n",
    "아래는 _VGG19_ 와 _ResNet34_ 의 네트워크 구조를 비교한 그림이다. 왼쪽의 VGG와 중간, 우측의 ResNet을 비교해 보면 ResNet은 VGG보다 확연히 레이어 수가 많다.\n",
    "\n",
    "가운데 *Plain ResNet*과 우측의 *Residual ResNet*을 비교해 볼까?. ResNet에서는 Residual Model에서 보이는 것처럼 __Skip Connection__이라는 구조를 사용해서 Gradient Vanishing 문제를 해결했다.\n",
    "\n",
    "<img src=\"./image/resnet2.png\" alt=\"resnet\" />\n",
    "\n",
    "__Skip Connection__은 아래처럼 레이어의 입력을 다른 곳에 이어서 Gradient가 깊은 곳까지 이어지도록 한다. 아래 그림처럼 레이어와 Skip Connection이 있는 블록을 Residual Block이라고 한다. 이번에도 Andrew Ng 교수님의 ResNet에 대한 설명을 보고 더 깊게 배워보자.\n",
    "\n",
    "<img src=\"./image/skip_connection.png\" alt=\"Skip Connection\" />\n",
    "\n",
    "[![Skip Connection](http://img.youtube.com/vi/ZILIbUvp5lk/0.jpg)](https://youtu.be/ZILIbUvp5lk) \n",
    "\n",
    "__Skip connection을 가진 네트워크__\n",
    "\n",
    "* 다양한 모델들이 있다. segmentation에서 활용되는 U-Net의 구조도 일종의 Skip connection으로 볼 수 있다.\n",
    "\n",
    "이론은 대략적으로 본 것 같다. 이제 네트워크가 어떻게 만들어지는지 직접 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-potential",
   "metadata": {},
   "source": [
    "## 7. 딥네트워크 속속들이\n",
    "지금까지 잘 알려진 딥네트워크들이 어떤 이유에서 어떤 구조를 갖게 되었는지 확인해 보았다.\n",
    "\n",
    "이제 직접 모델을 만들어보는 대신, 우리가 봐왔던 모델이 Tensorflow의 model API와 Keras에서 어떻게 구현되어 있는지 모델 구조와 비교하며 코드를 확인해보도록 하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-party",
   "metadata": {},
   "source": [
    "### 목표\n",
    "---\n",
    "1. 그림과 글로만 보던 딥네트워크 어떻게 만들고 있는지 알아본다.\n",
    "2. 논문의 방법이 사용된 부분을 코드에서 찾을 수 있다.\n",
    "3. 나도 할 수 있다는 자신감을 챙긴다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-april",
   "metadata": {},
   "source": [
    "## 8. Model API\n",
    "\n",
    "__Tensorflow__\n",
    "\n",
    "Tensorflow의 사전학습 모델(pre-trained model)들은 slim이라는 고수준 API로 구현되어있다.\n",
    "\n",
    "[Tensorflow models repository](https://github.com/tensorflow/models) 에서 구현된 모델들을 확인하실 수 있다.\n",
    "\n",
    "__Keras__\n",
    "\n",
    "Keras는 Keras의 Layer를 사용해서 모델이 구현되어 있다. 또, Keras applications에서 사전학습된 모델들이 구현되어 있다.\n",
    "\n",
    "[Keras applications docs](https://www.tensorflow.org/api_docs/python/tf/keras/applications) 에서 어떤 모델을 지원하는지 확인이 가능하다. 그리고 [Keras applications](https://github.com/keras-team/keras-applications) 에서 구현된 코드를 확인하실 수 있다.\n",
    "\n",
    "공부를 위해서 직접 구현하거나 특정 부분의 수정을 원할 때, 또는 공식 문서로 봐도 충분히 이해가 되지 않을 때는 코드를 직접 읽는 것이 가장 이해가 빠를 수 있다. Tensorflow가 편한 분들은 Tensorflow 구현으로 진행하셔도 무방하다. 다만 Keras의 모델 구현이 간결해 초심자에게 유용하니 Keras로 프로젝트를 진행하는 것을 추천한다.\n",
    "\n",
    "__각 프레임워크(TensorFlow, Keras)__\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/applications 에서 확인할 수 있다.\n",
    "* VGG-16 :  https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/applications/vgg16.py#L39-L216\n",
    "\n",
    "* ReNet-50 :  https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/applications/resnet.py#L444-L463\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-enemy",
   "metadata": {},
   "source": [
    "## 9. VGG-16\n",
    "\n",
    "<img src=\"./image/vgg.png\" alt=\"VGG-16\" />\n",
    "\n",
    "네 번째 스텝(CNN을 잘 쓰자)에서 봤던 VGG16이다. Max pooling과 softmax 등의 활성화 함수(Activation function)를 제외하면 CNN 계층과 완전 연결 계층(Fully Connected Layer)을 합쳐 16개의 레이어로 구성되어있다.\n",
    "\n",
    "그냥 VGG16 코드를 찾아서 읽어보기만 하면 재미가 없을 것이니 그동안 우리가 만들어 본 분류기의 모델을 VGG로 바꿔보는 과정을 진행해보자. 예를 들면 아래와 같은 분류기 말이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lyric-union",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:46:42.600501Z",
     "start_time": "2021-03-15T02:46:40.565552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.applications import imagenet_utils\n",
    "\n",
    "# CIFAR100 데이터셋을 가져옵시다. \n",
    "cifar100 = keras.datasets.cifar100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "married-horizon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:46:42.804500Z",
     "start_time": "2021-03-15T02:46:42.642585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demographic-apparatus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:46:43.205395Z",
     "start_time": "2021-03-15T02:46:42.848268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               25700     \n",
      "=================================================================\n",
      "Total params: 325,956\n",
      "Trainable params: 325,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input = keras.Input(shape=(32, 32, 3))\n",
    "\n",
    "x = keras.layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "x = keras.layers.MaxPool2D((2,2))(x)\n",
    "x = keras.layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = keras.layers.MaxPool2D((2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "predictions = keras.layers.Dense(100, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=img_input, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-federation",
   "metadata": {},
   "source": [
    "우리가 그동안 학습했던 Convolution Neural Network는 대략 위와 같은 model 구조를 가지고 있을 것이다. 학습해야 할 파라미터가 대략 326K 정도 되는 모습이다.\n",
    "\n",
    "1 Epoch만 훈련시켜보자. 우리는 성능 최적화를 시도하는 게 아니라 모델 구조를 분석하고 있는 것이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "underlying-supervision",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:46:57.326818Z",
     "start_time": "2021-03-15T02:46:43.247068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 7s 5ms/step - loss: 3.6300 - accuracy: 0.1522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbd6d2daed0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습!! \n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)    # 1 Epoch만 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-kruger",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:46:57.379468Z",
     "start_time": "2021-03-15T02:46:57.375762Z"
    }
   },
   "source": [
    "좋다. 그럼 이제 위 코드에서 model 부분을 VGG16으로 바꿔보겠다. 앞장에서 언급한 [Keras VGG16 코드 구현](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/applications/vgg16.py#L39-L216) 소스코드로부터 다음 구현 부분을 찾아서 적용해보자.\n",
    "\n",
    "지금부터 해볼 문제는 코드에서 Block마다 구현이 어떻게 되어 있는지 찾는 것이다. 구분해서 찾아야 할 블록은 다음과 같다.\n",
    "\n",
    "1. 첫 번째 블록(첫 번째 Max pooling까지)\n",
    "2. 두 번째 블록(두 번째 Max pooling까지)\n",
    "3. 세 번째 블록(세 번째 Max pooling까지)\n",
    "4. 네 번째 블록(네 번째 Max pooling까지)\n",
    "5. 다섯 번째 블록(다섯 번째 Max pooling까지)\n",
    "6. 여섯 번째 블록(완전 연결 계층 + softmax까지)\n",
    "\n",
    "아래 예시는 Keras의 `VGG16` 구현에서 첫 번째 블록의 예시답안은 다음과 같다. Keras의 Functional API 구조가 익숙하다면 어려움 없이 답을 찾으실 수 있을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "broken-consultancy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:48:33.716242Z",
     "start_time": "2021-03-15T02:48:33.676106Z"
    }
   },
   "outputs": [],
   "source": [
    "# 첫 번째 블록(예시)\n",
    "x = layers.Conv2D(64, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block1_conv1')(img_input)\n",
    "x = layers.Conv2D(64, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block1_conv2')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-wages",
   "metadata": {},
   "source": [
    "블록과 레이어마다 커널이 어떻게 달라지는지, 또 구현할 때 레이어에 argument로 어떤 것들이 추가되는지 생각해보면 공부에 도움이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naked-director",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:48:35.076628Z",
     "start_time": "2021-03-15T02:48:35.033959Z"
    }
   },
   "outputs": [],
   "source": [
    "# 두 번째 블록\n",
    "x = layers.Conv2D(\n",
    "  128, (3, 3), activation='relu', padding='same', name='block2conv1')(x)\n",
    "x = layers.Conv2D(\n",
    "  128, (3, 3), activation='relu', padding='same', name='block2conv2')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hearing-relation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:48:41.286655Z",
     "start_time": "2021-03-15T02:48:41.235454Z"
    }
   },
   "outputs": [],
   "source": [
    "# 세 번째 블록\n",
    "x = layers.Conv2D(\n",
    "  256, (3, 3), activation='relu', padding='same', name='block3conv1')(x)\n",
    "x = layers.Conv2D(\n",
    "  256, (3, 3), activation='relu', padding='same', name='block3conv2')(x)\n",
    "x = layers.Conv2D(\n",
    "  256, (3, 3), activation='relu', padding='same', name='block3conv3')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3pool')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "material-services",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:48:49.882020Z",
     "start_time": "2021-03-15T02:48:49.828900Z"
    }
   },
   "outputs": [],
   "source": [
    "# 네 번째 블록\n",
    "x = layers.Conv2D(\n",
    "  512, (3, 3), activation='relu', padding='same', name='block4conv1')(x)\n",
    "x = layers.Conv2D(\n",
    "  512, (3, 3), activation='relu', padding='same', name='block4conv2')(x)\n",
    "x = layers.Conv2D(\n",
    "  512, (3, 3), activation='relu', padding='same', name='block4conv3')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4pool')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "official-ethernet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:48:58.715287Z",
     "start_time": "2021-03-15T02:48:58.683612Z"
    }
   },
   "outputs": [],
   "source": [
    "# 다섯 번째 블록\n",
    "x = layers.Conv2D(\n",
    "  512, (3, 3), activation='relu', padding='same', name='block5conv1')(x)\n",
    "x = layers.Conv2D(\n",
    "  512, (3, 3), activation='relu', padding='same', name='block5conv2')(x)\n",
    "x = layers.Conv2D(\n",
    "  512, (3, 3), activation='relu', padding='same', name='block5conv3')(x)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5pool')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accomplished-shift",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:49:05.155418Z",
     "start_time": "2021-03-15T02:49:05.112252Z"
    }
   },
   "outputs": [],
   "source": [
    "# 여섯 번째 블록\n",
    "x = layers.Flatten(name='flatten')(x)\n",
    "x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
    "x = layers.Dense(4096, activation='relu', name='fc2')(x)\n",
    "\n",
    "classes=100\n",
    "x = layers.Dense(classes, activation='softmax', name='predictions')(x)    # CIFAR100을 위한 모델 Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-thirty",
   "metadata": {},
   "source": [
    "위에 쌓은 VGG 레이어를 실제로 model로 만들어 보아야겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aggregate-agreement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:49:23.531548Z",
     "start_time": "2021-03-15T02:49:23.518979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2conv1 (Conv2D)         (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2conv2 (Conv2D)         (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3conv1 (Conv2D)         (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3conv2 (Conv2D)         (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3conv3 (Conv2D)         (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3pool (MaxPooling2D)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4conv1 (Conv2D)         (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4conv2 (Conv2D)         (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4conv3 (Conv2D)         (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4pool (MaxPooling2D)    (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5conv1 (Conv2D)         (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5conv2 (Conv2D)         (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5conv3 (Conv2D)         (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5pool (MaxPooling2D)    (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 100)               409700    \n",
      "=================================================================\n",
      "Total params: 34,006,948\n",
      "Trainable params: 34,006,948\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Model(inputs=img_input, outputs=x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-spring",
   "metadata": {},
   "source": [
    "VGG16 모델이 제대로 만들어졌다면 학습 파라미터 크기는 무려, 무려… 34M이나 된다. 위에서 우리가 만들어 보았던 Convolutional Neural Network의 무려 100배가 넘어가는 크기이다.\n",
    "\n",
    "이렇게 크고 복잡한 모델이라 할지라도 소스코드 구조를 분석하는 것은 의외로 어렵지 않다. Github에 있는 무궁무진한 소스코드들이 모두 우리의 훌륭한 학습자료가 되어줄 것이다.\n",
    "\n",
    "그럼 제대로 가져왔는지 실제로 학습을 수행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "moral-chair",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:51:48.698772Z",
     "start_time": "2021-03-15T02:50:04.559982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 71s 45ms/step - loss: 4.6069 - accuracy: 0.0087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbd515f6bd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습!! \n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)    # 1 Epoch만 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-california",
   "metadata": {},
   "source": [
    "파라미터가 34M나 되는 VGG16은 이미지넷 데이터를 토대로 수 시간 동안 학습을 진행해야 안정적으로 수렴한다. CIFAR100은 1 Epoch 정도로는 어림없을 것이다.<br>\n",
    "하지만 복잡해 보이는 모델도 충분히 가져와서 우리의 테스크에 활용할 수 있을 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-insulin",
   "metadata": {},
   "source": [
    "## 10. ResNet-50\n",
    "\n",
    "<img src=\"./image/resnet3.png\" alt=\"resnet\" />\n",
    "\n",
    "이번에는 __Skip connection__이 추가되어 있는 `ResNet`이다.\n",
    "\n",
    "Skip connection이 포함된 Residual Block은 어떻게 구현되어 있을까? 그리고 레이어 수가 많아졌는데 이전 VGG와 어떤 점이 다르게 구현이 되어 있을지 궁금하다.\n",
    "\n",
    "위에서 `ResNet`의 구조를 보면 색깔이 서로 다른 블록들이 있다. 이는 블록마다 feature의 크기가 서로 다르기 때문인데, 이렇게 크게 4개의 Stage로 구분해서 생각할 수 있다. 하나의 Stage 안에서는 kernel 사이즈와 channel 수가 동일하니, 이런 블록은 일일이 하나씩 짜지 않고 블록 단위로 생성한다.\n",
    "\n",
    "그렇다면 구현한 방법을 직접 찾아서 정리해보도록 하자. 이번에도 [Keras ResNet50 코드 구현](https://git.dst.etit.tu-chemnitz.de/external/tf-models/-/blob/1d057dfc32f515a63ab1e23fd72052ab2a954952/official/vision/image_classification/resnet_model.py) 소스코드로부터 모델 구현부분을 가져와보자. 이번 코드는 `resnet50()` 함수를 이용하면 모델까지 깔끔하게 생성해 주기 때문에 훨씬 가져오기 쉽도록 잘 정리되어 있다.\n",
    "\n",
    "코드를 분석해 보면 ResNet50 모델을 생성하기 위해서 반복적으로 활용하는 `conv_block`과 `identity_block`이 있을 것이다. 이런 블록 구조를 잘 활용하여, 50개나 되는 복잡한 레이어 구조를 간결하게 표현하고 있는 것을 확인할 수 있다. 데이터셋은 이전 스텝에서 활용했던 CIFAR100을 그대로 활용해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "joined-information",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:55:00.711464Z",
     "start_time": "2021-03-15T02:55:00.706185Z"
    }
   },
   "outputs": [],
   "source": [
    "# 추가로 import해야 할 패키지들을 먼저 가져옵니다. \n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import models\n",
    "\n",
    "# block 안에 반복적으로 활용되는 L2 regularizer를 선언해 줍니다.\n",
    "def _gen_l2_regularizer(use_l2_regularizer=True, l2_weight_decay=1e-4):\n",
    "    return regularizers.l2(l2_weight_decay) if use_l2_regularizer else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-serial",
   "metadata": {},
   "source": [
    "이어서 반복해서 활용되는 `conv_block`과 `identity_block`을 가져와보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "synthetic-india",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T02:58:08.440018Z",
     "start_time": "2021-03-15T02:58:08.430016Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2),\n",
    "               use_l2_regularizer=True,\n",
    "               batch_norm_decay=0.9,\n",
    "               batch_norm_epsilon=1e-5):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    Note that from stage 3,\n",
    "    the second conv layer at main path is with strides=(2, 2)\n",
    "    And the shortcut should have strides=(2, 2) as well\n",
    "    Args:\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        strides: Strides for the second conv layer in the block.\n",
    "        use_l2_regularizer: whether to use L2 regularizer on Conv layer.\n",
    "        batch_norm_decay: Moment of batch norm layers.\n",
    "        batch_norm_epsilon: Epsilon of batch borm layers.\n",
    "    Returns:\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters1, (1, 1),\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '2a')(\n",
    "            input_tensor)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=batch_norm_decay,\n",
    "        epsilon=batch_norm_epsilon,\n",
    "        name=bn_name_base + '2a')(\n",
    "            x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters2,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '2b')(\n",
    "            x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=batch_norm_decay,\n",
    "        epsilon=batch_norm_epsilon,\n",
    "        name=bn_name_base + '2b')(\n",
    "            x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters3, (1, 1),\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '2c')(\n",
    "            x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=batch_norm_decay,\n",
    "        epsilon=batch_norm_epsilon,\n",
    "        name=bn_name_base + '2c')(\n",
    "            x)\n",
    "\n",
    "    shortcut = layers.Conv2D(\n",
    "        filters3, (1, 1),\n",
    "        strides=strides,\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '1')(\n",
    "            input_tensor)\n",
    "    shortcut = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=batch_norm_decay,\n",
    "        epsilon=batch_norm_epsilon,\n",
    "        name=bn_name_base + '1')(\n",
    "            shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "formed-stream",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T03:00:05.454127Z",
     "start_time": "2021-03-15T03:00:05.447131Z"
    }
   },
   "outputs": [],
   "source": [
    "def identity_block(input_tensor,\n",
    "                   kernel_size,\n",
    "                   filters,\n",
    "                   stage,\n",
    "                   block,\n",
    "                   use_l2_regularizer=True,\n",
    "                   batch_norm_decay=0.9,\n",
    "                   batch_norm_epsilon=1e-5):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    Args:\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        use_l2_regularizer: whether to use L2 regularizer on Conv layer.\n",
    "        batch_norm_decay: Moment of batch norm layers.\n",
    "        batch_norm_epsilon: Epsilon of batch borm layers.\n",
    "    Returns:\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters1, (1, 1),\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '2a')(\n",
    "            input_tensor)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=batch_norm_decay,\n",
    "        epsilon=batch_norm_epsilon,\n",
    "        name=bn_name_base + '2a')(\n",
    "            x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters2,\n",
    "        kernel_size,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '2b')(\n",
    "            x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=batch_norm_decay,\n",
    "        epsilon=batch_norm_epsilon,\n",
    "        name=bn_name_base + '2b')(\n",
    "            x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters3, (1, 1),\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '2c')(\n",
    "            x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=batch_norm_decay,\n",
    "        epsilon=batch_norm_epsilon,\n",
    "        name=bn_name_base + '2c')(\n",
    "            x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-brook",
   "metadata": {},
   "source": [
    "자, 이제 `resnet50()` 함수를 가져올 준비가 다 되었다.\n",
    "\n",
    "한 가지만 유의할 점이 있다. resnet50 모델을 생성하는 함수 안에 Imagenet 데이터셋에 해당하는 input shape가 `input_shape = (224, 224, 3)`으로 선언되어 있다. 우리는 CIFAR100을 다루고 있으니 `input_shape = (32, 32, 3)`이어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ranging-export",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T03:06:58.772360Z",
     "start_time": "2021-03-15T03:06:58.758121Z"
    }
   },
   "outputs": [],
   "source": [
    "def resnet50(num_classes,\n",
    "             batch_size=None,\n",
    "             use_l2_regularizer=True,\n",
    "             rescale_inputs=False,\n",
    "             batch_norm_decay=0.9,\n",
    "             batch_norm_epsilon=1e-5):\n",
    "    \"\"\"Instantiates the ResNet50 architecture.\n",
    "    Args:\n",
    "        num_classes: `int` number of classes for image classification.\n",
    "        batch_size: Size of the batches for each step.\n",
    "        use_l2_regularizer: whether to use L2 regularizer on Conv/Dense layer.\n",
    "        rescale_inputs: whether to rescale inputs from 0 to 1.\n",
    "        batch_norm_decay: Moment of batch norm layers.\n",
    "        batch_norm_epsilon: Epsilon of batch borm layers.\n",
    "    Returns:\n",
    "        A Keras model instance.\n",
    "    \"\"\"\n",
    "\n",
    "    input_shape = (32, 32, 3)  # CIFAR100을 위한 input_shape 조정입니다. \n",
    "    img_input = layers.Input(shape=input_shape, batch_size=batch_size)\n",
    "    if rescale_inputs:\n",
    "        # Hub image modules expect inputs in the range [0, 1]. This rescales these\n",
    "        # inputs to the range expected by the trained model.\n",
    "        x = layers.Lambda(\n",
    "            lambda x: x * 255.0 - backend.constant(\n",
    "                imagenet_preprocessing.CHANNEL_MEANS,\n",
    "                shape=[1, 1, 3],\n",
    "                dtype=x.dtype),\n",
    "            name='rescale')(\n",
    "                img_input)\n",
    "    else:\n",
    "        x = img_input\n",
    "\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        x = layers.Permute((3, 1, 2))(x)\n",
    "        bn_axis = 1\n",
    "    else:  # channels_last\n",
    "        bn_axis = 3\n",
    "\n",
    "    block_config = dict(\n",
    "          use_l2_regularizer=use_l2_regularizer,\n",
    "          batch_norm_decay=batch_norm_decay,\n",
    "          batch_norm_epsilon=batch_norm_epsilon)\n",
    "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(x)\n",
    "    x = layers.Conv2D(\n",
    "          64, (7, 7),\n",
    "          strides=(2, 2),\n",
    "          padding='valid',\n",
    "          use_bias=False,\n",
    "          kernel_initializer='he_normal',\n",
    "          kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "          name='conv1')(\n",
    "                x)\n",
    "    x = layers.BatchNormalization(\n",
    "          axis=bn_axis,\n",
    "          momentum=batch_norm_decay,\n",
    "          epsilon=batch_norm_epsilon,\n",
    "          name='bn_conv1')(\n",
    "                x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    x = conv_block(\n",
    "        x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), **block_config)\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', **block_config)\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', **block_config)\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', **block_config)\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', **block_config)\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', **block_config)\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', **block_config)\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', **block_config)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b', **block_config)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c', **block_config)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d', **block_config)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e', **block_config)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f', **block_config)\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', **block_config)\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', **block_config)\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', **block_config)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(\n",
    "        num_classes,\n",
    "        kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        bias_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name='fc1000')(\n",
    "                x)\n",
    "\n",
    "    # A softmax that is followed by the model loss must be done cannot be done\n",
    "    # in float16 due to numeric issues. So we pass dtype=float32.\n",
    "    x = layers.Activation('softmax', dtype='float32')(x)\n",
    "\n",
    "    # Create model.\n",
    "    return models.Model(img_input, x, name='resnet50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-bookmark",
   "metadata": {},
   "source": [
    "`resnet50()` 가 드디어 완성되었다. 이제 이를 활용해서 우리의 model을 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "jewish-archive",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T03:07:21.540601Z",
     "start_time": "2021-03-15T03:07:19.844053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 16, 16, 64)   9408        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 16, 16, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16, 16, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 8, 8, 64)     4096        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 8, 8, 64)     256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 8, 8, 64)     0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 8, 8, 64)     36864       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 8, 8, 64)     256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 8, 8, 64)     0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 8, 8, 256)    16384       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 8, 8, 256)    16384       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 8, 8, 256)    1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 8, 8, 256)    1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8, 8, 256)    0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 256)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 8, 8, 64)     16384       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 8, 8, 64)     256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 8, 64)     0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 8, 8, 64)     36864       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 8, 8, 64)     256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 64)     0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 8, 8, 256)    16384       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 8, 8, 256)    1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 8, 8, 256)    0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 256)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 8, 8, 64)     16384       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 8, 8, 64)     256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 64)     0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 8, 8, 64)     36864       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 8, 8, 64)     256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 64)     0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 8, 8, 256)    16384       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 8, 8, 256)    1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 8, 256)    0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 256)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 8, 8, 128)    32768       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 8, 8, 128)    512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 128)    0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 4, 4, 128)    147456      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 4, 4, 128)    512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 4, 4, 128)    0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 4, 4, 512)    65536       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 4, 4, 512)    131072      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 4, 4, 512)    2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 4, 4, 512)    2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 4, 4, 512)    0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 4, 4, 512)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 4, 4, 128)    65536       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 4, 4, 128)    512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 4, 4, 128)    0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 4, 4, 128)    147456      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 4, 4, 128)    512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 4, 4, 128)    0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 4, 4, 512)    65536       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 4, 4, 512)    2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 4, 4, 512)    0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 4, 4, 512)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 4, 4, 128)    65536       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 4, 4, 128)    512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 4, 4, 128)    0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 4, 4, 128)    147456      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 4, 4, 128)    512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 4, 4, 128)    0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 4, 4, 512)    65536       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 4, 4, 512)    2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 4, 512)    0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 4, 4, 512)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 4, 4, 128)    65536       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 4, 4, 128)    512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 4, 4, 128)    0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 4, 4, 128)    147456      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 4, 4, 128)    512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 4, 4, 128)    0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 4, 4, 512)    65536       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 4, 4, 512)    2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 512)    0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 4, 4, 512)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 4, 4, 256)    131072      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 4, 4, 256)    1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 256)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 2, 2, 256)    589824      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 2, 2, 256)    0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 2, 2, 1024)   262144      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 2, 2, 1024)   524288      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 2, 2, 1024)   4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 2, 2, 1024)   0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 2, 2, 1024)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 2, 2, 256)    262144      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 2, 2, 256)    0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 2, 2, 256)    589824      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 2, 2, 256)    0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 2, 2, 1024)   262144      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 2, 2, 1024)   0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 2, 2, 1024)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 2, 2, 256)    262144      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 2, 2, 256)    0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 2, 2, 256)    589824      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 2, 2, 256)    0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 2, 2, 1024)   262144      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 2, 2, 1024)   0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 2, 2, 1024)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 2, 2, 256)    262144      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 2, 2, 256)    0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 2, 2, 256)    589824      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 2, 2, 256)    0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 2, 2, 1024)   262144      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 2, 2, 1024)   0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 2, 2, 1024)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 2, 2, 256)    262144      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 2, 2, 256)    0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 2, 2, 256)    589824      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 2, 2, 256)    0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 2, 2, 1024)   262144      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 2, 2, 1024)   0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 2, 2, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 2, 2, 256)    262144      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 2, 2, 256)    1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 2, 2, 256)    0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 2, 2, 256)    589824      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 2, 2, 256)    1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 2, 2, 256)    0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 2, 2, 1024)   262144      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 2, 2, 1024)   4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 2, 2, 1024)   0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 2, 2, 1024)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 2, 2, 512)    524288      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 2, 2, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 1, 1, 512)    2359296     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 1, 1, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 1, 1, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 1, 1, 2048)   1048576     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 1, 1, 2048)   2097152     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 1, 1, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 1, 1, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 1, 1, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1, 1, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 1, 1, 512)    1048576     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 1, 1, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 1, 1, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 1, 1, 512)    2359296     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 1, 1, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 1, 1, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 1, 1, 2048)   1048576     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 1, 1, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 1, 1, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 1, 1, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 1, 1, 512)    1048576     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 1, 1, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 1, 1, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 1, 1, 512)    2359296     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 1, 1, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1, 1, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 1, 1, 2048)   1048576     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 1, 1, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 1, 1, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 1, 1, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (None, 100)          204900      global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 100)          0           fc1000[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,766,052\n",
      "Trainable params: 23,712,932\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet50(num_classes=100)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-subscription",
   "metadata": {},
   "source": [
    "이번에는 24M가 안 되는 크기이다. VGG16보다 레이어 수는 훨씬 많지만 모델의 크기는 오히려 10M 정도 작다는 것을 알 수 있다.\n",
    "\n",
    "그럼 이번에도 잘 학습이 돌아가는지 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "blocked-percentage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T03:10:46.710342Z",
     "start_time": "2021-03-15T03:07:51.535587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 110s 70ms/step - loss: 8.3219 - accuracy: 0.0650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbc44165d90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습!! \n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)    # 1 Epoch만 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-southeast",
   "metadata": {},
   "source": [
    "아주 미세하지만 VGG-16보다 loss와 accuracy가 개선된 것이 보인다.\n",
    "\n",
    "처음부터 이렇게 안정되게 학습이 진행된다는 것이 Resnet의 구조가 가지는 또 하나의 장점인 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-press",
   "metadata": {},
   "source": [
    "__resnet50 Skip connection(또는 Short cut이라고도 합니다.)__\n",
    "\n",
    "conv_block 내에서 구현\n",
    "\n",
    "```python\n",
    "x = layers.add([x, shortcut])\n",
    "```\n",
    "\n",
    "identity_block 내에서 구현\n",
    "\n",
    "```python\n",
    "    x = layers.add([x, input_tensor])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
