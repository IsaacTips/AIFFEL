{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "essential-expert",
   "metadata": {},
   "source": [
    "# 26. 빅데이터 생태계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-paraguay",
   "metadata": {},
   "source": [
    "## 1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-priority",
   "metadata": {},
   "source": [
    "### 학습 목표\n",
    "---\n",
    "* 하둡과 스파크의 에코시스템을 통해 빅데이터를 구성하는 기술들에 대해 학습해본다.\n",
    "* 스파크의 개요와 특징에 대해 알아보고, PySpark를 이용해 빅데이터 툴을 다루어본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-netscape",
   "metadata": {},
   "source": [
    "### 목차\n",
    "---\n",
    "1. 빅데이터의 양대 산맥: Hadoop과 Spark\n",
    "    1. 빅데이터 연대기\n",
    "    2. Hadoop Ecosystem\n",
    "    3. Spark Ecosystem\n",
    "2. Introduction to Spark\n",
    "    1. Spark 데이터처리: RDD\n",
    "    2. RDD 생성과 동작\n",
    "    3. PySpark 설치\n",
    "    4. SparkContext를 통한 스파크 초기화\n",
    "3. Spark 실습\n",
    "    1. RDD Creation\n",
    "    2. RDD Operation (1) Transformations\n",
    "    3. RDD Operation (2) Actions\n",
    "    4. RDD Operation (3) 실습:MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-spring",
   "metadata": {},
   "source": [
    "## 2. 빅데이터 연대기\n",
    "빅데이터기술은 2000년대 들어 급속도록 발전해 왔다. 시간순으로 나열하면 GFS(Google File System)와 맵리듀스 기술이 공개되고 하둡(Hadoop)이 나오고 그 뒤, 스파크(Spark)가 발표되었다. 빅데이터 기술 자체가 굉장히 광범위하기 때문에 API형태로 필요한 기술만 쓰는 경우도 많이 있지만, 보통 Hadoop Ecosystem이라고 부르는 하둡 기반의 거대한 빅데이터 플랫폼 위에 스파크가 적용되는 것이 일반적이다.\n",
    "\n",
    "그리고 하둡은 기본적으로 Java 프로그래밍 언어 기반이며 스파크는 Java와 Scala가 기본 언어이다. 그러나 일부 기능에 한해서 Python으로도 조작이 가능하도록 API 형태로 제공하기도 한다. 오늘은 시간순으로 빅데이터 기술이 어떻게 발전되어 왔는지 그리고 Hadoop Ecosystem과 스파크란 무엇인지에 대해 알아볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-teens",
   "metadata": {},
   "source": [
    "### 1) 빅데이터 연대기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-graphic",
   "metadata": {},
   "source": [
    "#### 2003년: GFS(Google File System)\n",
    "IT업계에서 매우 유명한 회사인 구글은 검색엔진뿐만 아니라 Youtube, AI, 클라우드, 자율주행 자동차 등 정말 많은 분야를 선도하는 회사이다. 사실 구글은 새로운 데이터에 대한 필요성(Needs), 그 변화의 바람을 먼저 읽고 준비한 회사라고 할 수 있다. 그만큼 초기부터 데이터에 대한 고민을 정말 많이 하였고 새로운 데이터 시대에 맞는 파일시스템과 프로그래밍 모델을 각각 발표하였는데, 이는 현재 빅데이터의 근간이 되는 기술들이다. 우선 제일 처음으로 2003년에는 빅데이터용의 새로운 파일 시스템인 GFS(Google File System)을 발표하였다. 시스템 고장을 효과적으로 처리할 수 있게 복제가 굉장히 용이하고 분산처리에 적합한 파일 시스템이다.\n",
    "\n",
    "공식 논문 링크이다. \n",
    "\n",
    "[The Google File System](https://static.googleusercontent.com/media/research.google.com/ko//archive/gfs-sosp2003.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-assistant",
   "metadata": {},
   "source": [
    "#### 2004년: MapReduce on Simplified Data Processing on Large Clusters\n",
    "뒤이어 바로 1년 뒤인 2004년 구글에서는 분산 처리 환경에 맞는 프로그래밍 모델인 맵리듀스를 발표한다. GFS와 같은 분산 처리 파일 시스템에 적용하기 쉬운 프로그래밍 모델이다. 데이터관련 작업을 맵(map) 함수와 리듀스(reduce) 함수의 2가지 작업으로 나누어 처리하는 것으로, 맵 함수에서는 키-값 쌍을 처리해 중간의 키-값 쌍을 생성하고 리듀스 함수는 동일한 키와 연관된 모든 중간의 값들을 병합하는 함수였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-queens",
   "metadata": {},
   "source": [
    "#### 2004년 - 2005년: NDFS Project\n",
    "아파치(Apache) 재단의 더그 커팅(Doug Cutting)과 마이크 카파렐라(Mike Cafarella)가 중심이 되어 검색엔진의 효과적인 분산처리를 위해 NDFS(Nutch Distributed File System) 이란 프로젝트를 시작한다. Nutch(너치)는 엘라스틱서치라고 하는 검색엔진의 전신 소프트웨어입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-strand",
   "metadata": {},
   "source": [
    "#### 2006년 - 2007년: Apache Hadoop\n",
    "그리고 더그 커팅(Doug Cutting)이 중심이 되어 중심이 되어 구글이 발표한 맵리듀스와 GFS개념을 더 보완하여 하둡(Hadoop)이라는 이름의 빅데이터용 오픈소스 프로젝트를 시작하는데, 하둡이 어느 정도 성과를 보이자 아파치 재단에서는 이 프로젝트를 가장 우선순위가 높은 공식 프로젝트로 선정하게 되고, 2006년에 하둡 1.0 이 만들어진다. 하둡 1.0의 핵심 기술은 __HDFS(Hadoop Distributed File System)__과 __MapReduce__ 2가지이다. 현재 빅데이터에 근간이 되는 기술들이다. (공식 발표는 2007년)\n",
    "\n",
    "<img src=\"./image/hadoop.png\" alt=\"hadoop\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-softball",
   "metadata": {},
   "source": [
    "#### 2007년 - 2008년: 폭발적인 성장\n",
    "그리고 이 무렵 더그 커팅은 야후(yahoo)에서 근무하게 되는데, 야후에서 그 확장성을 인정받은 뒤, 세계의 유수한 기업 Facebook, linkedin, Twitter 등의 회사에서 하둡을 사용하며 그 인기가 증가하게된다.\n",
    "\n",
    "<img src=\"./image/hadoop2.png\" alt=\"hadoop\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-syntax",
   "metadata": {},
   "source": [
    "#### 2009년 - 2013년: Apache Spark\n",
    "하둡에도 한 가지 단점이 있었다. 이는 바로 하드디스크에 파일을 처리하는 점이다. 물론 하드디스크가 값도 싸고 보존에도 용이하다는 장점이 있지만 간혹 고속으로 데이터를 처리할 때, 메모리에 올려서 처리해야 할 때도 있다. 이러한 니즈(Needs)가 점점 생겨나면서 메모리 기반(In-Memory)의 데이터 처리 방법에 대해 고민하게 된다. UC 버클리의 마태 자하리아(matei zaharia)는 이점을 개선하기 위한 프로젝트를 시작하는데요 하둡과 똑같은 맵리듀스 개념을 사용하지만 데이터 처리 방법과 Task 정리 방법을 개선하여 RDD(Resilient Distributed Dataset)를 이용하는 스파크란 프로그램을 발표한다. RDD를 한국어로 \"탄력적 분산 데이터셋\"으로 번역하기도 한다. 이는 스파크의 기본이 되는 데이터셋이다. 본 프로젝트는 2009년에 시작해서 2012년 마태 자하리아의 박사과정 논문으로 공식 발표된다.\n",
    "\n",
    "아래는 논문 링크이다.\n",
    "\n",
    "[Resilient Distributed Datasets: A Fault-Tolerant Abstraction forIn-Memory Cluster Computing](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
    "\n",
    "[Spark: Cluster Computing with Working Sets](https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf)\n",
    "\n",
    "그리고 2012년부터 이 프로젝트는 아파치 재단으로 넘어가 아파치 재단의 최상위 프로젝트로 선정되고, 2013년 아파치 스파크 0.7이 발표된다.\n",
    "상세 이력은 아래 공식 홈페이지에서 확인할 수 있다.\n",
    "\n",
    "[News | Apache Spark](https://spark.apache.org/news/index.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-carbon",
   "metadata": {},
   "source": [
    "#### 2014년 - 2020년: Databricks와 Apache Spark\n",
    "\n",
    "<img src=\"./image/databricks.png\" alt=\"databricks\" width=\"40%\"/>\n",
    "\n",
    "데이터브릭스(Databrics)는 2014년도에 설립된 스타트업 회사로 스파크를 만든 마태 자하리아가 설립한 회사이다. 스파크는 기본적으로 클러스터 위에서 동작하는 프로그램인데, (물론 로컬에서도 동작한다.) 데이터브릭스는 스파크관련 데이터 분석 및 클러스터 환경을 제공해 주는 회사이다. 데이터브릭스란 회사에서 상업용으로도 만들거나 스파크 관련 클라우드 환경까지 제공해주고 있다. 동시에 스파크는 아파치 재단에서 오픈소스로도 개발을 진행하고 있으며, 2021년 현재 아파치 스파크는 3.1.1이 공개되었다.\n",
    "\n",
    "다음 동영상은 마태 자하리아의 Spark 소개 영상이다.\n",
    "\n",
    "[![Spark](http://img.youtube.com/vi/p8FGC49N-zM/0.jpg)](https://youtu.be/p8FGC49N-zM) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-service",
   "metadata": {},
   "source": [
    "### 2) 연대기 요약\n",
    "---\n",
    "빅데이터가 어떤식으로 발전되어 왔는지 조금더 상세하게 연대 순으로 요약하면 다음과 같다.\n",
    "\n",
    "2002년\n",
    "\n",
    "* 더그 커팅(Doug Cutting)과 마이크 카파렐라(Mike Cafarellr)가 웹 크롤러 너치(Nutch) 개발\n",
    "\n",
    "2003년\n",
    "\n",
    "* 구글에서 분산 처리용 파일 시스템인 GFS(Google File System) 논문 발표\n",
    "\n",
    "2004년\n",
    "\n",
    "* 구글에서 분산 처리용 프로그래밍 모델인 맵리듀스(MapReduce) 논문 발표\n",
    "\n",
    "2005년\n",
    "\n",
    "* 더그 커팅, 마이크 카파렐라의 주도 하에 NDFS(Nutch Distributed File System) 프로젝트 시작\n",
    "\n",
    "2006년\n",
    "\n",
    "* 1월: 더그 커팅 야후(Yahoo) 에 합류\n",
    "* 3월: 야후에서 최소의 하둡 기반 클러스터 구축\n",
    "* 11월: 구글에서 빅테이블(BigTable) 논문 공개 이는 향후 아파치 하둡의 HBase의 근간이 됨\n",
    "\n",
    "2007년\n",
    "\n",
    "* 아파치 하둡 1.0 발표\n",
    "\n",
    "2008년\n",
    "\n",
    "* 6월: 하둡의 하위 프로젝트로 SQL관련 컴포넌트인 하이브(Hive) 개발\n",
    "* 8월: 클라우데라(Cloudera) 창업 (*하둡 상용화 버전)\n",
    "\n",
    "2009년\n",
    "\n",
    "* UC 버클리대학의 마태 자하리아(Matei Zaharia)가 스파크(Spark) 연구 시작\n",
    "\n",
    "2012년\n",
    "\n",
    "* 8월: 하둡의 컴포넌트인 얀(YARN) 개발\n",
    "* 10월: 최초의 하둡용 네이티브 MPP 분석용 컴포넌트 임팔라(Impala) 개발\n",
    "\n",
    "2014년\n",
    "\n",
    "* 아파치 스파크 1.0 출시\n",
    "* 마태 자하리아, 알리 고디시(Ali Ghodsi)등이 데이터브릭스(databrics) 창업\n",
    "\n",
    "이렇게 시간순으로 보니 빅데이터기술이 21세기 들어 급격히 발전한 것 같다. 끝으로 Hadoop을 만든 더그 커딩 관련 재밌는 글을 읽어보자.\n",
    "\n",
    "[빅데이터 시대를 열다, 하둡을 창시한 더그 커팅](https://brunch.co.kr/@hvnpoet/98)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-processor",
   "metadata": {},
   "source": [
    "## 3. 빅데이터 양대산맥 (1) Hadoop Ecosystem\n",
    "이렇게 2003년부터 빅데이터 처리를 위한 새로운 시스템이 생겨났고 가장 기본이 되는 기술은 하둡과 스파크이다. 하둡은 맵리듀스 HDFS가 나온 이후에도 각각의 컴포넌트들이 추가되며 매우 큰 SW를 만들었다. 이를 Hadoop Ecosystem 이라고 한다. 아래 그림은 하둡의 에코시스템을 도식화한 그림이다.\n",
    "\n",
    "<img src=\"./image/hadoop_ecosystem.png\" alt=\"Hadoop Ecosystem\" />\n",
    "\n",
    "각각의 컴포넌트들을 역할에 따라 재정렬하면 다음과 같다.\n",
    "\n",
    "<img src=\"./image/hadoop_ecosystem2.png\" alt=\"Hadoop Ecosystem\" />\n",
    "\n",
    "위 그림에 나타난 주요 컴포넌트들의 역할을 소개하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-spirit",
   "metadata": {},
   "source": [
    "#### 데이터 수집(Data Ingestion)\n",
    "* 스쿱(Sqoop) : RDBMS(오라클, MySQL등..)와 하둡 사이의 데이터를 이동시켜준다.\n",
    "* 플럼(Flume) : 분산환경에서 대량의 로그데이터를 효과적으로 수집하여 합친 후 다른 곳으로 전송한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-humanitarian",
   "metadata": {},
   "source": [
    "#### 데이터 처리(Data Processing)\n",
    "* 하둡 분산파일시스템(HDFS): 하둡의 분산 처리 파일 시스템.\n",
    "* 맵리듀스(MapReduce): Java기반의 맵리듀스 프로그래밍 모델이다.\n",
    "* 얀(Yarn): 하둡 클러스터의 자원(Resource)을 관리.\n",
    "* 스파크(Spark): In-memory기반의 클러스터 컴퓨팅 데이터 처리이다. 스파크 안에도 스파크 코어, 스파크SQ, Milib, GraphX과 같은 컴포넌트가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-wings",
   "metadata": {},
   "source": [
    "#### 데이터 분석(Data Analysis)\n",
    "* 피그(Pig): 맵리듀스로 실행하기 어려운 데이터 관련 작업, filter, join, query와 같은 작업을 실행한다.\n",
    "* 임팔라(Impala): 고성능의 SQL 엔진.\n",
    "* 하이브(Hive): 임팔라와 유사한 SQL 관련 기능을 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-spyware",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T01:08:14.283414Z",
     "start_time": "2021-03-05T01:08:14.280244Z"
    }
   },
   "source": [
    "#### 데이터 검색(Data Exploration)\n",
    "* 클라우데라 서치(Cloudera Search): real-time으로 데이터에 검색이 가능하다.\n",
    "* 휴(Hue): 웹 인터페이스 제공."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-exploration",
   "metadata": {},
   "source": [
    "#### 기타\n",
    "* 우지(Oozie): 워크플로우 관리, Job 스케쥴러.\n",
    "* HBase: NoSQL기반으로 HDFS에 의해 처리된 데이터를 저장한다.\n",
    "* 제플린(Zeppelin): 데이터 시각화.\n",
    "* SparkMLlib, 머하웃(mahout): 머신러닝 관련 라이브러리."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-still",
   "metadata": {},
   "source": [
    "## 4. 빅데이터 양대산맥 (2) Spark Ecosystem\n",
    "스파크(Spark)에 대해서 알아보자. 앞서 하둡 에코시스템을 소개하면서 스파크는 In-memory기반의 클러스터 컴퓨팅 데이터 처리 프로그램이라고 했는데, 그렇게 보면 스파크가 이전의 하둡 에코시스템과 완전히 별개로 존재하는 독립적인 생태계를 이루고 있는 것이 아니라, 하둡 기반의 빅데이터 생태계를 이루는 주요한 컴포넌트로 어울려 존재하고 있다는 것을 알 수 있다.\n",
    "\n",
    "하지만 스파크 안에도 Spark SQL, Spark Streaming, MiLib과 같은 라이브러리가 있으며, 스파크 관점에서 빅데이터 생태계를 아래 그림과 같이 재구성해 볼 수 있다.\n",
    "\n",
    "<img src=\"./image/spark_ecosystem.png\" alt=\"Spark Ecosystem\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-domestic",
   "metadata": {},
   "source": [
    "#### 프로그래밍 언어: Scala, Java, Python, R, SQL\n",
    "스파크가 지원하는 프로그래밍 언어로는 Scala, Java, Python, R 이 있다. 위 그림에는 없지만 SQL역시 지원하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-multimedia",
   "metadata": {},
   "source": [
    "#### 스파크 라이브러리\n",
    "각각의 라이브러리는 다음과 같은 역할을 한다.\n",
    "\n",
    "* Spark SQL: SQL 관련 작업\n",
    "* Streaming: Streaming 데이터 처리\n",
    "* MLlib: Machine Learning 관련 라이브러리\n",
    "* GraphX: Graph Processing\n",
    "\n",
    "자원관리(주로 클러스터 관리)는 하둡의 Yarn 또는 Mesos를 사용하거나, 또는 스파크 자체의 관리기능을 그대로 사용한다.\n",
    "\n",
    "데이터 저장(Storage)은 Local FS(File System)이나 하둡의 HDFS를 이용하거나 AWS의 S3 인스턴스를 이용하기도 한다. (주로 Amazon S3를 많이 사용한다.) 그리고 기존의 RDBMS나 NoSQL을 사용하는 경우도 있다. 하둡의 HDFS같이 스파크의 전용 분산 데이터 저장 시스템을 별도로 가지고 있지 않다는 점에서, 스파크의 에코시스템이 가지는 유연한 확장성이 강조된 설계 사상을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-words",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T01:12:20.512315Z",
     "start_time": "2021-03-05T01:12:20.509347Z"
    }
   },
   "source": [
    "### 3) Hadoop과 Spark비교\n",
    "---\n",
    "빅데이터의 큰 시스템인 하둡과 스파크에 대해 알아보았다. 끝으로 두 시스템의 차이점을 확인해보자.\n",
    "\n",
    "[![ Hadoop vs Spark](http://img.youtube.com/vi/xDpvyu0w0C8/0.jpg)](https://youtu.be/xDpvyu0w0C8) \n",
    "\n",
    "* 성능 측면에서 스파크는 하둡에 비해 처리속도가 빨라졌다. 그 이유는 스파크의 in memory 기반의 데이터 처리방식이 real-time분석을 가능하게 해 주었기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-personality",
   "metadata": {},
   "source": [
    "## 5. Spark 데이터 처리: RDD\n",
    "하둡은 주 언어가 Java기반이고 스파크는 주 언어가 Scala이다. 그리고 기본적으로 클러스터 환경에서 동작하는 프로그램이기 때문에 AWS, Microsoft Azure 혹은 GCP(Google Cloud Platform)등의 클라우드 환경에서 주로 사용한다.\n",
    "\n",
    "<img src=\"./image/pyspark.png\" alt=\"pyspark\" />\n",
    "\n",
    "그러나 일부 기능에 한해서는 Local에서도 실습이 가능하고, 스파크의 경우 파이썬을 지원하기 때문에 몇가지 기본 개념은 실습이 가능하다. 따라서 여기서는 스파크에서 제공하는 파이썬 API인 PySpark를 이용해서 연습해볼 것이다.\n",
    "\n",
    "우선 PySpark를 사용하기 전에 Spark의 동작 원리에 대해서 간단히 살펴보도록 하자. 아래는 마태 자하리아의 논문에서 RDD와 Spark에 대해 소개하는 부분이다.\n",
    "\n",
    "<img src=\"./image/abstract.png\" alt=\"abstract\" />\n",
    "\n",
    "파란색으로 표시한 부분을 읽어보자.\n",
    "\n",
    "> _We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks._\n",
    "\n",
    "즉, 스파크는 RDD(Resilient Distributed Dataset)를 구현하기 위한 프로그램이다. RDD를 스파크라는 프로그램을 통해 실행 시킴으로써 메모리기반의 대량의 데이터 연산이 가능하게 되었고 이는 하둡보다 100배는 빠른 연산을 가능하게 해 주었다.\n",
    "\n",
    "따라서 오늘은 스파크 기본 개념으로 RDD와 스파크가 어떻게 실행되는지(Spark Execution)에 대해 알아보도록 하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-frequency",
   "metadata": {},
   "source": [
    "### 1) RDD 등장 배경\n",
    "---\n",
    "우선 하둡 기반의 데이터 처리 방법과 스파크 기반의 데이터 처리 방법을 비교해보자.\n",
    "\n",
    "<img src=\"./image/rdd.png\" alt=\"RDD\" />\n",
    "\n",
    "하둡은 파일을 디스크에 저장한 뒤 그걸 불러와 연산(주로 맵리듀스 연산)하고 다시 디스크에 저장하면서 파일 처리 작업을 수행한다. 모든 연산마다 디스크에 파일을 읽고 불러오니 디스크에서 파일을 읽고 쓰는 데 시간이 너무 오래 걸린다. 즉, __I/O 바운드가 하둡의 주요 병목현상__인 것이다.\n",
    "\n",
    "이것을 해결하기 위해 스파크는 하드디스크에서 파일을 읽어온 뒤 연산 단계에는 데이터를 메모리에 저장하자는 아이디어를 생각해냈다. 그랬더니 속도가 매우 빨라졌다. 그런데 메모리는 태생이 비휘발성이다. 뭔가 메모리에 적재하기 좋은 새로운 형태의 추상화 작업(abstraction)이 필요하다. 그렇게 고안된 것이 바로 RDD(Resilient Distributed Dataset), \"탄력적 분산 데이터셋\"이다.\n",
    "\n",
    "> _정리하면, 탄력적 분산 데이터셋 (RDD: Resilient Distributed Dataset)는 스파크에서 사용하는 기본 추상개념으로 클러스터의 머신(노드)의 여러 메모리에 분산하여 저장할 수 있는 데이터의 집합을 의미한다._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-wheat",
   "metadata": {},
   "source": [
    "### 2) RDD의 특징\n",
    "---\n",
    "메모리에 저장된 데이터가 중간에 데이터가 유실되면 어떻게 할까? 다시 말해, 결함(fault)이 생기면 어떻게 할까? 스파크는 새로운 방법을 고안한다. 메모리의 데이터를 읽기 전용(Read-Only, 변경 불가)로 만든다. 그리고 데이터를 만드는 방법을 기록하고 있다가 데이터가 유실되면 다시 만드는 방법을 사용한다. 이를 데이터 만드는 방법 즉, 계보(Lineage)를 저장한다.\n",
    "\n",
    "RDD의 특징을 단어로 표현하면 다음과 같다.\n",
    "\n",
    "* In-Memory\n",
    "* Fault Tolerance\n",
    "* Immutable(Read-Only)\n",
    "* Partition [파티션]\n",
    "\n",
    "(각 파티션은 RDD의 전체 데이터 중 일부를 나타낸다. 스파크는 데이터를 여러 대의 머신에 분할해서 저장하며, Chunk, 혹은 파티션으로 분할되어 저장한다. 파티션을 RDD데이터의 부분을 표현하는 단위 정도로 이해하면 된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-syria",
   "metadata": {},
   "source": [
    "## 6. RDD의 생성과 동작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-improvement",
   "metadata": {},
   "source": [
    "### RDD 생성(Creation)\n",
    "---\n",
    "RDD를 만드는 방법에는 두 가지가 있다.\n",
    "\n",
    "* 내부에서 만들어진 데이터 집합을 병렬화하는 방법: `parallelize()`함수 사용\n",
    "* 외부의 파일을 로드하는 방법: `.textFile()` 함수 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-sally",
   "metadata": {},
   "source": [
    "### RDD 동작(Operation)\n",
    "---\n",
    "RDD의 동작은 크게 Transformations와 Actions 두 가지이다.\n",
    "\n",
    "* Transformations\n",
    "* Actions\n",
    "\n",
    "각각의 동작들에 해당하는 함수들은 다음과 같다.\n",
    "\n",
    "<img src=\"./image/functions.png\" alt=\"functions\" />\n",
    "\n",
    "RDD는 immutable(불변)하다고 하였다. 따라서 연산 수행에 있어 기존의 방식과는 다르게 수행되는데, Transformations은 RDD에게 변형 방법(연산 로직, 계보, lineage)을 알려주고 새로운 RDD를 만든다. 그러나 실제 연산의 수행은 Actions을 통해 행해진다.\n",
    "\n",
    "이를 도식화하면 다음과 같다.\n",
    "\n",
    "<img src=\"./image/operation.png\" alt=\"RDD Operation\" />\n",
    "\n",
    "Transformations를 통해 새로운 RDD를 만든다. actions은 결괏값을 보여주고 저장하는 역할을 하며 실제 Transformations연산을 지시하기도 한다.\n",
    "\n",
    "<img src=\"./image/transformations.png\" alt=\"Transformations\" />\n",
    "\n",
    "다음 그림은 RDD의 생성, transformations동작, actions동작을 도식화한 그림이다.\n",
    "\n",
    "<img src=\"./image/rdd2.png\" alt=\"RDD\" />\n",
    "\n",
    "`sc.textFile()`을 통해 RDD를 생성한다.\n",
    "\n",
    "그러나 이 작업은 실제로는 RDD의 lineage(계보)를 만드는데 지나지 않는다. 실제 객체는 생성되지 않았다. 그리고 transformations함수 중 하나인 `filter()`를 만든다. 이 역시 lineage를 만드는 일에 지나지 않는다.\n",
    "\n",
    "실제 RDD가 생성되는 시점은 __Actions__의 함수인 `counts()`를 실행할 때이다.\n",
    "\n",
    "이런 식으로 결괏값이 필요할 때까지 계산을 늦추다가 정말 필요한 시기에 계산을 수행하는 방법을 __느긋한 계산법(Lazy evaluation)__이라고 한다.\n",
    "\n",
    "참고 : [위키피디아-느긋한 계산법](https://ko.wikipedia.org/wiki/%EB%8A%90%EA%B8%8B%ED%95%9C_%EA%B3%84%EC%82%B0%EB%B2%95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-contribution",
   "metadata": {},
   "source": [
    "### 정리\n",
    "---\n",
    "__RDD를 생성(Creation)하는 방법은 2가지가 있다.__\n",
    "* 내부에서 만들어진 데이터 집합을 병렬화하는 방법: `parallelize()` 함수 사용\n",
    "* 외부의 파일을 로드하는 방법: `textFile()` 함수 사용\n",
    "\n",
    "__RDD 동작(Operation)에는 크게 2가지가 있다.__\n",
    "* transformations\n",
    "* actions\n",
    "\n",
    "__RDD의 transformations와 actions에 해당하는 함수들__\n",
    "\n",
    "<img src=\"./image/functions.png\" alt=\"functions\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-static",
   "metadata": {},
   "source": [
    "## 7. PySpark 설치하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-victoria",
   "metadata": {},
   "source": [
    "### 1) PySpark 설치\n",
    "---\n",
    "PySpark를 이용하기 위해선 다음과 같은 패키지가 설치되어 있어야 한다.\n",
    "\n",
    "* java (>= 8.0)\n",
    "* Spark (>= 2.2.0)\n",
    "* Python (>= 3.4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-davis",
   "metadata": {},
   "source": [
    "#### <자바 버전 확인>\n",
    "\n",
    "1) 자바 버전을 확인한다.\n",
    "\n",
    "```bash\n",
    "$ sudo apt-get update\n",
    "$ java -version\n",
    "```\n",
    "\n",
    "1-1) 만약 자바가 설치되어 있지 않으면 자바를 설치한다.\n",
    "\n",
    "```bash\n",
    "$ sudo apt-get install openjdk-8-jdk-headless -y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-hardwood",
   "metadata": {},
   "source": [
    "#### <스파크 설치>\n",
    "\n",
    "2) 터미널을 열어 아래 명령어를 통해 spark를 다운로드할 수 있다.\n",
    "\n",
    "```bash\n",
    "$ wget https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "```\n",
    "\n",
    "혹은, 아래 홈페이지에 가서 직접 다운로드받는다.\n",
    "\n",
    "\n",
    "[홈페이지 바로가기](https://www.apache.org/dyn/closer.lua/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz)\n",
    "\n",
    "3) tgz파일의 압축을 풉니다.\n",
    "\n",
    "```bash\n",
    "$ tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-palace",
   "metadata": {},
   "source": [
    "#### <스파크 실행>\n",
    "\n",
    "4) 스파크가 설치된 폴더로 이동한다.\n",
    "\n",
    "```bash\n",
    "$ cd spark-3.0.1-bin-hadoop2.7\n",
    "```\n",
    "\n",
    "5)bin폴더로 이동한다.\n",
    "\n",
    "```bash\n",
    "$ cd bin\n",
    "```\n",
    "\n",
    "6) spark실행 명령어 `./spark-shell` 를 입력한다.\n",
    "\n",
    "```bash\n",
    "$ ./spark-shell\n",
    "```\n",
    "\n",
    "7) 실행화면\n",
    "\n",
    "<img src=\"./image/terminal.png\" alt=\"terminal\" width=\"80%\"/>\n",
    "\n",
    "정상 설치되었다. pyspark를 설치해 보도록 하자.\n",
    "\n",
    "```bash\n",
    "$ pip install pyspark\n",
    "```\n",
    "\n",
    "잘 설치되었다면, 버전을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "looking-thermal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:05:39.974166Z",
     "start_time": "2021-03-05T02:05:39.914763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-edgar",
   "metadata": {},
   "source": [
    "## 8. SparkContext를 통한 스파크 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-basketball",
   "metadata": {},
   "source": [
    "### 스파크의 엔트리포인트 SparkContext 객체선언\n",
    "---\n",
    "분산환경에서 운영되는 스파크는 driver프로그램을 구동시킬 때 SparkContext라는 특수 객체를 만들게 된다. 스파크는 이 SparkContext 객체를 통해 스파크의 모든 기능에 접근한다. 이 객체는 스파크 프로그램당 한 번만 실행할 수 있고 사용 후에는 종료해야 한다. 따라서 SparkContext를 다른 말로 스파크의 \"엔트리 포인트(entry point)\"라고도 하고, SparkContext를 생성하는 것을 \"스파크를 초기화한다(Initializing Spark)\" 라고 한다.\n",
    "\n",
    "<img src=\"./image/cluster.png\" alt=\"cluster\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-coverage",
   "metadata": {},
   "source": [
    "### PySpark에서는…\n",
    "---\n",
    "PySpark에서 선언하는 SparkContext객체는 내부의 JVM(Java Virtual Machine) 위에 동작하는 Py4J의 SparkContext와 연결된다. 이 Py4J의 SparkContext는 Worker노드들과도 연결되어 있고 이 Worker노드들 역시 실제 동작은 JVM 위에서 동작한다. Worker노드들의 조작 역시 Python을 통해 할 수 있지만, 실제 동작은 다 JVM 위에서 행해진다. 그러니깐 사실 PySpark는 Python으로 코딩을 하긴 하지만 실제 동작은 JVM에 의해 행해지고 있는 것이다.\n",
    "\n",
    "<img src=\"./image/jvm.png\" alt=\"JVM\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-chester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:12:10.969036Z",
     "start_time": "2021-03-05T02:12:10.966593Z"
    }
   },
   "source": [
    "### 공식문서 확인\n",
    "---\n",
    "다음은 PySpark에서 설명하는 SparkContext객체 사양이다.\n",
    "\n",
    "<img src=\"./image/SparkContext.png\" alt=\"SparkContext\" />\n",
    "\n",
    "위 문서를 통해 확인할 수 있는 SparkContext의 주요 내용은 다음과 같다.\n",
    "\n",
    "* 문법: pyspark.SparkContext()\n",
    "* 스파크기능의 기본 엔트리포인트이다.\n",
    "* 스파크 클러스터와 연결을 나타내며 RDD를 만들고 브로드캐스트하는데 사용될 수 있다.\n",
    "* JVM 당 하나만 활성화해야 하며, 새로운 것을 만들기 전에는 활성을 중지해야 한다.\n",
    "\n",
    "그럼 코드로 구현해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-issue",
   "metadata": {},
   "source": [
    "### 실습\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-ribbon",
   "metadata": {},
   "source": [
    "#### code-1\n",
    "모듈을 import한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "useful-beverage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:13:41.090998Z",
     "start_time": "2021-03-05T02:13:38.478935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.35.155:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-valentine",
   "metadata": {},
   "source": [
    "보통 변수명은 sc로 선언한다.\n",
    "\n",
    "SparkContext객체 타입을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cross-necklace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:14:42.046341Z",
     "start_time": "2021-03-05T02:14:42.043357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-halloween",
   "metadata": {},
   "source": [
    "잘 만들어졌다\n",
    "\n",
    "하지만 실수로 SparkContext를 한 개 더 만들면 에러가 난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "special-envelope",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:15:05.916921Z",
     "start_time": "2021-03-05T02:15:05.861300Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-5-09e2fe521565>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7fba5e91dae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#에러 발생을 확인하세요!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 347\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-5-09e2fe521565>:3 "
     ]
    }
   ],
   "source": [
    "#에러 발생을 확인하세요!\n",
    "new_sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-pillow",
   "metadata": {},
   "source": [
    "SparkContext를 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "attempted-polls",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:15:23.662840Z",
     "start_time": "2021-03-05T02:15:23.289579Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-spread",
   "metadata": {},
   "source": [
    "#### code-2\n",
    "SparkContext의 Configuration을 세팅할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "modified-myrtle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:15:40.051814Z",
     "start_time": "2021-03-05T02:15:39.939825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.35.155:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext(master='local', appName='PySpark Basic')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-makeup",
   "metadata": {},
   "source": [
    "Spark Web-UI 버전을 이용하여 로컬 모드에서 사용할 경우 master url은 http://localhost:8080 으로 설정한다. 주피터 노트북에서 실행할 것이므로 위와 같이 master=`local`로 설정하면 된다. 상세 내용은 아래 공식 홈페이지의 설명을 참고.\n",
    "\n",
    "[spark 공식 홈페이지](https://spark.apache.org/docs/latest/spark-standalone.html)\n",
    "\n",
    "생성한 SparkContext의 Configuration을 확인하기 위해서 .`getConf()`. `getAll()`을 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "whole-clock",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:17:55.499364Z",
     "start_time": "2021-03-05T02:17:55.483521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.startTime', '1614910539949'),\n",
       " ('spark.driver.port', '38357'),\n",
       " ('spark.app.name', 'PySpark Basic'),\n",
       " ('spark.driver.host', '192.168.35.155'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1614910540023'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-quarterly",
   "metadata": {},
   "source": [
    "하나씩도 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "separate-freeware",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:18:13.810841Z",
     "start_time": "2021-03-05T02:18:13.807895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "continent-produce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:18:19.765551Z",
     "start_time": "2021-03-05T02:18:19.762288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySpark Basic'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-conjunction",
   "metadata": {},
   "source": [
    "SparkContext를 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "regulation-august",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:18:26.699947Z",
     "start_time": "2021-03-05T02:18:26.269888Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-tuning",
   "metadata": {},
   "source": [
    "#### code-3\n",
    "`SparkConf()`를 이용해 SparkContext의 Configuration을 설정하는 방법을 사용해서 SparkContext를 만들 수 있다.\n",
    "\n",
    "`.setMaster()`, `.setAppName()`을 이용해 어플리케이션의 이름과 Master의 URL을 설정해 줄 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "seeing-fossil",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:19:14.272720Z",
     "start_time": "2021-03-05T02:19:14.174053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.35.155:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('PySpark Basic').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-banks",
   "metadata": {},
   "source": [
    "3가지 방법 모두 많이 사용하는 코딩 스타일이니 눈에 익혀 두자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-murder",
   "metadata": {},
   "source": [
    "## 9. RDD Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-satellite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:21:24.504175Z",
     "start_time": "2021-03-05T02:21:24.501385Z"
    }
   },
   "source": [
    "### 1) 내부에서 만들어진 데이터 집합을 병렬화하는 방법: parallelize()함수 사용\n",
    "---\n",
    "방금 전 만든 `SparkContext()`의 `parallelize()`함수를 이용해서 내부의 데이터 집합을 RDD로 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "complex-daisy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:21:36.624607Z",
     "start_time": "2021-03-05T02:21:36.363582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-evidence",
   "metadata": {},
   "source": [
    "뭔가 이상하다. 그냥 설명만 나오고 있다..\n",
    "\n",
    "데이터 타입을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "choice-future",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:22:05.268906Z",
     "start_time": "2021-03-05T02:22:05.266353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-scratch",
   "metadata": {},
   "source": [
    "정상적으로 생성은 된 것 같다.\n",
    "\n",
    "RDD는 생성과 transformations 연산을 바로 수행하지 않는다. 이 단계에서는 연산을 하고 있진 않고 계보(lineage)만 만들어 놓고 Actions 동작을 할 때 RDD가 비로서 만들어지며, 이를 느긋한 계산법이라 위에서 언급했다.\n",
    "\n",
    "<img src=\"./image/rdd2.png\" alt=\"RDD\" />\n",
    "\n",
    "(단, 여기서는 RDD를 만들 때 `.textFile()`대신 `.parallelize()`를 사용하고 있다.)\n",
    "\n",
    "따라서! 지금은 RDD를 생성했지만 RDD가 만들어지지는 않았다.\n",
    "\n",
    "Actions를 해보자.\n",
    "\n",
    "함수는 `take()` 라는 함수를 사용해보자. RDD의 원소를 반환한다. 인자로는 반환하고자 하는 RDD의 원소의 개수를 입력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "compliant-cologne",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:24:55.898380Z",
     "start_time": "2021-03-05T02:24:55.115570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-inspector",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:25:30.675791Z",
     "start_time": "2021-03-05T02:25:30.673410Z"
    }
   },
   "source": [
    "데이터가 확인되었다.\n",
    "\n",
    "`take()` 함수 스펙이다.\n",
    "\n",
    "<img src=\"./image/take.png\" alt=\"take()\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-addiction",
   "metadata": {},
   "source": [
    "### 2) 외부의 파일을 로드하는 방법: .textFile() 함수 사용\n",
    "---\n",
    "`.textFile()`함수를 이용해 외부 파일을 로드하여 RDD를 만들 수 있다.\n",
    "\n",
    "우선 간단히 파일을 하나 만들어보자. 먼저 아래와 같이 파일 작업 환경을 만들자.\n",
    "\n",
    "```bash\n",
    "$ mkdir -p ~/AIFFEL/Fundamentals/F26_big_data/bigdata_ecosystem\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tired-thumbnail",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:31:17.034144Z",
     "start_time": "2021-03-05T02:31:17.030568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = os.getenv('HOME')+'/AIFFEL/Fundamentals/F26_big_data/bigdata_ecosystem/test.txt'\n",
    "with open(file_path, 'w') as f:\n",
    "    for i in range(10):\n",
    "        f.write(str(i)+'\\n')\n",
    "        \n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-message",
   "metadata": {},
   "source": [
    "방금 만든 파일을 불러와 RDD를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fewer-madness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:31:54.148771Z",
     "start_time": "2021-03-05T02:31:54.008462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj10/AIFFEL/Fundamentals/F26_big_data/bigdata_ecosystem/test.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.textFile(file_path)\n",
    "print(rdd2)\n",
    "print(type(rdd2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-likelihood",
   "metadata": {},
   "source": [
    "Actions를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "loaded-freedom",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:32:09.371657Z",
     "start_time": "2021-03-05T02:32:09.088927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-puzzle",
   "metadata": {},
   "source": [
    "한 가지 특이사항이 있다. 숫자를 입력했으므로 위의 결과가 `[0, 1, 2]`가 될 줄 알았는데, 문자열의 list가 얻어졌다. 이것은 spark가 `.textFile()`을 통해 얻어온 데이터 타입을 무조건 string으로 처리하기 때문에 그렇다. 그렇다면 이 데이터를 숫자로 변환하려면 어떻게 해야 할까? 바로 다음 스텝에 나오는 Transformation 같은 RDD Operation이 필요한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-calculator",
   "metadata": {},
   "source": [
    "## 10. RDD Operation (1) Transformations\n",
    "이어서 RDD동작에 대해 연습해 보도록 하겠다.\n",
    "\n",
    "RDD의 연산은 NumPy연습하듯이 자주 사용되는 함수들을 간단한 예제코드를 통해 학습하는 것이 좋다. 우선 기본이 되는 몇 가지 동작을 설명하여 RDD연산과정을 확인해 보고 동영상 자료를 통해 여러 가지 예제를 익혀보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-burst",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "---\n",
    "* map()\n",
    "* filter()\n",
    "* flatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-roman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:36:31.349163Z",
     "start_time": "2021-03-05T02:36:31.345727Z"
    }
   },
   "source": [
    "#### map\n",
    "\n",
    "<img src=\"./image/map.png\" alt=\"map()\" />\n",
    "\n",
    "x의 모든 원소에 대해 map함수를 적용한 결과는 y값이다. 따라서 x와 y의 원소 개수는 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "previous-march",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:37:05.386974Z",
     "start_time": "2021-03-05T02:37:05.304043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a', 'c', 'd']\n",
      "[('b', 1), ('a', 1), ('c', 1), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([\"b\", \"a\", \"c\", \"d\"])\n",
    "y = x.map(lambda z: (z, 1))\n",
    "print(x.collect()) #collect()는 actions입니다. \n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-blanket",
   "metadata": {},
   "source": [
    "다른 예제를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "opening-stick",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:37:22.570709Z",
     "start_time": "2021-03-05T02:37:22.531114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3])\n",
    "squares = nums.map(lambda x: x*x)\n",
    "print(squares.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-prison",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:37:56.398737Z",
     "start_time": "2021-03-05T02:37:56.396421Z"
    }
   },
   "source": [
    "#### filter\n",
    "\n",
    "<img src=\"./image/filter.png\" alt=\"filter()\" />\n",
    "\n",
    "filter연산은 어떤 조건을 만족하는 값만을 반환한다. 따라서 조건문이 들어가야 하며, x와 y의 원소의 개수는 같지 않을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "falling-thong",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:38:56.172063Z",
     "start_time": "2021-03-05T02:38:56.102999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5])\n",
    "y = x.filter(lambda x: x%2 == 0) \n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-portable",
   "metadata": {},
   "source": [
    "다른 예제를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "regulation-lotus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:39:13.772857Z",
     "start_time": "2021-03-05T02:39:13.719411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd']\n",
      "['A']\n"
     ]
    }
   ],
   "source": [
    "text = sc.parallelize(['a', 'b', 'c', 'd'])\n",
    "capital = text.map(lambda x: x.upper())\n",
    "A = capital.filter(lambda x: 'A' in x)\n",
    "print(text.collect())\n",
    "print(A.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-barrel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:41:35.421636Z",
     "start_time": "2021-03-05T02:41:35.418684Z"
    }
   },
   "source": [
    "#### flatmap\n",
    "다음은 flatmap의 연산과정을 나타낸 그림이다.\n",
    "\n",
    "<img src=\"./image/flatmap.png\" alt=\"flatmap\" />\n",
    "<img src=\"./image/flatmap2.png\" alt=\"flatmap\" />\n",
    "<img src=\"./image/flatmap3.png\" alt=\"flatmap\" />\n",
    "<img src=\"./image/flatmap4.png\" alt=\"flatmap\" />\n",
    "\n",
    "FlatMap은 RDD의 원소에 map연산을 수행하고 원소의 개수를 증가시키기도 한다. 원소의 개수는 꼭 동일하게 증가시키지 않아도 된다.\n",
    "\n",
    "<img src=\"./image/flatmap5.png\" alt=\"flatmap\" />\n",
    "\n",
    "다음 예제들을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "protective-weapon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:42:21.652050Z",
     "start_time": "2021-03-05T02:42:21.591886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 10, 30, 2, 20, 30, 3, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, x*10, 30))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-request",
   "metadata": {},
   "source": [
    "아래 예제는 RDD transformation 몇 가지를 1개 라인에 중첩 적용한 경우이다. 언뜻 복잡해 보이겠지만 적용된 transformation 함수의 효과를 하나씩 따로 적용해서 collect()로 결과를 확인해 보면 어렵지 않게 파악 가능할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "conservative-fever",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:42:50.550058Z",
     "start_time": "2021-03-05T02:42:50.515031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'is',\n",
       " 'funny',\n",
       " 'it',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'and',\n",
       " 'also',\n",
       " 'it',\n",
       " 'is',\n",
       " 'implemented',\n",
       " 'by',\n",
       " 'python']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsDataset = sc.parallelize([\"Spark is funny\", \"It is beautiful\", \"And also It is implemented by python\"])\n",
    "result = wordsDataset.flatMap(lambda x: x.split()).filter(lambda x: x != \" \").map(lambda x: x.lower())\n",
    "# 공백은 제거합니다.\n",
    "# 단어를 공백기준으로 split 합니다. \n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-capitol",
   "metadata": {},
   "source": [
    "### 심화 : CSV 파일 읽어들이기\n",
    "---\n",
    "위에서 살펴본 RDD transformation 함수들을 토대로 좀 더 실전적인 예제를 다루어보자. 우리가 다루는 많은 데이터들은 주로 CSV 파일로 되어 있다. 유명한 Titanic 데이터셋 파일을 스파크로 읽어 들이는 것을 연습해보자. 우선 작업환경에 아래와 같이 CSV 파일을 다운받아보자.\n",
    "\n",
    "```bash\n",
    "$ wget https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
    "$ mv train.csv ~/AIFFEL/Fundamentals/F26_big_data/bigdata_ecosystem\n",
    "```\n",
    "\n",
    "파일을 불러들여 RDD를 생성하는 방법은 이미 알고 있다. `sc.textFile()`를 아래와 같이 활용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "irish-favor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:45:40.844689Z",
     "start_time": "2021-03-05T02:45:40.783988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone',\n",
       " '0,male,22.0,1,0,7.25,Third,unknown,Southampton,n',\n",
       " '1,female,38.0,1,0,71.2833,First,C,Cherbourg,n',\n",
       " '1,female,26.0,0,0,7.925,Third,unknown,Southampton,y',\n",
       " '1,female,35.0,1,0,53.1,First,C,Southampton,n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "csv_path = os.getenv('HOME')+'/AIFFEL/Fundamentals/F26_big_data/bigdata_ecosystem/train.csv'\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "csv_data_0.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-quality",
   "metadata": {},
   "source": [
    "파일을 그대로 읽어서 상위 5라인만 출력해 보았다. 이것을 데이터셋으로 만들려면, 1번째 라인의 컬럼 부분을 분리해 내고, 매 데이터 라인마다 [(column1, 데이터1), (column2, 데이터2)…] 의 list 형태로 바꿔 주고 싶다. 좀 더 데이터를 가공해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "useful-syndicate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:46:30.957312Z",
     "start_time": "2021-03-05T02:46:30.921529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비어있는 라인은 제외하고, delimeter인 ,로 line을 분리해 줍니다. \n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "csv_data_1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-sheriff",
   "metadata": {},
   "source": [
    "컬럼 부분만 아래와 같이 분리해 낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "successful-chemical",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:46:56.421296Z",
     "start_time": "2021-03-05T02:46:56.386376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = csv_data_1.take(1)\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-passenger",
   "metadata": {},
   "source": [
    "컬럼을 제외한 나머지 데이터만 분리해 낼 방법이 필요하다. 데이터의 첫 번째 컬럼은 0 또는 1의 숫자로만 이루어져 있다. 이 조건을 filter로 활용하면 컬럼 부분을 제외할 수 있을 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "interior-parts",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:47:17.454353Z",
     "start_time": "2021-03-05T02:47:17.416480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '28.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8.4583',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Queenstown',\n",
       "  'y']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data_2 = csv_data_1.filter(lambda line: line[0].isdecimal())  # 첫 번째 컬럼이 숫자인 것만 필터링\n",
    "csv_data_2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-segment",
   "metadata": {},
   "source": [
    "거의 다 왔습니다. 이제 컬럼 기준으로 `csv_data_2`를 정리해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "vietnamese-popularity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:47:37.031464Z",
     "start_time": "2021-03-05T02:47:36.989345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '35.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '53.1'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '28.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '8.4583'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Queenstown'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data_3 = csv_data_2.map(lambda line: [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "csv_data_3.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-classics",
   "metadata": {},
   "source": [
    "이제 원하는 형태로 CSV 파일이 가공되었다. 이 형태라면 다음 스텝에 나올 다양한 Action 함수를 적용하여 다양하게 분석해 볼 수 있을 것 같다.\n",
    "\n",
    "하지만 CSV 파일을 꼭 이렇게 가공해야 할까? 꼭 그렇지 않다. 마지막으로, CSV 파일을 DataFrame으로 읽어 들이는 방법을 소개하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "pleasant-valuable",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:49:56.609316Z",
     "start_time": "2021-03-05T02:49:52.777847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class|deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|0       |male  |22.0|1                 |0    |7.25   |Third|unknown|Southampton|n    |\n",
      "|1       |female|38.0|1                 |0    |71.2833|First|C      |Cherbourg  |n    |\n",
      "|1       |female|26.0|0                 |0    |7.925  |Third|unknown|Southampton|y    |\n",
      "|1       |female|35.0|1                 |0    |53.1   |First|C      |Southampton|n    |\n",
      "|0       |male  |28.0|0                 |0    |8.4583 |Third|unknown|Queenstown |y    |\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "\n",
    "url = 'https://storage.googleapis.com/tf-datasets/titanic/train.csv'\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.csv(SparkFiles.get(\"train.csv\"), header=True, inferSchema= True)\n",
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-community",
   "metadata": {},
   "source": [
    "지금까지 우리가 사용했던 SparkContext를 한 번 더 가공한 SQLContext에서 제공하는 `read.csv()` 함수를 이용하면 스파크의 DataFrame을 얻을 수 있다. 이것은 우리에게 익숙한 Pandas의 DataFrame을 떠올리게 한다. 아주 똑같은 것은 아니지만 아주 유사하게 이용할 수 있다. 실제로 SQLContext에는 RDD를 이용해 데이터를 분석하는 것보다 훨씬 편리하고 강력한 기능들을 많이 제공하고 있다.\n",
    "\n",
    "자세한 내용은 아래 링크를 참고.\n",
    "\n",
    "[Spark SQL, DataFrames and Datasets Guidel](https://spark.apache.org/docs/1.6.1/sql-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "molecular-monkey",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:57:50.715706Z",
     "start_time": "2021-03-05T02:57:50.494207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class |deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "|0       |male  |66.0|0                 |0    |10.5   |Second|unknown|Southampton|y    |\n",
      "|0       |male  |42.0|1                 |0    |52.0   |First |unknown|Southampton|n    |\n",
      "|1       |female|49.0|1                 |0    |76.7292|First |D      |Cherbourg  |n    |\n",
      "|0       |male  |65.0|0                 |1    |61.9792|First |B      |Cherbourg  |n    |\n",
      "|0       |male  |45.0|1                 |0    |83.475 |First |C      |Southampton|n    |\n",
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 위에서 얻은 데이터에서 40세 이상인 사람들의 데이터만 필터링해 봅시다. \n",
    "df2 = df[df['age']>40]\n",
    "df2.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-local",
   "metadata": {},
   "source": [
    "#### RDD - Transformations 더 많은 함수들\n",
    "더 많은 Transformations함수들이 있다.\n",
    "\n",
    "[RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-marks",
   "metadata": {},
   "source": [
    "## 11. RDD Operation (2) Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-consultancy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T02:59:43.099807Z",
     "start_time": "2021-03-05T02:59:43.095895Z"
    }
   },
   "source": [
    "### Actions\n",
    "---\n",
    "* collect()\n",
    "* take()\n",
    "* count()\n",
    "* reduce()\n",
    "* saveAsTextFile()\n",
    "\n",
    "`collect()`, `take()`는 이미 몇 번 사용해 봤던 익숙한 Action들이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-duplicate",
   "metadata": {},
   "source": [
    "#### collect\n",
    "RDD 내의 모든 값을 리턴한다. 정말 __빅__데이터를 다루고 있다면 함부로 호출하지 않는 게 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "found-field",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:00:19.105115Z",
     "start_time": "2021-03-05T03:00:19.078937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize(list(range(10)))\n",
    "nums.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-orientation",
   "metadata": {},
   "source": [
    "#### take\n",
    "RDD에서 앞쪽 n개의 데이터의 list를 리턴한다. `collect()`보다는 안전하게 데이터를 확인해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "listed-precipitation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:00:45.844168Z",
     "start_time": "2021-03-05T03:00:45.804943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-burden",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:00:57.294138Z",
     "start_time": "2021-03-05T03:00:57.291464Z"
    }
   },
   "source": [
    "#### count\n",
    "RDD에 포함된 데이터 개수를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "atmospheric-anger",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:01:09.029799Z",
     "start_time": "2021-03-05T03:01:08.997570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-isaac",
   "metadata": {},
   "source": [
    "#### reduce\n",
    "\n",
    "드디어 `reduce()` 함수가 나왔다. MapReduce의 그 reduce에 해당한다.<br>\n",
    "MapReduce를 스파크에서 어떻게 구현했는지 알 수 있다. Map은 Transformation 함수로, Reduce는 Action 함수로 구현했다. Reduce할 데이터가 RDD로 메모리상에 존재하므로 이전의 다른 구현체보다 훨씬 빠르게 MapReduce 를 실행할 수 있을 것이다.\n",
    "\n",
    "아래는 RDD의 모든 데이터를 차례차례 더하는 `sum()`을 구현한 예시이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dirty-stage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:02:05.945991Z",
     "start_time": "2021-03-05T03:02:05.918267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-savage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:02:25.842338Z",
     "start_time": "2021-03-05T03:02:25.839760Z"
    }
   },
   "source": [
    "#### saveAsTextFile\n",
    "\n",
    "RDD 데이터를 파일로 저장한다. 아래 코드를 실행하면 `file.txt`라는 파일에 RDD 내용이 저장될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "labeled-wellington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:03:03.129043Z",
     "start_time": "2021-03-05T03:03:02.876328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\r\n",
      "drwxr-xr-x 2 aiffel-dj10 aiffel-dj10  4096  3월  5 12:03 file.txt\r\n",
      "-rw-r--r-- 1 aiffel-dj10 aiffel-dj10    20  3월  5 11:31 test.txt\r\n",
      "-rw-r--r-- 1 aiffel-dj10 aiffel-dj10 30874  2월 21  2019 train.csv\r\n"
     ]
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/AIFFEL/Fundamentals/F26_big_data/bigdata_ecosystem/file.txt'\n",
    "nums.saveAsTextFile(file_path)\n",
    "\n",
    "!ls -l ~/AIFFEL/Fundamentals/F26_big_data/bigdata_ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-words",
   "metadata": {},
   "source": [
    "`file.txt` 내용을 잘 보면, 놀랍게도 디렉토리 타입으로 생성되어 있다. 이 디렉토리 안에 들어가 보면 `part-00000` 라는 이름의 텍스트 파일이 생성되어 있어서, 실제 우리가 기록하고 싶었던 내용은 이 파일 안에 있다.\n",
    "\n",
    "왜 이런 일이 생겼을까?\n",
    "\n",
    "우리가 다루고 있는 스파크가 바로 분산형 빅데이터 시스템이라는 것을 잊어서는 안 된다. 스파크가 다룰 파일 사이즈는 하드디스크 하나에 다 담지 못할 만큼 큰 경우일 수도 있다. 비록 작은 RDD지만 우리는 이미 `sc.parallelize()`를 통해 분산형 데이터로 생성했음을 잊지말아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-warehouse",
   "metadata": {},
   "source": [
    "#### RDD - Actions\n",
    "\n",
    "[RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-marriage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:06:20.779823Z",
     "start_time": "2021-03-05T03:06:20.777051Z"
    }
   },
   "source": [
    "#### 요약\n",
    "RDD의 전체 과정을 한번 한번 복습해보자\n",
    "\n",
    "<img src=\"./image/operation.png\" alt=\"RDD Operation\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "every-delay",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:07:18.070572Z",
     "start_time": "2021-03-05T03:07:18.028910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD 생성\n",
    "rdd = sc.parallelize(range(1,100))\n",
    "\n",
    "# RDD Transformation \n",
    "rdd2 = rdd.map(lambda x: 0.5*x - 10).filter(lambda x: x > 0)\n",
    "\n",
    "# RDD Action \n",
    "rdd2.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-rebel",
   "metadata": {},
   "source": [
    "## 12. RDD Operation (3) 실습:MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-tenant",
   "metadata": {},
   "source": [
    "### 1) Word Counter 구현하기\n",
    "---\n",
    "MapReduce 개념을 처음 배울 때 항상 다뤄보게 되는 Word Counter 구현하기 문제이다. 이번에는 스파크 RDD 함수를 이용해 구현해보자.\n",
    "\n",
    "* Map 함수를 구현할 때, 입력 스트링의 각 문자 x에 대해 x -> (x, 1) 형태의 tuple로 매핑하면 수월하다. \n",
    "* 일반적인 reduce 함수보다는 reduceByKey 함수를 사용하는 것을 추천한다. \n",
    "* 좋은 예시로 [여기](https://backtobazics.com/big-data/spark/apache-spark-reducebykey-example/)를 참고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "introductory-jacob",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:10:27.584975Z",
     "start_time": "2021-03-05T03:10:27.236170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('h', 2),\n",
       " ('e', 1),\n",
       " ('l', 2),\n",
       " ('o', 2),\n",
       " ('p', 1),\n",
       " ('y', 1),\n",
       " ('t', 1),\n",
       " ('n', 1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.parallelize('hello python')\n",
    "\n",
    "# map 함수를 적용한 RDD 구하기\n",
    "text_1 = text.filter(lambda x: x != \" \")\n",
    "text_2 = text_1.map(lambda x:(x, 1))\n",
    "\n",
    "#reduceByKey 함수를 적용한 Word Counter 출력\n",
    "word_count = text_2.reduceByKey(lambda accum, n: accum + n)  \n",
    "word_count.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-samba",
   "metadata": {},
   "source": [
    "### 2) Titanic 데이터 분석하기\n",
    "---\n",
    "이제 스파크를 이용해 Titanic 데이터를 분석해보자.  Pandas로 분석하는 것보다는 다소 까다롭겠지만, 엄청난 빅데이터라고 가정한다면 충분히 의미 있는 연습이 될 수 있다.\n",
    "\n",
    "이번에는 타이타닉에서의 생존자와 사망자의 평균연령을 구해 보는 것이다. 둘 사이에 얼마나 차이가 날까?\n",
    "\n",
    "* map 함수를 이용해 모든 데이터를 (생존 여부, 연령)의 형태로 바꾸면 생존자, 사망자 각각의 연령의 총합을 쉽게 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "powered-invalid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:12:25.656959Z",
     "start_time": "2021-03-05T03:12:25.563568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전 스텝에서 CSV 파일을 로딩했던 내역입니다. \n",
    "csv_path = os.getenv('HOME')+'/AIFFEL/Fundamentals/F26_big_data/bigdata_ecosystem/train.csv'\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "columns = csv_data_1.take(1)\n",
    "csv_data_2 = csv_data_1.filter(lambda line: line[0].isdecimal())\n",
    "csv_data_3 = csv_data_2.map(lambda line: [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "\n",
    "csv_data_3.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "primary-testimony",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:12:54.488687Z",
     "start_time": "2021-03-05T03:12:54.275629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생존자 평균 연령: 29.110411522633743\n",
      "사망자 평균 연령: 29.9609375\n"
     ]
    }
   ],
   "source": [
    "# csv_data_3을 가공하여 생존자, 사망자의 연령 총합과 사람 수를 각각 구해 봅시다. \n",
    "# 이후 각각의 데이터로부터 생존자와 사망자의 평균 연령을 구할 수 있습니다. \n",
    "\n",
    "# 생존자와 사망자의 연령 총합 구하기\n",
    "csv_data_4 = csv_data_3.map(lambda line:(line[0][1], line[2][1]))   # (생존여부, 연령)\n",
    "age_sum_data = csv_data_4.reduceByKey(lambda accum, age: float(accum) + float(age))  \n",
    "age_sum = age_sum_data.collect()\n",
    "\n",
    "# 생존자와 사망자의 사람 수 구하기\n",
    "csv_data_5 = csv_data_3.map(lambda line:(line[0][1], 1))\n",
    "survived_data = csv_data_5.reduceByKey(lambda accum, count: int(accum) + int(count)) \n",
    "survived_count = survived_data.collect()\n",
    "\n",
    "age_sum_dict = dict(age_sum)\n",
    "survived_dict = dict(survived_count)\n",
    "avg_age_survived = age_sum_dict['1']/survived_dict['1']\n",
    "print('생존자 평균 연령:' ,avg_age_survived)\n",
    "avg_age_died = age_sum_dict['0']/survived_dict['0']\n",
    "print('사망자 평균 연령:' ,avg_age_died)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
