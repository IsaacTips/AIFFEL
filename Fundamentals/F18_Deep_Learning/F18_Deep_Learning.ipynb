{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec3fff8a",
   "metadata": {},
   "source": [
    "# 18. 딥러닝 들여다보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f77dd0",
   "metadata": {},
   "source": [
    "## 1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75037398",
   "metadata": {},
   "source": [
    "### 학습 목표\n",
    "---\n",
    "1. 딥러닝 문제 구성에 대한 기본적인 이해를 높인다.\n",
    "2. Neural Network에 사용되는 용어들에 대한 이해를 높인다.\n",
    "3. 딥러닝 프레임워크를 사용하지 않고, Numpy만을 이용해 딥러닝 모델과 훈련 과정을 직접 구현해 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e4be5",
   "metadata": {},
   "source": [
    "### 오늘은 어떤 걸 배울까?\n",
    "---\n",
    "그동안 딥러닝을 이용한 몇 가지 프로젝트를 수행해 보고, 관련하여 딥러닝 프레임워크를 활용해 보면서 딥러닝의 문제 구성이 어떠한지 대략적인 흐름을 익혔다.\n",
    "그러나 딥러닝 프레임워크 안쪽에서 실제로 어떤 메커니즘으로 모델이 학습되는지 그 디테일까지 명확하게 파악하지는 못해서 나름의 답답함을 느꼈다.\n",
    "\n",
    "오늘은 딥러닝 내부를 좀 더 깊게 들여다보겠다. 신경망(Neural Network)이 어떤 식으로 구성되어 있고, 그 과정에서 어떤 용어들이 사용되는지 공부할 것이다. 그 과정에서 그동안 활용해 왔던 딥러닝 프레임워크를 사용하지 않고, 오직 Numpy만을 이용해 간단한 신경망과 그 훈련 과정을 직접 구현해 볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32700662",
   "metadata": {},
   "source": [
    "## 2. 신경망 구성 (1) 개요\n",
    "신경망(Neural Network)이란 무엇일까?\n",
    "\n",
    "우리 뇌에는 `1000억 개`에 가까운 신경계 뉴런들이 있다고 한다. 이 뉴런들은 서로 매우 복잡하게 얽혀 있고, 조금 물러서서 보면 하나의 거대한 그물망과 같은 형태를 이루고 있다. 보통 이를 신경망이라고 부른다.\n",
    "\n",
    "머신러닝/딥러닝 과학자들도 자연에서 답을 찾으려 노력했고, 우리 뇌 속의 신경망 구조에 착안해서 퍼셉트론(Perceptron)이라는 형태를 제안하며 이를 연결한 형태를 인공신경망(Artificial Neural Network)이라고 부르기 시작했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf4b66",
   "metadata": {},
   "source": [
    "### MNIST Revisited\n",
    "---\n",
    "MNIST 이미지 분류기 모델이 어떻게 구성되었는지 기억을 되짚어 보자. 여러번 구현해 보았던 Tensorflow 기반 분류 모델 예시 코드이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ceb37ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4993 - accuracy: 0.8840\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2287 - accuracy: 0.9358\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1786 - accuracy: 0.9499\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1487 - accuracy: 0.9581\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1280 - accuracy: 0.9636\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1122 - accuracy: 0.9682\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0998 - accuracy: 0.9720\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0895 - accuracy: 0.9749\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0814 - accuracy: 0.9775\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0742 - accuracy: 0.9796\n",
      "313/313 - 0s - loss: 0.1008 - accuracy: 0.9691\n",
      "test_loss: 0.10084670037031174 \n",
      "test_accuracy: 0.9690999984741211\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e530c",
   "metadata": {},
   "source": [
    "가장 기본적인 신경망 형태인 다층 퍼셉트론(Multi-Layer Perceptron; MLP)만을 이용해서 더욱 간단하게 구현해 보았다. 뇌 속의 뉴런이 1000억 개라지만 위에서 만든 모델은 굳이 Conv2D를 사용하지도 않았는데도 39,760개의 파라미터만으로 테스트 성능이 97%에 육박하고 있다.<br>\n",
    "인공신경망의 실제 구현 원리를 보다 명확하게 이해하기 위해, 그동안 들춰보지 않았던 프레임워크 내부에서 일어나는 일을 Numpy를 활용해 직접 구현해 보면서 이해해 보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e51989",
   "metadata": {},
   "source": [
    "### 다층 퍼셉트론 Overview\n",
    "---\n",
    "위의 이미지는 총 3개의 레이어로 구성 된 퍼셉트론을 나타낸다. 은닉층에는 `H개의 노드`가, 출력층에는 `K개의 노드`가 존재하는 인공신경망을 표현한 것이다. (+1 부분은 bias를 뜻하는 부분이므로 이전 레이어와의 연결이 없다.) 위의 코드에서는 `H=50`, `K=10`, 그리고 입력층 노드 개수 d=784로 정의되었다.\n",
    "\n",
    "bias 노드에 대해서는 아래 참고자료.\n",
    "\n",
    "`참고자료: bias term`\n",
    "* [The Basic Artificial Neuron: Bias neuron(Backpropagation)](https://funnypr.tistory.com/entry/The-Basic-Artificial-Neuron-Bias-neuron)\n",
    "* [What is the role of the bias in neural networks?](https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks)\n",
    "\n",
    "이제 각 레이어에 대해 한번 알아보자.\n",
    "\n",
    "위의 이미지를 보면 입력값이 있는 입력층(input layer), 최종 출력값이 있는 출력층(output layer), 그리고 그 사이에 있는 층인 은닉층(hidden layer)이 있다. 보통 입력층과 출력층 사이에 몇 개의 층이 존재하든 모두 은닉층이라고 부른다.\n",
    "\n",
    "보통 그림으로 인공신경망을 표현할 때에는 노드를 기준으로 레이어를 표시해서 3개의 레이어라고 생각할 수 있지만, 실제로는 `총 2개의 레이어`를 가졌다. 레이어 개수를 셀 때는 노드와 노드 사이의 연결하는 부분이 몇 개 존재하는지 세면 보다 쉽게 알 수 있다.\n",
    "\n",
    "이렇게 인공신경망이 어떻게 생겼는지 대략 알아보았다. 인공신경망 중에서도 위의 이미지처럼 2개 이상의 레이어를 쌓아서 만든 것을 보통 `다층 퍼셉트론(Multi-Layer Perceptron; MLP)`이라고 부른다. 그리고 입력층, 출력층을 제외한 은닉층이 많아지면 많아질수록 인공신경망이 `DEEP` 해졌다고 이야기한다.\n",
    "\n",
    "지금 알아보려고 하는 딥러닝이 바로 이 인공신경망이 `DEEP`해졌다는 뜻에서 나온 단어이다. 그래서 딥러닝은 충분히 깊은 인공신경망을 활용하며 이를 보통 다른 단어로 `DNN(Deep Neural Network)`이라고 부른다.\n",
    "\n",
    "`Tips`\n",
    "\n",
    ">*💡Fully-Connected Neural Network와 같은 단어를 들어보았을 것이다 이는 앞에서 설명한 MLP의 다른 용어이다. 이 Fully-Connnected Nerual Network는 서로 다른 층에 위치한 노드 간에는 연결 관계가 존재하지 않으며, 인접한 층에 위치한 노드들 간의 연결만 존재한다는 의미를 내포한다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f84298",
   "metadata": {},
   "source": [
    "### Parameters/Weights\n",
    "---\n",
    "앞에서 설명한 입력층-은닉층, 은닉층-출력층 사이에는 사실 각각 행렬(Matrix)이 존재한다. 고등학교 때 나오는 행렬 곱셈을 사용해보자.\n",
    "\n",
    "예를 들어 입력값이 100개, 은닉 노드가 20개라면 사실 이 입력층-은닉층 사이에는 100x20의 형태를 가진 행렬이 존재한다. 똑같이, MNIST 데이터처럼 10개의 클래스를 맞추는 문제를 풀기 위해 출력층이 10개의 노드를 가진다면 은닉층-출력층 사이에는 20x10의 형태를 가진 행렬이 존재하게 된다.\n",
    "\n",
    "이 행렬들을 Parameter 혹은 Weight라고 부른다. 두 단어는 보통 같은 뜻으로 사용되지만, 실제로 Paraemter에는 위의 참고자료에서 다룬 bias 노드도 포함된다는 점 유의해야 한다.\n",
    "\n",
    "이때 인접한 레이어 사이에는 아래와 같은 관계가 성립한다.\n",
    "\n",
    "$y = W * X + b $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66fc15",
   "metadata": {},
   "source": [
    "간단히 만들어 보았던 MLP 기반 딥러닝 모델을 Numpy로 다시 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a013b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X = x_train_reshaped[:5]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b00693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size=50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)  \n",
    "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55bc335c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.66812842,  1.21796762,  1.07363698, -0.09432575,  0.0760824 ,\n",
       "        0.30708839, -0.63175526,  0.58774867, -1.02994591, -0.04817563,\n",
       "        1.18097436, -1.05733832, -1.82666649, -1.0094017 , -0.02244052,\n",
       "       -0.27265812, -0.68179908,  1.82527517, -0.19865562,  0.77474247,\n",
       "        0.11511811,  1.51474221,  0.19132956, -1.39466579,  0.48986256,\n",
       "        0.83423317, -1.71459918, -1.8022644 ,  1.16650174,  0.78464758,\n",
       "        0.0132319 ,  0.09772938,  0.28977091, -1.161101  ,  0.29028108,\n",
       "        0.43745435, -0.8172701 , -0.00253478,  0.62268625, -0.61532497,\n",
       "        1.46014522, -0.23094997, -1.80560602, -2.09141006, -0.96998288,\n",
       "        1.61527891, -0.4723759 ,  0.47537838, -1.59787021, -0.65766017])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 데이터의 은닉층 출력을 확인해 봅시다.  50dim의 벡터가 나오나요?\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea5fcc",
   "metadata": {},
   "source": [
    "## 3. 신경망 구성 (2) 활성화 함수와 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c9dcb",
   "metadata": {},
   "source": [
    "### 활성화 함수 (Activation Functions)\n",
    "---\n",
    "MLP의 또 다른 중요한 구성요소에는 활성화 함수가 있다. 딥러닝에서는 이 활성화 함수의 존재가 필수적이다. 수학적인 이유가 있지만, 간단히만 설명하자면 이 활성화 함수는 보통 비선형 함수를 사용하는데 이 비선형 함수를 MLP 안에 포함시키면서 모델의 표현력이 좋아지게 된다. (정확히는 레이어 사이에 이 비선형 함수가 포함되지 않은 MLP는 한개의 레이어로 이루어진 모델과 수학적으로 다른 점이 없다.)\n",
    "\n",
    "(참고) [Why must a nonlinear activation function be used in a backpropagation neural network?](https://stackoverflow.com/questions/9782071/why-must-a-nonlinear-activation-function-be-used-in-a-backpropagation-neural-net/54503251#54503251)\n",
    "\n",
    "그럼 이렇게 중요한 역할을 하는 활성화 함수에는 어떤 종류가 있을까? 몇 가지 자주 쓰이는 활성화 함수들에 대해서 알아보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a94b5e",
   "metadata": {},
   "source": [
    "#### 1. Sigmoid\n",
    "$\\sigma (\\mathit{x}) = \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94740108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
