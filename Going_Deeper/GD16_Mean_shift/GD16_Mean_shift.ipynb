{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "necessary-defendant",
   "metadata": {},
   "source": [
    "# 16. 불안한 시선 이펙트 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-registration",
   "metadata": {},
   "source": [
    "## 1. 들어가며\n",
    "\n",
    "지난 시간에 이어서 눈동자를 검출 할 수 있는 방법에 대해 다루겠다. 이런 태스크를 위해 꼭 필요한 데이터셋이 충분히 갖추어져 있지 않은 상황을 우리는 헤쳐나가야 한다.\n",
    "\n",
    "이번 시간의 목표이다.\n",
    "\n",
    "1. 공개 데이터 사용해서 라벨 직접 모아보기\n",
    "2. 색상 값을 이용한 검출 방법\n",
    "3. 라벨링 툴 만들기 - point selection\n",
    "4. 째려보는 효과 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-divorce",
   "metadata": {},
   "source": [
    "### 준비물\n",
    "\n",
    "---\n",
    "\n",
    "아직 프로젝트 폴더를 만들지 않았다면, 터미널을 열고 개인 실습 환경에 따라 경로를 수정, 프로젝트를 위한 디렉토리를 생성하자.\n",
    "\n",
    "```bash\n",
    "$ mkdir -p ~/aiffel/coarse_to_fine/data\n",
    "```\n",
    "\n",
    "[coarse_to_fine_pjt.zip](https://aiffelstaticprd.blob.core.windows.net/media/documents/coarse_to_fine_pjt.zip)\n",
    "\n",
    "위 제공한 압축파일을 다운로드받아 아래와 같이 해당 디렉토리에서 작업하자.\n",
    "\n",
    "```\n",
    "$ cd ~/aiffel/coarse_to_fine\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/coarse_to_fine_pjt.zip\n",
    "$ unzip coarse_to_fine_pjt.zip\n",
    "```\n",
    "\n",
    "> _이번 시간에는 주피터 커널 대신 코드 에디터와 터미널을 사용하자. 다음에 나올 코드를 붙여넣어 적절한 파일로 저장 후 사용하면 된다._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-legislation",
   "metadata": {},
   "source": [
    "## 2. 위치 측정을 위한 라벨링 툴 만들기 (1) OpenCV 사용\n",
    "\n",
    "> _이번 시간에는 주피터 커널 대신 코드 에디터와 터미널을 사용하자. 아래 있는 코드를 붙여넣어 적절한 파일로 저장 후 사용하면 된다._\n",
    "\n",
    "이전 노드에[서는 True/False를 라벨링(labeling)할 수 있는 도구를 만들어 보았다. 품질이 좋지 못한 데이터에 위치를 직접 입력하려면 어떻게 해야 할까? 이번에는 눈동자 위치를 선택할 수 있는 도구를 만들어 보겠다.\n",
    "\n",
    "예를 들어, 아래와 같은 눈 이미지가 있다.\n",
    "\n",
    "<img src=\"./image/eye1.png\" />\n",
    "\n",
    "기존 예측 결과는 이상한 곳을 측정했다. 이런 경우 눈동자 위치를 새로 지정해서 fine 라벨로 만들어야 한다.\n",
    "\n",
    "<img src=\"./image/eye2.png\" />\n",
    "\n",
    "정확한 곳을 지정하기 위해서 마우스를 사용해야 할 것 같은데.. 마우스 이벤트를 사용하는 방법이 있을까?\n",
    "\n",
    "<img src=\"./image/eye3.png\" />\n",
    "\n",
    "다행히 **`OpenCV`**에서 마우스 이벤트를 callback 함수 형태로 지원한다.\n",
    "\n",
    "callback 함수가 무엇인지는 아래 링크를 참고.\n",
    "\n",
    "- [콜백 함수(Callback)의 정확한 의미는 무엇일까?](https://satisfactoryplace.tistory.com/18)\n",
    "\n",
    "__callback 함수는 무엇인가?__\n",
    "\n",
    "* 어떤 이벤트에 의해 호출되어지는 함수, 다른 함수의 인자로 이용되는 함수\n",
    "\n",
    "**`OpenCV`**에서 지원하는 마우스 이벤트 형태는 아래 참고자료를 통해 확인해보자.\n",
    "\n",
    "- [Mouse로 그리기 - gramman 0.1 documentation](https://opencv-python.readthedocs.io/en/latest/doc/04.drawWithMouse/drawWithMouse.html)\n",
    "\n",
    "__OpenCV에서 마우스 이벤트를 확인하고 callback을 호출하는 함수는 무엇인가?__\n",
    "\n",
    "* cv2.setMouseCallback\n",
    "\n",
    "__OpenCV에서 마우스 왼쪽 버튼이 눌러졌을 때 `on` 되는 `flag` 는 무엇인가?__\n",
    "\n",
    "* cv2.EVENT_LBUTTONDOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-sunday",
   "metadata": {},
   "source": [
    "## 3. 위치 측정을 위한 라벨링 툴 만들기 (2) 툴 만들기\n",
    "\n",
    "이제 OpenCV의 마우스 이벤트를 이용해서 라벨링 툴(labeling tool)을 만들어보겠다.\n",
    "\n",
    "아래 코드를 복사해 **`keypoint_using_mouse.py`**로 저장하자.<br>\n",
    "(참고) **`들어가며`** 스텝에서 다운받은 코드 중에 이미 포함되어 있으니 참고하자.\n",
    "\n",
    "아래 코드는 필요한 패키지를 불러온다. 주로 **`cv2`** 를 이용한다. 이번에는 기존 라벨을 읽지 않고 새로 위치를 정하기 때문에 **`img_path`** 만 불러오면 된다. **`flg_button`** 은 마우스 이벤트가 발생할 때 사용할 불리언(boolean) 타입 전역변수이다.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "args = argparse.ArgumentParser()\n",
    "\n",
    "# hyperparameters\n",
    "args.add_argument('img_path', type=str, nargs='?', default=None)\n",
    "\n",
    "config = args.parse_args()\n",
    "\n",
    "flg_button = False\n",
    "\n",
    "```\n",
    "\n",
    "사용할 함수를 만들겠다.\n",
    "\n",
    "해당 함수에서는 먼저 **`img_path`** 가 유효한지 체크한다. **`img_path`** 로 디렉토리가 입력될 경우 해당 디렉토리 내의 첫 번째 이미지를 **`img_path`** 에 입력하고 경로를 반환한다.\n",
    "\n",
    "이미지 간 이동할 **`move()`** 함수도 선언한다.\n",
    "\n",
    "```python\n",
    "def check_dir():\n",
    "    if config.img_path is None \\\n",
    "        or len(config.img_path) == 0 \\\n",
    "        or config.img_path == '' \\\n",
    "        or os.path.isdir(config.img_path):\n",
    "        root = os.path.realpath('./')\n",
    "        if os.path.isdir(config.img_path):\n",
    "            root = os.path.realpath(config.img_path)\n",
    "        img_list = sorted(glob(join(root, '*.png')))\n",
    "        img_list.extend(sorted(glob(join(root, '*.jpg'))))\n",
    "        config.img_path = img_list[0]\n",
    "\n",
    "    img_dir = os.path.dirname(os.path.realpath(config.img_path))\n",
    "\n",
    "    return img_dir\n",
    "\n",
    "def move(pos, idx, img_list):\n",
    "    if pos == 1:\n",
    "        idx += 1\n",
    "        if idx == len(img_list):\n",
    "            idx = 0\n",
    "    elif pos == -1:\n",
    "        idx -= 1\n",
    "        if idx == -1:\n",
    "            idx = len(img_list) - 1\n",
    "    return idx\n",
    "\n",
    "```\n",
    "\n",
    "mouse callback 함수를 정의한다. 전역변수인 **`gparam`** 에는 **`img`**와 **`point`** 정보를 저장한다. 마우스 왼쪽 버튼이 눌러졌다 떨어질 때 **`gparam`** 의 **`point`** 에 **`x, y`** 를 리스트로 저장한다.\n",
    "\n",
    "```python\n",
    "# Mouse callback functiondef select_point(event, x,y, flags, param):\n",
    "    global flg_button, gparam\n",
    "    img = gparam['img']\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        flg_button = True\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONUP and flg_button == True:\n",
    "        flg_button = False\n",
    "        print (f'({x}, {y}), size:{img.shape}')\n",
    "        gparam['point'] = [x,y]\n",
    "\n",
    "```\n",
    "\n",
    "**`main`** 함수인 **`blend_view()`** 를 구현하자. 마찬가지로 코드를 복사해 **`keypoint_using_mouse.py`**에 붙여넣자.\n",
    "\n",
    "```python\n",
    "def blend_view():\n",
    "    global gparam\n",
    "    gparam = {}\n",
    "    cv2.namedWindow('show', 0)\n",
    "    cv2.resizeWindow('show', 500, 500)\n",
    "\n",
    "    img_dir = check_dir()\n",
    "\n",
    "    fname, ext = os.path.splitext(config.img_path)\n",
    "    img_list = [os.path.basename(x) for x in sorted(glob(join(img_dir,'*%s'%ext)))]\n",
    "\n",
    "    dict_label = {}\n",
    "    dict_label['img_dir'] = img_dir\n",
    "    dict_label['labels'] = {}\n",
    "\n",
    "    json_path = os.getenv('HOME')+'/aiffel/coarse_to_fine/eye_annotation.json'\n",
    "    json_file = open(json_path, 'w', encoding='utf-8')\n",
    "\n",
    "    idx = img_list.index(os.path.basename(config.img_path))\n",
    "    pfname = img_list[idx]\n",
    "    orig = None\n",
    "    local_point = []# 저장할 point listwhile True:\n",
    "        start = cv2.getTickCount()\n",
    "        fname = img_list[idx]\n",
    "# 파일의 변경이 없거나 이미지가 없을 때, point 를 초기화함if pfname != fname or orig is None:\n",
    "            orig = cv2.imread(join(img_dir, fname), 1)\n",
    "            gparam['point'] = []\n",
    "            pfname = fname\n",
    "# 저장할 point(local point) 와 새로 지정한 gparam['point'] 가 변경된 경우,# local_point 를 업데이트if local_point != gparam['point']:\n",
    "            orig = cv2.imread(join(img_dir, fname), 1)\n",
    "            local_point = gparam['point']\n",
    "\n",
    "        img_show = orig\n",
    "        gparam['img'] = img_show\n",
    "        cv2.setMouseCallback('show', select_point)# mouse eventif len(local_point) == 2:\n",
    "            img_show = cv2.circle(img_show, tuple(local_point),\n",
    "                                  2, (0,255,0), -1)\n",
    "            dict_label['labels'][fname] = local_point# label 로 저장\n",
    "\n",
    "        time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 1000\n",
    "\n",
    "        if img_show.shape[0] > 300:\n",
    "            cv2.putText(img_show, '%s'%fname, (5,10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.5, (255,255,255))\n",
    "\n",
    "        print (f'[INFO] ({idx+1}/{len(img_list)}) {fname}... time: {time:.3f}ms', end='\\r')\n",
    "\n",
    "        cv2.imshow('show', img_show)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            return -1\n",
    "        if key == ord('n'):\n",
    "            idx = move(1, idx, img_list)\n",
    "        elif key == ord('p'):\n",
    "            idx = move(-1, idx, img_list)\n",
    "        elif key == ord('v'):\n",
    "            print ()\n",
    "            pprint (dict_label)\n",
    "            print ()\n",
    "        elif key == ord('s'):\n",
    "            json.dump(dict_label, json_file, indent=2)\n",
    "            print (f'[INFO] < {json_path} > saved!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    blend_view()\n",
    "\n",
    "```\n",
    "\n",
    "**`main`** 함수인 **`blend_view()`** 는 이론 노드에서 만든 라벨링 툴의 구조와 비슷하게 구현했다.\n",
    "\n",
    "달라진 점은,\n",
    "\n",
    "- 마우스 이벤트를 사용하기 위해 무한루프를 사용해서 **`gparam`**을 입력 받을 수 있게 한 것\n",
    "- 이미지 변경이 없다면 **`gparam['point']`** 를 초기화하지 않을 것\n",
    "- 이미지 변경이 없더라도 callback 함수에서 **`gparam`** 변경이 일어나는 경우는 수정할 것\n",
    "\n",
    "이 있다.\n",
    "\n",
    "만든 프로그램을 실행시켜서 마우스로 점을 찍어보자.\n",
    "\n",
    "우선 위 이미지를 다운받아, **`eye.png`** 이름으로 현재 프로젝트 폴더의 하위 디렉토리인 **`data`** 에 저장한 후, 아래 코드를 터미널에서 실행한다.\n",
    "\n",
    "```bash\n",
    "$ cd ~/aiffel/coarse_to_fine\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/original_images/1_hZAciQ9.png -O ./data/eye.png\n",
    "$ python keypoint_using_mouse.py ./data/eye.png\n",
    "\n",
    "```\n",
    "\n",
    "눈동자 지점을 클릭한 후 **`s`**를 눌러 저장하면 **`esc`**를 눌러 프로그램을 종료할 때 <br> **`~/aiffel/coarse_to_fine/eye_annotation.json`** 에 레이블이 저장된다.\n",
    "\n",
    "이제 레이블을 모아서 학습시킬 수 있다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-produce",
   "metadata": {},
   "source": [
    "## 4. 데이터를 모아보자\n",
    "\n",
    "이제 라벨링할 초기 데이터를 수집해야 한다. 직접 촬영한 데이터를 쓸 수도 있지만 들이는 노력에 비해 데이터가 많이 모이지 않는다. 때문에 공개된 데이터를 적극적으로 활용할 필요가 있다.\n",
    "\n",
    "우리가 모아야 할 라벨은 눈동자 위치이기 때문에 아래 순서로 데이터셋을 찾으면 좋다.\n",
    "\n",
    "1. 눈이 crop 되어 있고 눈동자 위치를 라벨로 가지고 있는 데이터\n",
    "2. 얼굴 랜드마크(face landmark)를 가지고 있는 데이터\n",
    "3. 얼굴 이미지를 가지고 있는 데이터\n",
    "\n",
    "1번에 해당하는 데이터셋은 **BioID** 가 있다.\n",
    "\n",
    "- [BioID Face Database | Dataset for Face Detection | facedb - BioID](https://www.bioid.com/facedb/)\n",
    "\n",
    "__BioID는 몇 장의 이미지를 가지고 있을까? 이미지 해상도는 얼마일까?__\n",
    "\n",
    "* 1,521장의 gray image, 384x286\n",
    "\n",
    "__BioID는 몇 명의 사람으로 구성되어있는가?__\n",
    "\n",
    "* 23명\n",
    "\n",
    "BioID의 경우 우리가 해결해야 할 문제를 위한 데이터셋이지만 규모가 너무 작다는 단점이 있다. 충분한 양의 이미지를 수집하기 위해 다른 데이터셋도 조사해야 한다.\n",
    "\n",
    "얼굴 랜드마크가 제공되는 데이터를 생각해보자. 랜드마크가 제공되는 경우는 눈 부분의 사진을 쉽게 crop해서 사용할 수 있기 때문에 라벨을 쉽게 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-tackle",
   "metadata": {},
   "source": [
    "### 랜드마크를 제공하는 데이터셋을 찾아보자\n",
    "\n",
    "---\n",
    "\n",
    "랜드마크를 제공하는 데이터셋을 찾아보자. 데이터를 어떻게 찾아가는지 함께 고민하는 것에 집중하자.\n",
    "\n",
    "우리는 **`dlib`** 패키지의 얼굴 랜드마크를 사용해오고 있다. 그렇다면 **`dlib`** 패키지를 구현하기 위해 사용된 랜드마크는 어떤 데이터셋으로 학습되었을까? 를 생각해보면 간단히 데이터를 찾을 수 있다.\n",
    "\n",
    "구글에 \"dlib face landmark dataset\"을 검색해보자. 검색 첫 페이지에서 아래 링크를 찾을 수 있다.\n",
    "\n",
    "- [http://dlib.net/face_landmark_detection.py.html](http://dlib.net/face_landmark_detection.py.html)\n",
    "\n",
    "주석을 몇 줄 읽어보면 어떤 데이터셋을 사용해서 학습했는지 친절하게 안내되어 있다.\n",
    "\n",
    "```\n",
    "#   and was trained on the iBUG 300-W face landmark dataset (see\n",
    "#   https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/):\n",
    "```\n",
    "\n",
    "**`iBUG 300-W`** 라는 데이터셋으로 학습했다고 한다. 이 데이터셋을 이용하는 방법도 유용할 것이다.\n",
    "\n",
    "이 경우에는 운이 좋았다. 하지만 실무에서 대부분의 경우는 내가 풀고자 하는 문제의 관련 도메인 데이터셋이 없기 때문에 위에서 알아본 방법 중 3번 방법도 고려해야 한다.\n",
    "\n",
    "3번 방법을 연습해 보기 위해, 오늘은 LFW 데이터셋을 사용해보자. 이 데이터셋은 **안면 인식(face recognition)**과 관련된 데이터셋이다 (해당 사진이 누구의 얼굴인지 판단하는 데이터셋이다). 얼굴이 포함된 이미지만 있고 얼굴의 랜드마크에 대한 정보는 없는 데이터셋이다. 이 이미지에 **`dlib`**을 적용해서 얼굴 위치와 랜드마크 위치를 찾고 눈을 잘라낸 뒤 라벨링을 할 수 있다.\n",
    "\n",
    "데이터는 아래 링크에서 다운로드 받을 수 있다.\n",
    "\n",
    "- [http://vis-www.cs.umass.edu/lfw/lfw.tgz](http://vis-www.cs.umass.edu/lfw/lfw.tgz)\n",
    "\n",
    "```bash\n",
    "$ cd ~ && wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
    "$ tar -xvzf lfw.tgz\n",
    "```\n",
    "\n",
    "위와 같이 다운로드 후 압축을 풀면 아래와 같은 이미지를 볼 수 있다.\n",
    "\n",
    "<img src=\"./image/lfw.png\" />\n",
    "<center><b>[LFW 데이터셋 예시]</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-optimization",
   "metadata": {},
   "source": [
    "## 5. Mean-shift를 이용한 눈동자 검출 방법 (1) 이론\n",
    "\n",
    "<img src=\"./image/mean_shift.png\" />\n",
    "\n",
    "눈동자를 어떻게 검출할까? 쉽게 떠오르는 방법은 **\"눈동자는 주변 부분에 비해 어두운 색을 지니고 있다.\"** 를 가정으로 반전된 1D 이미지에서 최댓값을 찾는 방법이다.\n",
    "\n",
    "<img src=\"./image/mean_shift2.png\" />\n",
    "<center><b>[위 이미지의 하단 및 우측 1차원 벡터에서 최대값을 찾는 방법을 사용했습니다.]</b></center>\n",
    "\n",
    "물론 이와 같은 가우시안 블러가 모든 상황을 해결할 수 있는 방법은 아니다. 아래 이미지와 같이 눈 근처에 머리카락이 나타나서 눈 가장자리에 검정색이 큰 비중으로 등장하는 경우는 눈동자보다 가장자리에 수렴할 확률이 높다. 특히 머리가 긴 분들에게 자주 나타날 수 있는 현상이다.\n",
    "\n",
    "<img src=\"./image/mean_shift3.png\" />\n",
    "\n",
    "2차원 블러 특성 이미지(feature image)에서는 눈동자가 2차원 정규분포로 나타나는 영역이 있는 것으로 보인다. 하지만 1차원 누적 그래프를 보면 x축으로 2개의 봉우리를 가지는 것을 관찰할 수 있다. 최댓값을 찾는 알고리즘을 왼쪽부터 시작했다면 가장 왼쪽에서 만나는 255에 수렴할 것이다.\n",
    "\n",
    "따라서 우리는 **1D 누적 그래프**와 **2D 특성 이미지**를 모두 사용한다. 2D에서는 어떻게 최고점을 찾아갈 수 있을까?\n",
    "\n",
    "<img src=\"./image/mean_shift4.png\" />\n",
    "\n",
    "1. **이미지 중심점을 초기값으로 설정하겠다.**<br>눈의 중심에 눈동자가 있을 확률이 높기 때문에 초기값으로 정하기에 아주 좋다.\n",
    "2. **중심점을 기준으로 작은 box를 설정한다.**<br>box의 크기는 문제에 따라 적절한 값을 설정해야 한다. 그림에서 회색박스를 생각하면 된다.\n",
    "3. **box 내부의 pixel 값을 이용해서 '무게중심'을 찾는다.**<br>이 때 무게중심은 pixel intensity를 weight 로 사용할 수 있다.\n",
    "4. **찾은 무게중심을 새로운 box의 중심으로 설정한다.**<br>이 단계에서 박스가 이동하게 된다. 이제 회색박스에서 초록색박스로 관심영역이 이동했다.\n",
    "5. **다시 초록색 박스를 기준으로 2-4를 반복한다.**\n",
    "6. **중심점이 수렴할 때 까지 2~5를 반복하면 수렴한 점의 위치로 눈동자를 찾을 수 있다.**\n",
    "\n",
    "머신러닝에 이미 비슷한 알고리즘이 존재한다. 현재 위치와 탐색반경을 가질 때 평균의 위치를 이용해서 반복적으로 움직이는 알고리즘인 **Mean Shift** 라는 알고리즘이다.\n",
    "\n",
    "<img src=\"./image/mean_shift5.png\" />\n",
    "\n",
    "mean shift는 탐색반경 내 데이터 포인트의 평균을 구하고 평균 위치로 이동을 반복해 가면서 데이터 분포의 중심으로 이동한다. 더 자세한 설명은 아래 링크를 참고.\n",
    "\n",
    "- [영상추적#1 - Mean Shift 추적](https://darkpgmr.tistory.com/64)\n",
    "\n",
    "__mean shift를 이용해서 global optima를 찾을 수 있는 방법.__\n",
    "\n",
    "* 그런 방법은 존재하지 않는다. mean shift는 local optima에만 수렴하기 때문이다.\n",
    "\n",
    "__mean shift 의 단점__\n",
    "\n",
    "* 초기값에 따라 수렴 위치가 달라진다. 항상 일정한 성능을 보장하기 힘들다.\n",
    "\n",
    "__mean shift는 컴퓨터 비전의 어떤 분야에 응용할 수 있을까?__\n",
    "\n",
    "* Mean Shift는 물체추적(object tracking), 영상 세그멘테이션(segmentation), 데이터 클러스터링(clustering), 경계를 보존하는 영상 스무딩(smoothing) 등 다양하게 활용될 수 있다.\n",
    "\n",
    "__1차원 데이터 분포에 mean shift를 적용하면 어떤 형태를 나타낼까?__\n",
    "\n",
    "* 가우시안 분포에서 등산하듯이 위로 올라가는 형태를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-adolescent",
   "metadata": {},
   "source": [
    "## 6. Mean-shift를 이용한 눈동자 검출 방법 (2) 실습\n",
    "\n",
    "눈동자를 검출하는 mean shift 기법을 코드로 구현해보자. 오늘의 시작포인트는 바로 지난 노드에서 구현해 보았던 눈동자 찾기 실습코드이다.\n",
    "\n",
    "해당 코드를 하나의 파일로 정리하여 **`eye_center_basic.py`**에 저장하였다. 이 코드를 이후 실습의 베이스라인으로 사용하겠다.\n",
    "\n",
    "```python\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import dlib\n",
    "import argparse\n",
    "\n",
    "args = argparse.ArgumentParser()\n",
    "\n",
    "# hyperparameters\n",
    "args.add_argument('show_substep', type=bool, nargs='?', default=False)\n",
    "\n",
    "config = args.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "img = cv2.imread('./images/image.png')\n",
    "print (img.shape)\n",
    "\n",
    "if config.show_substep:\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "img_bgr = img.copy()\n",
    "\n",
    "detector_hog = dlib.get_frontal_face_detector() # detector 선언\n",
    "landmark_predictor = dlib.shape_predictor('./models/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "dlib_rects = detector_hog(img_rgb, 1) # (image, num of img pyramid)\n",
    "\n",
    "list_landmarks = []\n",
    "for dlib_rect in dlib_rects:\n",
    "    points = landmark_predictor(img_rgb, dlib_rect)\n",
    "    list_points = list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "    list_landmarks.append(list_points)\n",
    "\n",
    "for dlib_rect in dlib_rects:\n",
    "    l = dlib_rect.left()\n",
    "    t = dlib_rect.top()\n",
    "    r = dlib_rect.right()\n",
    "    b = dlib_rect.bottom()\n",
    "    cv2.rectangle(img_rgb, (l,t), (r,b), (0,255,0), 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "for landmark in list_landmarks:\n",
    "    for idx, point in enumerate(list_points):\n",
    "        cv2.circle(img_rgb, point, 2, (255, 255, 0), -1) # yellow\n",
    "\n",
    "if config.show_substep:\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def eye_crop(bgr_img, landmark):\n",
    "    # dlib eye landmark: 36~41 (6), 42~47 (6)\n",
    "    np_left_eye_points = np.array(landmark[36:42])\n",
    "    np_right_eye_points = np.array(landmark[42:48])\n",
    "\n",
    "    np_left_tl = np_left_eye_points.min(axis=0)\n",
    "    np_left_br = np_left_eye_points.max(axis=0)\n",
    "    np_right_tl = np_right_eye_points.min(axis=0)\n",
    "    np_right_br = np_right_eye_points.max(axis=0)\n",
    "\n",
    "    list_left_tl = np_left_tl.tolist()\n",
    "    list_left_br = np_left_br.tolist()\n",
    "    list_right_tl = np_right_tl.tolist()\n",
    "    list_right_br = np_right_br.tolist()\n",
    "\n",
    "    left_eye_size = np_left_br - np_left_tl\n",
    "    right_eye_size = np_right_br - np_right_tl\n",
    "\n",
    "    ### if eye size is small\n",
    "    if left_eye_size[1] < 5:\n",
    "        margin = 1\n",
    "    else:\n",
    "        margin = 6\n",
    "\n",
    "    img_left_eye = bgr_img[np_left_tl[1]-margin:np_left_br[1]+margin, np_left_tl[0]-margin//2:np_left_br[0]+margin//2]\n",
    "    img_right_eye = bgr_img[np_right_tl[1]-margin:np_right_br[1]+margin, np_right_tl[0]-margin//2:np_right_br[0]+margin//2]\n",
    "\n",
    "    return [img_left_eye, img_right_eye]\n",
    "\n",
    "# 눈 이미지 crop\n",
    "img_left_eye, img_right_eye = eye_crop(img_bgr, list_landmarks[0])\n",
    "\n",
    "print (img_left_eye.shape) # (26, 47, 3)\n",
    "\n",
    "if config.show_substep:\n",
    "    plt.imshow(cv2.cvtColor(img_right_eye, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 눈 이미지에서 중심을 찾는 함수\n",
    "def findCenterPoint(gray_eye, str_direction='left'):\n",
    "    if gray_eye is None:\n",
    "        return [0, 0]\n",
    "    filtered_eye = cv2.bilateralFilter(gray_eye, 7, 75, 75)\n",
    "    filtered_eye = cv2.bilateralFilter(filtered_eye, 7, 75, 75)\n",
    "    filtered_eye = cv2.bilateralFilter(filtered_eye, 7, 75, 75)\n",
    "\n",
    "    # 2D images -> 1D signals\n",
    "    row_sum = 255 - np.sum(filtered_eye, axis=0)//gray_eye.shape[0]\n",
    "    col_sum = 255 - np.sum(filtered_eye, axis=1)//gray_eye.shape[1]\n",
    "\n",
    "    # normalization & stabilization\n",
    "    def vector_normalization(vector):\n",
    "        vector = vector.astype(np.float32)\n",
    "        vector = (vector-vector.min())/(vector.max()-vector.min()+1e-6)*255\n",
    "        vector = vector.astype(np.uint8)\n",
    "        vector = cv2.blur(vector, (5,1)).reshape((vector.shape[0],))\n",
    "        vector = cv2.blur(vector, (5,1)).reshape((vector.shape[0],))            \n",
    "        return vector\n",
    "    row_sum = vector_normalization(row_sum)\n",
    "    col_sum = vector_normalization(col_sum)\n",
    "\n",
    "    def findOptimalCenter(gray_eye, vector, str_axis='x'):\n",
    "        axis = 1 if str_axis == 'x' else 0\n",
    "        center_from_start = np.argmax(vector)\n",
    "        center_from_end = gray_eye.shape[axis]-1 - np.argmax(np.flip(vector,axis=0))\n",
    "        return (center_from_end + center_from_start) // 2\n",
    "\n",
    "    center_x = findOptimalCenter(gray_eye, row_sum, 'x')\n",
    "    center_y = findOptimalCenter(gray_eye, col_sum, 'y')\n",
    "\n",
    "    if center_x >= gray_eye.shape[1]-2 or center_x <= 2:\n",
    "        center_x = -1\n",
    "    elif center_y >= gray_eye.shape[0]-1 or center_y <= 1:\n",
    "        center_y = -1\n",
    "\n",
    "    return [center_x, center_y]\n",
    "\n",
    "# 눈동자 검출 wrapper 함수\n",
    "def detectPupil(bgr_img, landmark):\n",
    "    if landmark is None:\n",
    "        return\n",
    "\n",
    "    img_eyes = []\n",
    "    img_eyes = eye_crop(bgr_img, landmark)\n",
    "\n",
    "    gray_left_eye = cv2.cvtColor(img_eyes[0], cv2.COLOR_BGR2GRAY)\n",
    "    gray_right_eye = cv2.cvtColor(img_eyes[1], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if gray_left_eye is None or gray_right_eye is None:\n",
    "        return \n",
    "\n",
    "    left_center_x, left_center_y = findCenterPoint(gray_left_eye,'left')\n",
    "    right_center_x, right_center_y = findCenterPoint(gray_right_eye,'right')\n",
    "\n",
    "    return [left_center_x, left_center_y, right_center_x, right_center_y, gray_left_eye.shape, gray_right_eye.shape]\n",
    "\n",
    "# 눈동자 중심 좌표 출력\n",
    "left_center_x, left_center_y, right_center_x, right_center_y, le_shape, re_shape = detectPupil(img_bgr, list_landmarks[0])\n",
    "print ((left_center_x, left_center_y), (right_center_x, right_center_y), le_shape, re_shape)\n",
    "\n",
    "# 이미지 출력\n",
    "show = img_right_eye.copy()\n",
    "\n",
    "show = cv2.circle(show, (right_center_x, right_center_y), 2, (0,255,255), -1)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "이 코드의 동작은 아래와 같이 확인해 볼 수 있다. **`show_substep`** argument의 옵션을 True로 주게 되면 매 스텝의 작동을 차례차례 확인해볼 수 있다. 옵션을 False로 주거나 생략(기본옵션)하면 최종 결과만 확인하게 된다.\n",
    "\n",
    "```bash\n",
    "$ cd ~/aiffel/coarse_to_fine && python eye_center_basic.py True\n",
    "\n",
    "```\n",
    "\n",
    "이제 mean shift 알고리즘을 적용해보자. **`eye_center_basic.py`** 파일을 복사하여 **`eye_center_meanshift.py`**를 생성한다.\n",
    "\n",
    "```bash\n",
    "$ cd ~/aiffel/coarse_to_fine && cp eye_center_basic.py eye_center_meanshift.py\n",
    "\n",
    "```\n",
    "\n",
    "**`eye_center_meanshift.py`** 파일을 편집기로 열어 기존 함수 중 **`findCenterPoint`**를 다음과 같이 수정한다.\n",
    "\n",
    "```python\n",
    "def findCenterPoint(gray_eye, str_direction='left'):\n",
    "    if gray_eye is None:\n",
    "        return [0, 0]\n",
    "    filtered_eye = cv2.bilateralFilter(gray_eye, 7, 75, 75)\n",
    "    filtered_eye = cv2.bilateralFilter(filtered_eye, 7, 75, 75)\n",
    "    filtered_eye = cv2.bilateralFilter(filtered_eye, 7, 75, 75)\n",
    "\n",
    "# 2D images -> 1D signals\n",
    "    row_sum = 255 - np.sum(filtered_eye, axis=0)//gray_eye.shape[0]\n",
    "    col_sum = 255 - np.sum(filtered_eye, axis=1)//gray_eye.shape[1]\n",
    "\n",
    "# normalization & stabilizationdef vector_normalization(vector):\n",
    "        vector = vector.astype(np.float32)\n",
    "        vector = (vector-vector.min())/(vector.max()-vector.min()+1e-6)*255\n",
    "        vector = vector.astype(np.uint8)\n",
    "        vector = cv2.blur(vector, (5,1)).reshape((vector.shape[0],))\n",
    "        vector = cv2.blur(vector, (5,1)).reshape((vector.shape[0],))\n",
    "        return vector\n",
    "    row_sum = vector_normalization(row_sum)\n",
    "    col_sum = vector_normalization(col_sum)\n",
    "\n",
    "    def findOptimalCenter(gray_eye, vector, str_axis='x'):\n",
    "        axis = 1 if str_axis == 'x' else 0\n",
    "        center_from_start = np.argmax(vector)\n",
    "        center_from_end = gray_eye.shape[axis]-1 - np.argmax(np.flip(vector,axis=0))\n",
    "        return (center_from_end + center_from_start) // 2\n",
    "\n",
    "# x 축 center 를 찾는 알고리즘을 mean shift 로 대체합니다.# center_x = findOptimalCenter(gray_eye, row_sum, 'x')\n",
    "    center_y = findOptimalCenter(gray_eye, col_sum, 'y')\n",
    "\n",
    "# 수정된 부분\n",
    "    inv_eye = (255 - filtered_eye).astype(np.float32)\n",
    "    inv_eye = (255*(inv_eye - inv_eye.min())/(inv_eye.max()-inv_eye.min())).astype(np.uint8)\n",
    "\n",
    "    resized_inv_eye = cv2.resize(inv_eye, (inv_eye.shape[1]//3, inv_eye.shape[0]//3))\n",
    "    init_point = np.unravel_index(np.argmax(resized_inv_eye),resized_inv_eye.shape)\n",
    "\n",
    "    x_candidate = init_point[1]*3 + 1\n",
    "    for idx in range(10):\n",
    "        temp_sum = row_sum[x_candidate-2:x_candidate+3].sum()\n",
    "        if temp_sum == 0:\n",
    "            break\n",
    "        normalized_row_sum_part = row_sum[x_candidate-2:x_candidate+3].astype(np.float32)//temp_sum\n",
    "        moving_factor = normalized_row_sum_part[3:5].sum() - normalized_row_sum_part[0:2].sum()\n",
    "        if moving_factor > 0.0:\n",
    "            x_candidate += 1\n",
    "        elif moving_factor < 0.0:\n",
    "            x_candidate -= 1\n",
    "\n",
    "    center_x = x_candidate\n",
    "\n",
    "    if center_x >= gray_eye.shape[1]-2 or center_x <= 2:\n",
    "        center_x = -1\n",
    "    elif center_y >= gray_eye.shape[0]-1 or center_y <= 1:\n",
    "        center_y = -1\n",
    "\n",
    "    return [center_x, center_y]\n",
    "```\n",
    "\n",
    "먼저, 눈 이미지를 low pass filter를 이용해서 smoothing 한다. bilateral filter를 이용했다.\n",
    "\n",
    "다음으로 1차원 값으로 누적시킨 후 **`y`** 축 기준으로 최대값을 찾아서 **`y`**축의 중심점 좌표를 먼저 얻어낸다. (y 축은 x 축에 비해 상대적으로 변화가 적기 때문에 간단하게 구현한다.)\n",
    "\n",
    "**`x`**축은 1차원 최댓값 지점을 기준으로 mean shift를 수행한다. 양 끝단에 수렴하는 예외를 처리한 후 결과를 출력한다.\n",
    "\n",
    "```python\n",
    "# 눈동자 중심 좌표 출력\n",
    "left_center_x, left_center_y, right_center_x, right_center_y, le_shape, re_shape = detectPupil(img_bgr, list_landmarks[0])\n",
    "print ((left_center_x, left_center_y), (right_center_x, right_center_y), le_shape, re_shape)\n",
    "\n",
    "# 이미지 출력\n",
    "show = img_right_eye.copy()\n",
    "show = cv2.circle(show, (right_center_x, right_center_y), 2, (0,255,255), -1)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "결과를 뽑아보겠다.\n",
    "\n",
    "```bash\n",
    "$ cd ~/aiffel/coarse_to_fine && python eye_center_meanshift.py\n",
    "```\n",
    "\n",
    "<img src=\"./image/result.png\" />\n",
    "\n",
    "여전히 눈동자 중심은 아니어서 조금 아쉽다. 기존 머신러닝 기반 알고리즘으로는 성능을 큰 폭으로 향상시키기 어렵다. 하지만 예외 상황에 대해 더 강건한 모델을 만들 수 있는 장점이 있다. 일반화에 한 걸음 더 가까워지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-container",
   "metadata": {},
   "source": [
    "## 7. 키포인트 검출 딥러닝 모델 만들기 (1) 데이터 확인\n",
    "\n",
    "더 나은 성능을 위해 딥러닝 모델을 만들어 보겠다. 지난 이론 시간에 배웠던 딥러닝 모델링 기법을 적용해 실제로 학습을 수행해보겠다.\n",
    "\n",
    "이번 단계에서는 대량의 눈동자 위치 라벨이 필요하다. 앞에서 만든 coarse dataset 또는 직접 어노테이션 한 라벨이 10,000개 이상 있어야 성능을 확인할 수 있다.\n",
    "\n",
    "이전 스텝에서 다룬 눈동자 검출 방법을 LFW 데이터셋에 적용하여 필요한 만큼의 데이터셋을 생성해보자. 데이터셋을 생성하는 코드 **`prepare_eye_dataset.py`**를 실행하면 아래 사용할 데이터셋이 LFW 데이터셋으로부터 가공 생성된다.<br>\n",
    "생성된 데이터셋은 **`~/lfw/data/train`**, **`~/lfw/data/valid`** 아래에서 확인할 수 있다.\n",
    "\n",
    "```bash\n",
    "$ cd ~/aiffel/coarse_to_fine && python prepare_eye_dataset.py\n",
    "\n",
    "```\n",
    "\n",
    "> _(주의) 총 13000여 개의 LFW 데이터셋으로부터 데이터셋을 가공 생성하는 위 과정은 20분 이상 소요될 수 있다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "digital-missile",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:08:22.844487Z",
     "start_time": "2021-05-12T08:08:22.836078Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-virginia",
   "metadata": {},
   "source": [
    "TensorFlow Hub에서 제공하는 **pretrained image feature embedding**을 가지고 fine tuning을 해보겠다.\n",
    "\n",
    "가지고 있는 데이터를 케라스 **`ImageDataGenerator`** 형식으로 읽는다. 라벨을 **`image`** 형태로 저장해두었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "monetary-cradle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:08:24.306594Z",
     "start_time": "2021-05-12T08:08:23.500697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23716 23716\n",
      "Found 23712 images belonging to 1 classes.\n",
      "Found 23712 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "home_dir = os.getenv('HOME')+'/lfw'\n",
    "list_image = sorted(glob.glob(home_dir+'/data/train/input/img/*.png'))\n",
    "list_label = sorted(glob.glob(home_dir+'/data/train/label/mask/*.png'))\n",
    "print (len(list_image), len(list_label))\n",
    "\n",
    "# 32의 배수를 벗어나는 파일 경로들을 담은 list\n",
    "list_image_out_of_range = list_image[len(list_image) - (len(list_image) % 32):]\n",
    "list_label_out_of_range = list_label[len(list_label) - (len(list_label) % 32):]\n",
    "\n",
    "# 해당 list가 존재한다면, 파일 삭제\n",
    "if list_image_out_of_range:\n",
    "    for path in list_image_out_of_range:\n",
    "        os.remove(path)\n",
    "if list_label_out_of_range:\n",
    "    for path in list_label_out_of_range:\n",
    "        os.remove(path)\n",
    "\n",
    "IMAGE_SHAPE = (80, 120)\n",
    "data_root = home_dir+'/data/train/input'\n",
    "label_root = home_dir+'/data/train/label'\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "label_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "image_data = image_generator.flow_from_directory(str(data_root), class_mode=None, target_size=IMAGE_SHAPE, batch_size=32)\n",
    "label_data = label_generator.flow_from_directory(str(label_root), class_mode=None, target_size=IMAGE_SHAPE, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-refund",
   "metadata": {},
   "source": [
    "train 데이터셋이 23,712쌍, val 데이터셋이 2,638쌍 생성될 것이다.\n",
    "\n",
    "> _경우에 따라서는 train 데이터셋의 갯수가 23,712쌍과 다소 다르게 만들어질 수 있다. batch_size 32의 배수인 23,712쌍과 같아지도록 이미지 데이터의 갯수를 맞춰주도록 하자. 32의 배수 조건만 만족하면 된다._\n",
    "\n",
    "아래 코드에서는 **`image_generator`**, **`label generator`**를 학습할 수 있는 입출력 형식으로 편집한다. 텐서플로우의 제너레이터(generator) 형식을 사용하고 있기 때문에 출력 형식도 맞추어 주겠다.\n",
    "\n",
    "- 참고: [제너레이터](https://tensorflow.blog/%ED%9A%8C%EC%98%A4%EB%A6%AC%EB%B0%94%EB%9E%8C%EC%9D%84-%ED%83%84-%ED%8C%8C%EC%9D%B4%EC%8D%AC/%EC%A0%9C%EB%84%88%EB%A0%88%EC%9D%B4%ED%84%B0/)\n",
    "\n",
    "학습 라벨을 만들 때 3개의 점을 **`label`** 이미지에 표시했다. 눈의 왼쪽 끝점을 **`1`**의 값으로, 오른쪽 끝점은 **`2`**의 값으로, 가장 중요한 눈 중심(눈동자)는 **`3`**으로 인코딩 했다. **`np.where()`** 함수로 이미지에서 좌표로 복원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fitted-perth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:08:24.470429Z",
     "start_time": "2021-05-12T08:08:24.455232Z"
    }
   },
   "outputs": [],
   "source": [
    "def user_generation(train_generator, label_generator):\n",
    "    h, w = train_generator.target_size\n",
    "    for images, labels in zip(train_generator, label_generator):\n",
    "        images /= 255.\n",
    "        images = images[..., ::-1] # rgb to bgr\n",
    "\n",
    "        list_point_labels = []\n",
    "        for img, label in zip(images, labels):\n",
    "\n",
    "            eye_ls = np.where(label==1) # leftside\n",
    "            eye_rs = np.where(label==2) # rightside\n",
    "            eye_center = np.where(label==3)\n",
    "\n",
    "            lx, ly = [eye_ls[1].mean(), eye_ls[0].mean()]\n",
    "            rx, ry = [eye_rs[1].mean(), eye_rs[0].mean()]\n",
    "            cx, cy = [eye_center[1].mean(), eye_center[0].mean()]\n",
    "\n",
    "            if len(eye_ls[0])==0 or len(eye_ls[1])==0:\n",
    "                lx, ly = [0, 0]\n",
    "            if len(eye_rs[0])==0 or len(eye_rs[1])==0:\n",
    "                rx, ry = [w, h]\n",
    "            if len(eye_center[0])==0 or len(eye_center[1])==0:\n",
    "                cx, cy = [0, 0]\n",
    "\n",
    "            np_point_label = np.array([lx/w,ly/h,rx/w,ry/h,cx/w,cy/h], dtype=np.float32)\n",
    "\n",
    "            list_point_labels.append(np_point_label)\n",
    "        np_point_labels = np.array(list_point_labels)\n",
    "        yield (images, np_point_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-playback",
   "metadata": {},
   "source": [
    "__좌표로 복원할 때 eye_ls[1].mean() 으로 평균값을 구했다. 왜 그랬을까?__\n",
    "\n",
    "* 눈 크기가 이미지마다, 사람마다 다르기 때문에 반드시 resize를 해야 한다. 이 때 라벨을 이미지에 하나의 점으로 표현하면 resize 과정에서 소실될 수 있다. 이런 단점을 극복하기 위해 라벨 이미지를 만들 때 gaussian smoothing을 적용해서 변화에 유연하게 대응 할 수 있도록 했다. 이 방법을 취하면 이후 augmentation을 구현할 때도 추가적인 노력없이 바로 라벨을 사용할 수 있다.\n",
    "\n",
    "만들어진 제너레이터로 데이터 포인트를 뽑아 관찰해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "weighted-location",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:08:25.259156Z",
     "start_time": "2021-05-12T08:08:25.192269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 120, 3) [0.         0.         1.         1.         0.07932901 0.4791396 ]\n",
      "(80, 120, 3) [0.         0.         1.         1.         0.34166667 0.13513349]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj10/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice.\n",
      "  \n",
      "/home/aiffel-dj10/anaconda3/envs/aiffel/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/aiffel-dj10/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: Mean of empty slice.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "user_train_generator = user_generation(image_data, label_data)\n",
    "for i in range(2):\n",
    "    dd = next(user_train_generator)\n",
    "    print (dd[0][0].shape, dd[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-comparison",
   "metadata": {},
   "source": [
    "120x80의 정해진 크기로 이미지가 잘 출력되고 라벨 또한 0~1 값으로 정규화(normalize) 되어 있는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-richmond",
   "metadata": {},
   "source": [
    "## 8. 키포인트 검출 딥러닝 모델 만들기 (2) 모델 설계\n",
    "\n",
    "데이터가 준비되었으니 이제 네트워크를 디자인한다. 우리는 데이터가 없는 상황이기 때문에 미리 학습된 모델을 적극적으로 활용해야 한다. TensorFlow Hub에서 ResNet의 특성추출기 부분을 백본(backbone)으로 사용하겠다.\n",
    "\n",
    "**`tf.keras.Sequential()`**을 이용해서 백본 네트워크와 fully connected layer를 쌓아서 아주 쉽게 모델을 완성할 수 있다. 데이터 제너레이터를 만들 때 출력을 6개((x, y) 좌표 2개 * 점 3개) 로 했기 때문에 **`num_classes`** 는 6으로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "successful-observer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:08:42.952723Z",
     "start_time": "2021-05-12T08:08:29.529577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2048)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 2048)              23564800  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 23,577,094\n",
      "Trainable params: 12,294\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' tf hub feature_extractor '''\n",
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                            input_shape=(80,120,3))\n",
    "\n",
    "image_batch = next(image_data)\n",
    "feature_batch = feature_extractor_layer(image_batch)\n",
    "print(feature_batch.shape)\n",
    "\n",
    "num_classes = 6\n",
    "\n",
    "feature_extractor_layer.trainable = False\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    #layers.Dense(1024, activation='relu'),\n",
    "    #layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-financing",
   "metadata": {},
   "source": [
    "이 문제는 점을 맞는 위치로 추정하는 position regression 문제이기 때문에 **`loss`**와 **`metric`**을 각각 **`mse`** 와 **`mae`** 로 설정했다. **`mae`** 를 통해서 픽셀 위치가 평균적으로 얼마나 차이나는지 확인하면서 학습할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adjusted-croatia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:08:43.997248Z",
     "start_time": "2021-05-12T08:08:43.986359Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(),\n",
    "  loss='mse',\n",
    "  metrics=['mae']\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-wagon",
   "metadata": {},
   "source": [
    "학습률(learning rate)을 조절하는 함수도 제작해준다. 저는 지수적으로 감소하게 만들었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "experimental-costs",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:08:44.715286Z",
     "start_time": "2021-05-12T08:08:44.708184Z"
    }
   },
   "outputs": [],
   "source": [
    "def lr_step_decay(epoch):\n",
    "    init_lr = 0.0005 #self.flag.initial_learning_rate\n",
    "    lr_decay = 0.5 #self.flag.learning_rate_decay_factor\n",
    "    epoch_per_decay = 2 #self.flag.epoch_per_decay\n",
    "    lrate = init_lr * math.pow(lr_decay, math.floor((1+epoch)/epoch_per_decay))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-lesson",
   "metadata": {},
   "source": [
    "학습을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "narrow-campaign",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:11:43.299234Z",
     "start_time": "2021-05-12T08:08:45.469413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23712 32 741\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj10/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice.\n",
      "  \n",
      "/home/aiffel-dj10/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: Mean of empty slice.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 21s 25ms/step - loss: 0.0355 - mae: 0.0989\n",
      "Epoch 2/10\n",
      "741/741 [==============================] - 17s 23ms/step - loss: 0.0145 - mae: 0.0571\n",
      "Epoch 3/10\n",
      "741/741 [==============================] - 17s 23ms/step - loss: 0.0141 - mae: 0.0555\n",
      "Epoch 4/10\n",
      "741/741 [==============================] - 17s 23ms/step - loss: 0.0133 - mae: 0.0535\n",
      "Epoch 5/10\n",
      "741/741 [==============================] - 17s 24ms/step - loss: 0.0131 - mae: 0.0528\n",
      "Epoch 6/10\n",
      "741/741 [==============================] - 17s 23ms/step - loss: 0.0132 - mae: 0.0529\n",
      "Epoch 7/10\n",
      "741/741 [==============================] - 18s 24ms/step - loss: 0.0129 - mae: 0.0524\n",
      "Epoch 8/10\n",
      "741/741 [==============================] - 18s 24ms/step - loss: 0.0130 - mae: 0.0524\n",
      "Epoch 9/10\n",
      "741/741 [==============================] - 17s 23ms/step - loss: 0.0128 - mae: 0.0521\n",
      "Epoch 10/10\n",
      "741/741 [==============================] - 17s 24ms/step - loss: 0.0128 - mae: 0.0522\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = image_data.samples//image_data.batch_size\n",
    "print (image_data.samples, image_data.batch_size, steps_per_epoch)\n",
    "# 23712 32 741 -> 데이터를 batch_size(32) 의 배수로 맞춰 준비해 주세요. \n",
    "\n",
    "assert(image_data.samples % image_data.batch_size == 0)  # 데이터가 32의 배수가 되지 않으면 model.fit()에서 에러가 발생합니다.\n",
    "\n",
    "learning_rate = LearningRateScheduler(lr_step_decay)\n",
    "\n",
    "history = model.fit(user_train_generator, epochs=10,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    callbacks = [learning_rate]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-belgium",
   "metadata": {},
   "source": [
    "## 9. 키포인트 검출 딥러닝 모델 만들기 (3) 평가\n",
    "\n",
    "검증(validation)용 데이터는 섞어줄(shuffle) 필요가 없기 때문에 **`shuffle=False`** 옵션을 추가했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "atlantic-christian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:12:11.399500Z",
     "start_time": "2021-05-12T08:12:11.149660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2634 images belonging to 1 classes.\n",
      "Found 2634 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (80, 120)\n",
    "\n",
    "home_dir = os.getenv('HOME')+'/lfw'\n",
    "\n",
    "val_data_root = home_dir + '/data/val/input'\n",
    "val_label_root = home_dir + '/data/val/label'\n",
    "\n",
    "image_generator_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "label_generator_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "image_data_val = image_generator.flow_from_directory(str(val_data_root), class_mode=None, target_size=IMAGE_SHAPE, shuffle=False)\n",
    "label_data_val = label_generator.flow_from_directory(str(val_label_root), class_mode=None, target_size=IMAGE_SHAPE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-coalition",
   "metadata": {},
   "source": [
    "제너레이터를 만들고 **`evaluate_generator()`** 로 평가를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "compliant-float",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:12:14.761797Z",
     "start_time": "2021-05-12T08:12:12.389433Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj10/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n",
      "/home/aiffel-dj10/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice.\n",
      "  \n",
      "/home/aiffel-dj10/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: Mean of empty slice.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01276505459100008 0.05172451585531235\n"
     ]
    }
   ],
   "source": [
    "user_val_generator = user_generation(image_data_val, label_data_val)\n",
    "mse, mae = model.evaluate_generator(user_val_generator, image_data_val.n // 32)\n",
    "print(mse, mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-basket",
   "metadata": {},
   "source": [
    "평균 에러가 0.026 정도 나왔다. 우리가 찍은 점들은 120 픽셀을 기준으로 **`120*0.026 = 3.12`** 픽셀 정도 에러가 나는 것을 확인할 수 있다.\n",
    "\n",
    "실제로 이미지에 출력해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "wound-russian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:12:25.294792Z",
     "start_time": "2021-05-12T08:12:25.021415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# img test\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(val_data_root+'/img/eye_000010_l.png')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-heating",
   "metadata": {},
   "source": [
    "입력을 위해 이미지를 120x80 으로 resize 한 후, 배치(batch)를 나타낼 수 있는 4차원 텐서로 변경한다. 우선 이미지 1장에 대해서 출력을 하려 하니 지금은 배치 크기(batch size)를 1로 만들면 되겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "middle-australian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:12:39.099980Z",
     "start_time": "2021-05-12T08:12:27.634616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3.59295309e-03 1.27376057e-03]\n",
      "  [1.19998184e+02 7.99960327e+01]\n",
      "  [5.28573914e+01 2.71381721e+01]]]\n"
     ]
    }
   ],
   "source": [
    "np_inputs = np.expand_dims(cv2.resize(img, (120, 80)), axis=0)\n",
    "preds = model.predict(np_inputs/255., 1)\n",
    "\n",
    "repred = preds.reshape((1, 3, 2))\n",
    "repred[:,:,0] *= 120\n",
    "repred[:,:,1] *= 80\n",
    "print (repred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-command",
   "metadata": {},
   "source": [
    "출력결과를 뽑아보면 아래와 같이 나온다. 1행부터 좌측, 우측, 중앙 좌표를 나타낸다.\n",
    "\n",
    "결과를 이미지에 출력해보자. **`pt`** 값은 *120x80* 으로 뽑았는데 우리가 사용하는 데이터 크기는 *60x40*이다. 따라서 **`pt`** 에 **`0.5`** 를 곱해서 그림에 출력한다.\n",
    "\n",
    "> _(주의) **`pt`**값을 뽑을때의 이미지 크기 기준(120X80)은 고정이지만, 사용하는 데이터의 크기는 매번 달라질 것이다. 보정치 설정에 유의하자._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vulnerable-start",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:12:41.017686Z",
     "start_time": "2021-05-12T08:12:40.906286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[120.  80.]\n",
      "[53. 27.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAClCAYAAAB1Ebc2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMUklEQVR4nO3df6jd9X3H8ec7Nze5SYxGzQ9tbujV+aN10lUJ0i0w0NaRtqKD/aO0RVhBGG1nR0enFAZjMAobXQcrG6JOoU4pVql0thraihRa25j6IzG2dc7pTeKSmsZofnjvTd7745xbb5P743v1fM73c5vnAy73/OLNi8u5r/s93/v9fj+RmUiS6rWo7QCSpNlZ1JJUOYtakipnUUtS5SxqSaqcRS1JlVtcYujq1ZEjIyUma9LL/72q2OzxsbeKzD1y5EiRucuXLy8yFyDzeJG5iwcGiswdHxsrMhcgIhbU3EWF5gJMHDvW85mHxycYO3Z82tBFinpkBLZuLTFZkz73Z1cWmz368ktF5m7f/myRuZdf+r4icwGOjZf5o3X2GSuLzN2965UicwGWLC7zx2VwcLDI3KHBoSJzAfa/fqDnMx9/ee+Mz7nrQ5IqZ1FLUuUsakmqnEUtSZWzqCWpco2KOiI2R8TPI+KFiLildChJ0tvmLOqIGAC+BnwUuAS4ISIuKR1MktTRZIv6CuCFzHwxM8eA+4DrysaSJE1qUtTrgalH0Y92H5Mk9UGTop7ulMaTloWJiJsiYmtEbN23790HkyR1NCnqUWDDlPvDwO4TX5SZt2XmxszcuGZNr+JJkpoU9U+BCyPivIhYAlwPPFQ2liRp0pwXZcrMiYj4LPAIMADcmZk7iieTJAENr56XmQ8DDxfOIkmahmcmSlLlLGpJqpxFLUmVs6glqXIWtSRVzqKWpMoVWdxWb/v7v/hMkbk7nnusyFyA3bt2FZk7NLSiyNx9+14rMhfgogvOLzL38g/8fpG577/owiJzAX79WplrQ6w5+6wic48eLbci+67dr/Z85hOvvj7jc25RS1LlLGpJqpxFLUmVs6glqXIWtSRVzqKWpMpZ1JJUuSarkN8ZEXsjYns/AkmSfluTLeq7gM2Fc0iSZjBnUWfm48D+PmSRJE2jZ/uoXYVcksroWVG7CrkkleFRH5JUOYtakirX5PC8e4EfARdHxGhEfLp8LEnSpDmvR52ZN/QjiCRpeu76kKTKWdSSVDmLWpIqZ1FLUuUsakmqnEUtSZWb8/A8vTuPPPxfRebu3jNaZC7AqtPPKDL3+LHxInOPvvlGkbkA6848s8jcFYNLisxdNLSsyFyAXH5akbkTR98qMvfo4UNF5gKce87ans8cHJy5jt2ilqTKWdSSVDmLWpIqZ1FLUuUsakmqnEUtSZVrcpnTDRHxg4jYGRE7IuLmfgSTJHU0OY56AvhCZm6LiJXAkxGxJTOfK5xNkkSzVcj3ZOa27u03gJ3A+tLBJEkd89pHHREjwGXAE0XSSJJO0rioI+I04JvA5zPz4DTP3xQRWyNi6759vYwoSae2RkUdEYN0SvqezHxgutdk5m2ZuTEzN65Z08uIknRqa3LURwB3ADsz8yvlI0mSpmqyRb0J+BRwVUQ81f36WOFckqSuJquQ/xCIPmSRJE3DMxMlqXIWtSRVzqKWpMpZ1JJUOYtakipnUUtS5RbUKuRXnv/eYrOPHj1aaPKRIlOH1/Z+FeRJ+1/7VZG5y5YMFpl70cgFReZCuVXIxwqtkD1x+HCRuQBrzlxVZO6BAweKzD0+cazIXIDx8d6vnJ6ZMz7nFrUkVc6ilqTKWdSSVDmLWpIqZ1FLUuUsakmqXJPrUQ9FxE8i4unuKuR/149gkqSOJsdRvwVclZlvdld6+WFEfCczf1w4mySJZtejTuDN7t3B7tfMR2ZLknqq6ZqJAxHxFLAX2JKZrkIuSX3SqKgz81hmfhAYBq6IiEtPfI2rkEtSGfM66iMzDwCPAZunec5VyCWpgCZHfayJiFXd28uAjwDPF84lSepqctTHucDdETFAp9i/kZnfLhtLkjSpyVEfzwCX9SGLJGkanpkoSZWzqCWpcha1JFXOopakylnUklQ5i1qSKldkFfJfbF/C1RcN93zu2JE3537RO3ROodMpT1+xvMjcI4fKrTZ9wXvOKTJ39dllVvRev25dkbkARw4eKDJ30dCyInOPT4wXmQtw4FCZldMPHjxYZO7A0nLboccnxkpMnfEZt6glqXIWtSRVzqKWpMpZ1JJUOYtakipnUUtS5SxqSapc46Lurpv4s4jwWtSS1Efz2aK+GdhZKogkaXpNVyEfBj4O3F42jiTpRE23qL8KfJFZznGcugr5+LGZT4WUJM1Pk8VtrwH2ZuaTs71u6irkgwP+j1KSeqVJo24Cro2Il4D7gKsi4utFU0mSfmPOos7MWzNzODNHgOuB72fmJ4snkyQBHkctSdWb1/WoM/Mx4LEiSSRJ03KLWpIqZ1FLUuUsakmqnEUtSZWzqCWpcha1JFVuXofnNTU4OMi569b0fG6uPrvnMyctOn6syNyzzjityNzFZ6wsMhdg+dDSInOXLR0qMjfHx4rMBRg7dKjI3EXj40XmRmSRuQAH9v+6yNyly5YUmTu0pMz7GODg4d6/L3KWSyS5RS1JlbOoJalyFrUkVc6ilqTKWdSSVDmLWpIq1+jwvO6iAW8Ax4CJzNxYMpQk6W3zOY76ysz8VbEkkqRpuetDkirXtKgTeDQinoyIm0oGkiT9tqa7PjZl5u6IWAtsiYjnM/PxqS/oFvhNACuWljklVJJORY22qDNzd/f7XuBB4IppXnNbZm7MzI1LBwd7m1KSTmFzFnVErIiIlZO3gT8BtpcOJknqaLLrYx3wYERMvv4/M/O7RVNJkn5jzqLOzBeBP+hDFknSNDw8T5IqZ1FLUuUsakmqnEUtSZWzqCWpcha1JFUuMnu/anFE7AP+t+HLVwML6ap8Cy0vmLkfFlpeMHM/zCfvezNzzXRPFCnq+YiIrQvp+tYLLS+YuR8WWl4wcz/0Kq+7PiSpcha1JFWuhqK+re0A87TQ8oKZ+2Gh5QUz90NP8ra+j1qSNLsatqglSbNoragjYnNE/DwiXoiIW9rK0VREbIiIH0TEzojYERE3t52piYgYiIifRcS3287SRESsioj7I+L57s/6D9vONJeI+Kvue2J7RNwbEUNtZzpRRNwZEXsjYvuUx86KiC0R8cvu9zPbzDjVDHn/sfu+eCYiHoyIVS1GPMl0mac899cRkRGx+p3MbqWoI2IA+BrwUeAS4IaIuKSNLPMwAXwhM98PfAj4zALIDHAzsLPtEPPwL8B3M/N9dC6vW3X2iFgP/CWwMTMvBQaA69tNNa27gM0nPHYL8L3MvBD4Xvd+Le7i5LxbgEsz8wPAL4Bb+x1qDndxcmYiYgNwNfDyOx3c1hb1FcALmfliZo4B9wHXtZSlkczck5nburffoFMg69tNNbuIGAY+DtzedpYmIuJ04I+BOwAycywzD7QaqpnFwLKIWAwsB3a3nOck3TVO95/w8HXA3d3bdwN/2s9Ms5kub2Y+mpkT3bs/Bob7HmwWM/yMAf4Z+CKdRcLfkbaKej3wypT7o1ReelNFxAhwGfBEy1Hm8lU6b5DjLedo6nxgH/Af3d01t3eXf6tWZu4C/onO1tIe4PXMfLTdVI2ty8w90NkQAda2nGc+/hz4Ttsh5hIR1wK7MvPpdzOnraKOaR5bEIefRMRpwDeBz2fmwbbzzCQirgH2ZuaTbWeZh8XA5cC/ZeZlwCHq+jh+ku5+3euA84D3ACsi4pPtpvrdFhFforMr8p62s8wmIpYDXwL+9t3OaquoR4ENU+4PU+HHxRNFxCCdkr4nMx9oO88cNgHXRsRLdHYtXRURX2830pxGgdHMnPykcj+d4q7ZR4D/ycx9mTkOPAD8UcuZmvq/iDgXoPt9b8t55hQRNwLXAJ/I+o8t/j06f8Cf7v4eDgPbIuKc+Q5qq6h/ClwYEedFxBI6/3x5qKUsjURndd87gJ2Z+ZW288wlM2/NzOHMHKHz8/1+Zla9pZeZrwKvRMTF3Yc+DDzXYqQmXgY+FBHLu++RD1P5P0CneAi4sXv7RuBbLWaZU0RsBv4GuDYzD7edZy6Z+Wxmrs3Mke7v4Shwefd9Pi+tFHX3HwKfBR6h86b+RmbuaCPLPGwCPkVny/Sp7tfH2g71O+hzwD0R8QzwQeAf2o0zu+7W//3ANuBZOr9T1Z09FxH3Aj8CLo6I0Yj4NPBl4OqI+CWdoxK+3GbGqWbI+6/ASmBL9/fv31sNeYIZMvdmdv2fHiTp1OaZiZJUOYtakipnUUtS5SxqSaqcRS1JlbOoJalyFrUkVc6ilqTK/T/T7Dt4GzzH3AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show = img.copy()\n",
    "for pt in repred[0]:\n",
    "    print (pt.round())\n",
    "    show = cv2.circle(show, tuple((pt*0.5).astype(int)), 3, (0,255,255), -1)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-priority",
   "metadata": {},
   "source": [
    "<img src=\"./image/result2.png\" />\n",
    "\n",
    "눈의 왼쪽은 정말 잘 찾은 것 같다. 반면 눈 중심과 오른쪽 위치는 5픽셀 정도의 오차를 보이는 것 같다. 더 정확한 성능(적은 에러)를 원한다면 데이터를 추가로 넣어야 할 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
