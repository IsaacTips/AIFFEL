{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rough-victim",
   "metadata": {},
   "source": [
    "# 14. 카메라 스티커앱을 개선하자!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-aberdeen",
   "metadata": {},
   "source": [
    "## 1. 들어가며\n",
    "\n",
    "우리는 이전에 만들었던 카메라 스티커앱이 동영상을 처리할 수 있도록 개선하는 프로젝트를 진행하려고 한다. 하지만 지난 시간에 살펴 보았듯이, 동영상을 처리하는 카메라앱을 만들기 위해서는 단순 이미지를 처리하는 작업 이상으로 고려해야 할 것들이 있었다. 피사체인 사람이 움직이기 때문에 발생하는 여러가지 예외상황, 처리속도의 문제, 화면떨림 등의 안정성 문제 등에 대응하는 방법이 필요하다는 것을 알 수 있었다.\n",
    "\n",
    "이번 시간의 목표이다.\n",
    "\n",
    "1. 동영상 쉽게 다룰 수 있는 방법 익히기\n",
    "2. 스티커앱 성능 분석하기 + 간단한 개선 방법\n",
    "3. 원하는 스펙을 수치로 지정하기\n",
    "4. 칼만 필터로 동영상 성능 개선하기\n",
    "\n",
    "오늘은 스스로 문제를 찾고 조금씩 해결해 보는 과정이 많다. 어려워 보일 수도 있지만, 차근차근 해결해 나가면 이해가 훨씬 깊어진 스스로를 만날 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-safety",
   "metadata": {},
   "source": [
    "### 준비물\n",
    "\n",
    "---\n",
    "\n",
    "아직 프로젝트 폴더를 만들지 않았다면, 터미널을 열고 개인 실습 환경에 따라 경로를 수정, 프로젝트를 위한 디렉토리를 생성하자. 지난 시간에 생성했었다면 이 과정은 필요없을 것이다.\n",
    "\n",
    "```\n",
    "$ mkdir -p ~/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/models\n",
    "$ mkdir -p ~/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images\n",
    "\n",
    "```\n",
    "\n",
    "[new_video_sticker_app.zip](https://aiffelstaticprd.blob.core.windows.net/media/documents/new_video_sticker_app.zip)\n",
    "\n",
    "위 제공한 압축파일을 다운로드받아 아래와 같이 해당 디렉토리에서 작업하자.\n",
    "\n",
    "```\n",
    "$ cd ~/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/new_video_sticker_app.zip\n",
    "$ unzip new_video_sticker_app.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-alliance",
   "metadata": {},
   "source": [
    "## 2. 프로젝트 (1) moviepy로 비디오 처리하기\n",
    "\n",
    "먼저 우리는 동영상을 쉽게 다룰 수 있는 방법을 찾고 있다. 그래서 대안으로 떠오른 것은 **`moviepy`**라는 파이썬 기반의 동영상 처리 라이브러리이다.\n",
    "\n",
    "하지만 우리는 동영상을 다룰 때의 실행시간, 즉 처리 속도가 중요하다는 것도 알고 있다. 그래서 **`moviepy`**를 쓰는 것이 성능면에서 적합한지 알아보려고 한다.\n",
    "\n",
    "### 1. moviepy를 이용해서 주피터 노트북 상에서 비디오를 읽고 쓰는 프로그램을 작성하자.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intelligent-calculator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:22:12.519246Z",
     "start_time": "2021-05-10T16:22:11.873966Z"
    }
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoClip, VideoFileClip\n",
    "from moviepy.editor import ipython_display\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-recall",
   "metadata": {},
   "source": [
    "샘플로 제공된 **`video2.mp4`**를 **`moviepy`**로 읽어서 **`width=640`**으로 축소하여 화면에 플레이해 보고, 플레이한 내용을 **`mvpyresult.mp4`**라는 파일로 저장해 보겠다. 저장이 완료되면 샘플 원본과 새롭게 저장된 동영상 파일을 열어서 두 영상의 화면크기나 파일 사이즈를 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chief-agriculture",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:22:18.613909Z",
     "start_time": "2021-05-10T16:22:14.142439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/404 [00:00<?, ?it/s, now=None]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/aiffel-dj10/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult.mp4.\n",
      "MoviePy - Writing audio in mvpyresultTEMP_MPY_wvf_snd.mp3\n",
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/aiffel-dj10/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/aiffel-dj10/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult.mp4\n"
     ]
    }
   ],
   "source": [
    "# 읽기\n",
    "video_path = os.getenv('HOME')+'/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/video2.mp4'\n",
    "clip = VideoFileClip(video_path)\n",
    "clip = clip.resize(width=640)\n",
    "clip.ipython_display(fps=30, loop=True, autoplay=True, rd_kwargs=dict(logger=None))\n",
    "\n",
    "# 쓰기\n",
    "result_video_path = os.getenv('HOME')+'/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult.mp4'\n",
    "clip.write_videofile(result_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-yahoo",
   "metadata": {},
   "source": [
    "### 2. moviepy 로 읽은 동영상을 numpy 형태로 변환하고 영상 밝기를 50% 어둡게 만든 후에 저장하자.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "charming-alfred",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T02:10:00.726203Z",
     "start_time": "2021-04-28T02:09:56.083732Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  12%|█▏        | 48/403 [00:00<00:00, 439.52it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/aiffel-dj10/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult2.mp4.\n",
      "Moviepy - Writing video /home/aiffel-dj10/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/aiffel-dj10/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult2.mp4\n"
     ]
    }
   ],
   "source": [
    "# 읽기\n",
    "video_path = os.getenv('HOME')+'/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/video2.mp4'\n",
    "clip = VideoFileClip(video_path)\n",
    "clip = clip.resize(width=640)\n",
    "clip.ipython_display(fps=30, loop=True, autoplay=True, rd_kwargs=dict(logger=None))\n",
    "\n",
    "# clip 에서 numpy 로 데이터 추출\n",
    "vlen = int(clip.duration*clip.fps)\n",
    "video_container = np.zeros((vlen, clip.size[1], clip.size[0], 3), dtype=np.uint8)\n",
    "for i in range(vlen):\n",
    "    img = clip.get_frame(i/clip.fps)\n",
    "    video_container[i] = (img * 0.5).astype(np.uint8)\n",
    "\n",
    "# 새 clip 만들기\n",
    "dur = vlen / clip.fps\n",
    "outclip = VideoClip(lambda t: video_container[int(round(t*clip.fps))], duration=dur)\n",
    "\n",
    "# 쓰기\n",
    "result_video_path2 = os.getenv('HOME')+'/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult2.mp4'\n",
    "outclip.write_videofile(result_video_path2, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-croatia",
   "metadata": {},
   "source": [
    "### 3. 영상을 읽고 쓰는 시간을 측정해 주세요. OpenCV를 사용할 때와 차이를 측정해보자.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decimal-reader",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:26:19.956264Z",
     "start_time": "2021-05-10T16:26:16.940536Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  12%|█▏        | 48/403 [00:00<00:00, 429.50it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/aiffel-dj10/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult.mp4.\n",
      "Moviepy - Writing video /home/aiffel-dj10/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/aiffel-dj10/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult.mp4\n",
      "[INFO] moviepy time : 3.01ms\n"
     ]
    }
   ],
   "source": [
    "# CASE 1 : moviepy 사용\n",
    "start = cv2.getTickCount()\n",
    "clip = VideoFileClip(video_path)\n",
    "clip = clip.resize(width=640)\n",
    "\n",
    "vlen = int(clip.duration*clip.fps)\n",
    "video_container = np.zeros((vlen, clip.size[1], clip.size[0], 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(vlen):\n",
    "    img = clip.get_frame(i/clip.fps)\n",
    "    video_container[i] = (img * 0.5).astype(np.uint8)\n",
    "\n",
    "dur = vlen / clip.fps\n",
    "outclip = VideoClip(lambda t: video_container[int(round(t*clip.fps))], duration=dur)\n",
    "\n",
    "mvpy_video_path = os.getenv('HOME')+'/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/mvpyresult.mp4'\n",
    "outclip.write_videofile(mvpy_video_path, fps=30)\n",
    "\n",
    "time = (cv2.getTickCount() - start) / cv2.getTickFrequency()\n",
    "print (f'[INFO] moviepy time : {time:.2f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "modern-proposal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:26:21.616518Z",
     "start_time": "2021-05-10T16:26:19.964854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cv time : 1.65ms\n"
     ]
    }
   ],
   "source": [
    "# CASE 2 : OpenCV 사용\n",
    "start = cv2.getTickCount()\n",
    "vc = cv2.VideoCapture(video_path)\n",
    "\n",
    "cv_video_path = os.getenv('HOME')+'/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app/images/cvresult.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vw = cv2.VideoWriter(cv_video_path, fourcc, 30, (640,360))\n",
    "\n",
    "vlen = int(vc.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "for i in range(vlen):\n",
    "    ret, img = vc.read()\n",
    "    if ret == False: break\n",
    "    \n",
    "    img_result = cv2.resize(img, (640, 360)) * 0.5\n",
    "    vw.write(img_result.astype(np.uint8))\n",
    "    \n",
    "time = (cv2.getTickCount() - start) / cv2.getTickFrequency()\n",
    "print (f'[INFO] cv time : {time:.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-argentina",
   "metadata": {},
   "source": [
    "### 4. moviepy 를 이용할 때의 장단점을 분석해보자. 주피터 노트북에 답변을 작성해 코드와 함께 제출하자.\n",
    "---\n",
    "\n",
    "[참조](https://chacha95.github.io/2019-10-24-Movipy/#:~:text=MoviePy%EB%9E%80%20python%EC%9D%84%20%EC%9D%B4%EC%9A%A9%ED%95%9C,%EB%93%B1%EC%9D%84%20%ED%95%A0%20%EC%88%98%20%EC%9E%88%EC%8A%B5%EB%8B%88%EB%8B%A4.)\n",
    "\n",
    "* moviepy 활용 가능 범위\n",
    "    * 여러 비디오에 대해 처리할 때\n",
    "    * 여러 비디오를 복잡한 방식으로 합칠 때\n",
    "    * video effect를 추가하고 싶을 때(다른 video editor 없이)\n",
    "    * 여러 이미지를 이용해 GIF를 만들고 싶을 때\n",
    "* moviepy 특징\n",
    "    * 간단하며 직관적임\n",
    "    * Flexible함\n",
    "    * Protable함\n",
    "    * numpy와의 호환성\n",
    "    \n",
    "* 단점\n",
    "    * stream video에 대한 작업엔 적합하지 않다. \n",
    "    * 비디오의 개수가 많을 경우(100개 이상) 적합하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-receiver",
   "metadata": {},
   "source": [
    "## 3. 프로젝트 (2) 어디까지 만들고 싶은지 정의하기\n",
    "\n",
    "### 1. 실시간 카메라 스티커앱을 만들어봅시다.\n",
    "---\n",
    "\n",
    "이전에 우리는 **`img2sticker_orig`**라는 이름의 메소드를 구현하여 **`addsticker.py`**에 저장하였다. 해당 메소드를 그대로 복사하여 **`img2sticker`**라는 이름의 메소드를 만들고 이를 **`newaddsticker.py`**에 저장하겠다. 오늘의 프로젝트는 이 메소드를 보완하여, 구현하는 방식으로 진행될 것이다.\n",
    "\n",
    "동영상 입력을 받는 방법에는 어떤 것이 있을까? 만약 지금 사용하고 있는 컴퓨터에 웹캠이 달려 있다면 아주 손쉽게 진행 가능하다. 만약 웹캠을 이용할 수 없다 해도 실망하지 않아도 된다. 가지고 있는 스마트폰을 활용하면 되기 때문이다.\n",
    "\n",
    "- **CASE 1 : 웹캠 입력을 사용하는 경우**\n",
    "\n",
    "**`cv2.VideoCapture(0)`**를 이용하면 웹캠 입력을 받아올 수 있다. 현재의 기본적인 **`img2sticker`** 를 활용하여 가장 기본적인 웹캠 실시간 스티커앱을 아래와 같이 빠르게 만들어 보겠다. 여기서 파라미터로 주어지는 **`0`**은 시스템에 연결된 영상 입력장치의 인덱스이다. 대부분의 경우 웹캠이 하나만 달려있을 테니, **`0`**을 사용하면 된다.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "from newaddsticker import img2sticker\n",
    "\n",
    "detector_hog = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('./models/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "def main():\n",
    "    cv2.namedWindow('show', 0)\n",
    "    cv2.resizeWindow('show', 640, 360)\n",
    "\n",
    "    vc = cv2.VideoCapture(0)# 연결된 영상 장치의 인덱스, 하나만 있는 경우 0을 사용\n",
    "    img_sticker = cv2.imread('./images/king.png')\n",
    "\n",
    "    vlen = int(vc.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print (vlen)# 웹캠은 video length 가 0 입니다.# 정해진 길이가 없기 때문에 while 을 주로 사용합니다.# for i in range(vlen):while True:\n",
    "        ret, img = vc.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        start = cv2.getTickCount()\n",
    "        img = cv2.flip(img, 1)# 보통 웹캠은 좌우 반전# 스티커 메소드를 사용\n",
    "        img_result = img2sticker(img, img_sticker.copy(), detector_hog, landmark_predictor)\n",
    "\n",
    "        time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 1000\n",
    "        print ('[INFO] time: %.2fms'%time)\n",
    "\n",
    "        cv2.imshow('show', img_result)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "```\n",
    "\n",
    "위 코드를 **`webcam_sticker.py`**에 저장하였다. 이후 터미널에서 아래와 같이 실행해볼 수 있다.\n",
    "\n",
    "```bash\n",
    "$ cd ~/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app && python webcam_sticker.py\n",
    "\n",
    "```\n",
    "\n",
    "본인의 얼굴을 인식하고 왕관을 씌워주는 멋진 동영상 기반 스티커앱이 잘 동작하는 걸 확인하였을 것이다.\n",
    "\n",
    "- **CASE 2 : 스마트폰 영상의 스트리밍 입력을 사용하는 경우**\n",
    "\n",
    "만약 클라우드 환경에서 학습중이라면 실행환경에서 활용할 수 있는 웹캠이 없을 것이다. 그래서 이번엔 스마트폰에 있는 영상촬영 어플리케이션을 통해 녹화된 동영상 스트리밍을 입력으로 해서 진행해 보겠다. 절차가 다소 복잡하겠지만, 잘 따라오면 위 CASE 1과 동일하게 프로젝트를 진행할 수 있다.\n",
    "\n",
    "동영상 스트리밍을 위한 다양한 프로토콜 중, OpenCV로 처리할 수 있는 것으로 **`RTMP`**를 활용하도록 하겠다. 이 [링크](https://juyoung-1008.tistory.com/30)의 글을 참고하면 **`RTMP`**가 다양한 동영상 스트리밍 서비스에 널리 사용되고 있음을 알 수 있다.\n",
    "\n",
    "RTMP 스트리밍을 우리의 스티커앱의 입력으로 사용하기 위해서 다음과 같이 진행해보자.\n",
    "\n",
    "#### 1) 본인의 스마트폰에 RTMP 스트리밍 어플리케이션 설치\n",
    "\n",
    "스마트폰 카메라로 촬영한 영상을 RTMP로 스트리밍할 수 있게 해주는 어플리케이션은 많다. 그러나, RTMP 스트리밍을 위해서는 별도의 서버에서 **`rtmp://`** url을 오픈해 주어야 한다. 아래 어플리케이션은 그런 과정을 생략하고 RTMP url을 자동 생성해 주므로 편리하기 때문에 추천한다. 다른 어플리케이션을 활용해도 무방하다.\n",
    "\n",
    "<img src=\"./image/app.png\" />\n",
    "\n",
    "#### 2) 어플리케이션에서 동영상을 송출할 RTMP url을 지정\n",
    "\n",
    "영상촬영을 시작하기 전에 환경설정을 통해 동영상을 송출할 RTMP url을 지정해 주어야 한다. 위 어플리케이션의 환경설정 화면은 아래 그림과 같다.\n",
    "\n",
    "<img src=\"./image/url.png\" />\n",
    "\n",
    "빨간 테두리로 표시한 **`Server URL`** 항목을 누르면 아래 그림과 같이 rtmp url을 지정하는 화면이 나타난다. 이 어플리케이션의 좋은 점은 rtmp url을 자동 생성해 준다는 점이다. 그림에 있는 url은 생성될 때마다 달라질 것이다.\n",
    "\n",
    "<img src=\"./image/stream.png\" />\n",
    "\n",
    "화면에 보이는 **`rtmp://`** url 전체를 복사해서 보관하자.\n",
    "\n",
    "#### 3) 어플리케이션에서 영상 촬영 시작\n",
    "\n",
    "이제 RTMP 설정이 완료되었으므로, 촬영을 시작해보자. 본인의 얼굴이 나오게 카메라 방향을 지정해주어야 한다.\n",
    "\n",
    "#### 4) 카메라 스티커앱 구동\n",
    "\n",
    "이제 **`cv2.VideoCapture()`**를 이용해 촬영중인 동영상 스트리밍을 입력으로 받아올 수 있다. 단, 웹캠의 경우처럼 파라미터로 **`0`**을 줄수는 없을테니 어떻게 해야 할까? 바로 2)단계에서 복사해 두었던 rtmp url을 사용하면 된다.\n",
    "\n",
    "**`webcam_sticker.py`**에서 **`cv2.VideoCapture()`** 부분을 아래와 같이 바꾸기만 하면 충분하다.\n",
    "\n",
    "```python\n",
    "    vc = cv2.VideoCapture('rtmp://rtmp.streamaxia.com/streamaxia/XXXXXX')# XXXXXX 부분은 본인 어플리케이션에서 확인한 코드로 대체해 주세요.\n",
    "```\n",
    "\n",
    "이제 터미널에서 아래와 같이 실행하면, 스마트폰에서 촬영된 스트리밍 영상을 입력으로 하는 카메라 스티커앱이 동작하는 것을 확인할 수 있다.\n",
    "\n",
    "```bash\n",
    "$ cd ~/AIFFEL/Going_Deeper/GD13_Video_Sticker_App/video_sticker_app && python webcam_sticker.py\n",
    "\n",
    "```\n",
    "\n",
    "> 만약 클라우드 사용자라면 가상 데스크탑의 터미널에서 실행하는 것을 잊지 말자.\n",
    "\n",
    "\n",
    "### 2. 스티커앱을 실행하고 카메라를 고정하고 서서히 멀어져봅니다. 혹은 아주 가까이 다가가 봅니다. 얼굴을 찾지 못하는 거리를 기록해주세요\n",
    "---\n",
    "\n",
    "일반적으로 약 15cm ~ 1m 30cm 범위 사이에서 얼굴 인식이 가능하다고 한다. 실제로 측정했을 때 어떠한지 결과를 기록하자.\n",
    "\n",
    "* 최소거리 : 15cm 정도에서 꺼졌다.\n",
    "* 최대거리 : 1m 정도 이후로는 못했다.\n",
    "\n",
    "### 3. 다시 자리로 돌아온 후 고개를 상하좌우로 움직여주세요. *yaw, pitch, roll* 각도의 개념을 직접 실험해 보고 각각 몇 도까지 정상적으로 스티커앱이 동작하는지 기록해주세요\n",
    "---\n",
    "(참고)\n",
    "\n",
    "- yaw : y축 기준 회전 → 높이 축\n",
    "- picth : x축 기준 회전 → 좌우 축\n",
    "- roll : z축 기준 회전 → 거리 축\n",
    "\n",
    "일반적인 허용 범위는 아래와 같다고 알려져 있다.\n",
    "\n",
    "- yaw : -45 ~ 45도\n",
    "- pitch : -20 ~ 30도\n",
    "- roll : -45 ~ 45도\n",
    "\n",
    "실제 측정해 본 결과는 어떠한지 기록해보자.\n",
    "\n",
    "* 고개를 좌우로 돌리면서 확인했는데, 20도 정도도 못가는 거 같다.\n",
    "* 안경을 벗으니까 더 인식을 잘하는 듯 하다.\n",
    "\n",
    "### 4. 만들고 싶은 스티커앱의 스펙(허용 거리, 허용 인원 수, 허용 각도, 안정성)을 정해주세요\n",
    "---\n",
    "(예시)\n",
    "\n",
    "- 거리 : 25cm ~ 1m → 너무 가까우면 스티커의 의미가 없음, 셀카봉을 들었을 때의 유효거리\n",
    "- 인원 수 : 4명 → 4인 가족 기준\n",
    "- 허용 각도 : pitch : -20 ~ 30도, yaw : -45 ~ 45도, roll : -45 ~ 45도 → 화면을 바라볼 수 있는 각도\n",
    "- 안정성 : 위 조건을 만족하면서 FPPI (false positive per image) 기준 < 0.003, MR (miss rate) < 1 300장당 1번 에러 = 10초=30*10에 1번 에러\n",
    "\n",
    "기준의 이유를 어떻게 정했는지가 중요하다. → 서비스 관점, 엔지니어링 관점으로 설명하면 좋다.\n",
    "\n",
    "* 거리: 25cm ~ 1m\n",
    "* 인원수: 1명\n",
    "* 허용각도:\n",
    "    * yaw : -45 ~ 45도\n",
    "    * pitch : -20 ~ 30도\n",
    "    * roll : -45 ~ 45도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-hudson",
   "metadata": {},
   "source": [
    "## 4. 프로젝트 (3) 스티커 Out Bound 예외처리 하기\n",
    "\n",
    "이전 스텝에서 만든 기본 웹캠 스티커앱의 문제점들을 찾아서 보완해가도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-member",
   "metadata": {},
   "source": [
    "### 1. 지금까지 만든 스티커앱을 이용해서 예외 상황을 찾아주세요. 특히 서서히 영상에서 좌우 경계 밖으로 나가며 코드의 행동을 확인해 보세요.\n",
    "---\n",
    "웹캠을 이용해서 자유롭게 촬영하면 된다. 발생하는 예외 상황을 꼼꼼하게 기재하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-banks",
   "metadata": {},
   "source": [
    "### 2. 문제가 어디에서 발생하는지 코드에서 확인합니다.\n",
    "---\n",
    "(힌트) 얼굴이 카메라 왼쪽 경계를 벗어나서 detection 되는 경우 **`refined_x`** 의 값이 음수가 된다.**`img_bgr[..., refined_x:...]`** 에서 numpy array의 음수 index에 접근하게 되므로 예외가 발생한다.\n",
    "\n",
    "**`newaddsticker.py`**의 **`img2sticker`** 메소드에서 아래 부분을 수정해 주어야 합니다.\n",
    "\n",
    "```python\n",
    "### (이전 생략) #### sticker\n",
    "img_sticker = cv2.resize(img_sticker, (w,h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "refined_x = x - w // 2\n",
    "refined_y = y - h\n",
    "\n",
    "if refined_y < 0:\n",
    "    img_sticker = img_sticker[-refined_y:]\n",
    "    refined_y = 0\n",
    "\n",
    "#### TODO : x 축 예외처리 코드 추가###\n",
    "\n",
    "img_bgr = img_orig.copy()\n",
    "sticker_area = img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]]\n",
    "\n",
    "img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]] = \\\n",
    "    cv2.addWeighted(sticker_area, 1.0, img_sticker, 0.7, 0)\n",
    "\n",
    "return img_bgr\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-interest",
   "metadata": {},
   "source": [
    "### 3. Out bound 오류(경계 밖으로 대상이 나가서 생기는 오류)를 해결해 주세요.\n",
    "---\n",
    "위 예외처리 코드 부분에 들어가야 하는 코드는 아래와 같다. **`newaddsticker.py`** 파일을 수정하자.\n",
    "\n",
    "```python\n",
    "    if refined_x < 0:\n",
    "        img_sticker = img_sticker[:, -refined_x:]\n",
    "        refined_x = 0\n",
    "    elif refined_x + img_sticker.shape[1] >= img_orig.shape[1]:\n",
    "        img_sticker = img_sticker[:, :-(img_sticker.shape[1]+refined_x-img_orig.shape[1])]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-vocabulary",
   "metadata": {},
   "source": [
    "### 4. 다른 예외는 어떤 것들이 있는지 정의해 주세요. 어떤 것이 문제가 되는지 스스로 정해봅시다.\n",
    "---\n",
    "꼼꼼이 찾아보면 위에서 수정한 것과 같은 명백한 오류 발생 케이스 이외에도 다양한 예외 상황을 찾아볼 수 있을 것이다.\n",
    "\n",
    "정확한 정답은 없다. 본인이 정의하기 나름이다.<br>\n",
    "ex) `고개를 왼쪽아래로 향하게 할 때 스티커의 모양이 일정한 것.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-queensland",
   "metadata": {},
   "source": [
    "## 5. 프로젝트 (4) 스티커앱 분석 - 거리, 인원 수, 각도, 시계열 안정성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-sheriff",
   "metadata": {},
   "source": [
    "### 1. 멀어지는 경우에 왜 스티커앱이 동작하지 않는지 분석해주세요. detection, landmark, blending 단계 중 무엇이 문제일까요?\n",
    "---\n",
    "dlib detection 이 문제이다. 멀어지면 **`detector_hog`** 단계에서 bbox 가 출력되지 않는다.\n",
    "\n",
    "```python\n",
    "# preprocess\n",
    "    img_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "# detector\n",
    "    img_rgb_vga = cv2.resize(img_rgb, (640, 360))\n",
    "    dlib_rects = detector_hog(img_rgb_vga, 0)\n",
    "    if len(dlib_rects) < 1:\n",
    "        return img_orig\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-bennett",
   "metadata": {},
   "source": [
    "### 2. detector_hog의 문제를 해결하기 위해, 이미지 피라미드를 조절하여 성능을 향상시키는 간단한 방법이 있습니다. 이 방법을 활용하여 img2sticker 메소드를 간단히 고쳐 봅시다.\n",
    "---\n",
    "```python\n",
    "def img2sticker(img_orig, img_sticker, detector_hog, landmark_predictor):\n",
    "# preprocessimg_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# detectorimg_rgb_vga = cv2.resize(img_rgb, (640, 360))\n",
    "    dlib_rects = detector_hog(img_rgb_vga, 1) # <- 이미지 피라미드 수 변경\n",
    "    if len(dlib_rects) < 1:\n",
    "        return img_orig\n",
    "\n",
    "# landmarklist_landmarks = []\n",
    "    for dlib_rect in dlib_rects:\n",
    "        points = landmark_predictor(img_rgb_vga, dlib_rect)\n",
    "        list_points = list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "        list_landmarks.append(list_points)\n",
    "\n",
    "```\n",
    "수정 후에 **`webcam_sticker.py`** 를 다시한번 실행하여 스티커앱이 잘 실행되는지 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-quality",
   "metadata": {},
   "source": [
    "### 3. 위에서 새롭게 시도한 방법의 문제점은 무엇인가요?\n",
    "---\n",
    "속도가 현저히 느려진다. 기존 30ms/frame 에서 120ms/frame 으로 약 4배 느려짐 → 실시간 구동이 불가능 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-aggregate",
   "metadata": {},
   "source": [
    "### 4. 실행시간을 만족할 수 있는 방법을 찾아봅시다.\n",
    "---\n",
    "hog 디텍터를 딥러닝 기반 디텍터로 변경할 수 있다. hog 학습 단계에서 다양한 각도에 대한 hog 특징을 모두 추출해서 일반화 하기 어렵기 때문에 딥러닝 기반 검출기의 성능이 훨씬 좋다.\n",
    "\n",
    "딥러닝 기반 detection 방법을 조사한다. 아래 링크를 참고하면 도움이 될 것이다.<br>\n",
    "[How does the OpenCV deep learning face detector work?](https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/)\n",
    "\n",
    "opencv 는 intel cpu 을 사용할 때 dnn 모듈이 가속화를 지원하고 있다. 따라서 mobilenet 과 같은 작은 backbone 모델을 사용하고 ssd 를 사용한다면 충분히 만족할 만한 시간과 성능을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-citizenship",
   "metadata": {},
   "source": [
    "### 5. 인원 수, 각도 등 각 문제에 대해서 1 - 4번을 반복해주세요. (정해진 답은 없습니다.)\n",
    "---\n",
    "자유롭게 설계하자. 각도 문제에 대해서는 아래 방법을 적용해볼 수 있다.\n",
    "\n",
    "[Facial Landmark Detection](https://www.learnopencv.com/facial-landmark-detection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-blond",
   "metadata": {},
   "source": [
    "## 6. 프로젝트 (5) 칼만 필터 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-montreal",
   "metadata": {},
   "source": [
    "### 1. 카메라 앞에서 가만히 있을 때 스티커의 움직임을 관찰해 주세요. 어떤 문제가 발생하나요?\n",
    "---\n",
    "가만히 있어도 스티커의 크리가 일정하게 유지되지 않고, 떨리는 것처럼 보이는 현상이 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-victoria",
   "metadata": {},
   "source": [
    "### 2. 이론 강의에서 배운 칼만 필터를 적용해서 스티커 움직임을 안정화시켜 주세요.\n",
    "---\n",
    "칼만 필터를 구현한 모듈인 **`kalman.py`**와 이를 이용하여 tracker를 구현한 **`kpkf.py`**를 이용하여 칼만필터를 적용한 **`webcam_sticker_kf.py`**를 함께 첨부한다. 실행해 보면 현재는 웹캠이 아니라 샘플 동영상에 칼만필터가 적용된 것을 확인할 수 있다.\n",
    "\n",
    "동영상 입력을 웹캠으로 바꾸면 우리가 만들고 있는 웹캠에도 동일하게 적용할 수 있다. **`webcam_sticker_kf.py`**를 참고하여 자신만의 **`webcam_sticker.py`**를 완성하자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-harvest",
   "metadata": {},
   "source": [
    "지금까지 작성했던 주피터 노트북 파일과 파이썬 모듈(*.py)를 포함하여 제출하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-ecuador",
   "metadata": {},
   "source": [
    "### 회고\n",
    "---\n",
    "* vscode에서 오류가 있어서 터미널로 진행했다.\n",
    "* 칼만필터 부분은 이해가 잘 안돼서 스킵했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-geneva",
   "metadata": {},
   "source": [
    "### 루브릭\n",
    "---\n",
    "* 웹캠을 통한 스티커앱을 실행하고 다각도로 문제점을 분석하여 보았다.<br>\n",
    "(_거리, 인원수, 각도 등 다양한 측면에서의 문제점과 기술적 대안을 정리하였다._)\n",
    "\n",
    "    * __다양한 문제점과 대안을 정리했다.__\n",
    "    * https://drive.google.com/drive/folders/12v_FmtfSHpadYAU_JfmdVBlrik_BXkOu?usp=sharing\n",
    "\n",
    "* 스티커앱 초기버전이 가진 예외사항들을 분석하여 수정하였다.<br>\n",
    "( _프레임 좌표범위 예외처리 관련 오류를 수정하였다._)\n",
    "    \n",
    "    * __못했다.__\n",
    "\n",
    "* 칼만 필터를 웹캠 스티커앱에 적용하여 스티커의 안정성을 높여 보았다.<br>\n",
    "(_칼만 필터를 적용한 스티커앱을 작성하여 실행해 보고 안정성 여부를 확인하였다._)\n",
    "    \n",
    "    * __못했다.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
