{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8e0c3f",
   "metadata": {},
   "source": [
    "# 4. 영화리뷰 텍스트 감성분석하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d925c4",
   "metadata": {},
   "source": [
    "## 1. 들어가며\n",
    "\n",
    "오늘은 자연어 처리에 주로 활용되는 RNN(Recurrent Neural Network)에 대해 배워볼 예정이다. 그리고 컴퓨터 비전에서만 사용되는 줄 알았던 CNN(Convolutional Neural Network)이 자연어 처리에서 사용될 수도 있다는 것을 알게 될 것이다. 또한, 이 모델들의 구조를 학습하고 이를 활용하여 우리가 네이버나 다음 영화에서 확인할 수 있는 영화리뷰에 대한 **감성분석(sentiment analysis)**를 진행해보도록 하겠다.\n",
    "\n",
    "그럼 지금부터 시작하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d000ec",
   "metadata": {},
   "source": [
    "### 학습 목표\n",
    "\n",
    "---\n",
    "\n",
    "1. 텍스트 데이터를 머신러닝 입출력용 수치데이터로 변환하는 과정을 이해한다.\n",
    "2. RNN의 특징을 이해하고 시퀀셜한 데이터를 다루는 방법을 이해한다.\n",
    "3. 1-D CNN으로도 텍스트를 처리할 수 있음을 이해한다.\n",
    "4. IMDB와 네이버 영화리뷰 데이터셋을 이용한 영화리뷰 감성분류 실습을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8a5e7",
   "metadata": {},
   "source": [
    "## 2. 텍스트 감정분석의 유용성\n",
    "오늘 우리는 딥러닝을 통해 텍스트에 담긴 감성을 분석(Sentimental Analysis)하는 방법을 배워 볼 것이다. 구체적으로는 IMDb나 네이버 영화 리뷰 텍스트에 담긴 이용자의 감성이 긍정적인지 혹은 부정적인지를 분류(Classification)할 수 있는 딥러닝 모델을 만들어 볼 것이다.\n",
    "\n",
    "<img src=\"./image/text.png\" />\n",
    "<center><b>[텍스트에 담긴 미묘한 어감의 차이를 기계가 이해할 수 있게 하려면 어떻게 해야 할까요?]</b></center>\n",
    "\n",
    "---\n",
    "\n",
    "그런데 딥러닝을 이용한 텍스트 감성분석은 어떤 점에서 유용할까? 이 막연한 질문을 좀 더 세부적인 질문으로 잘게 쪼개면 다음과 같은 질문들로 나눠볼 수 있을 것이다.\n",
    "\n",
    "- 텍스트 데이터만이 가지고 있는 정보적 특성과 가치는 어떤 것일까?\n",
    "- 감성분석 등 텍스트 분류 모델이 다른 데이터 분석 업무에 어떤 점에서 도움을 줄까?\n",
    "- 텍스트 데이터 분석의 기술적 어려움은 무엇인가?\n",
    "- 텍스트 분류 작업을 하는데 딥러닝이 적용되면 어떤 점에서 유리해질까?\n",
    "\n",
    "이 질문들에 답을 제공하는 유용한 아티클 하나를 소개하겠다. 이 아티클을 정독하면서 위 질문들에 대한 답을 찾아서 스스로 정리해 보길 바란다. 하지만 정답이 있는 것은 아니다. 이 아티클을 통해 산업 현장에서 텍스트 분류가 실제로 활용되는 구체적인 사례도 확인할 수 있다.\n",
    "\n",
    "**참고문헌 : [동아비지니스리뷰 감성분석 활용사례 기고](https://dbr.donga.com/article/view/1202/article_no/8891/ac/magazine)**\n",
    "\n",
    "__텍스트 데이터에서만 얻을 수 있는 유용한 정보는 무엇일까? 그 유용성은 텍스트 데이터의 어떤 특징으로부터 비롯되는 것일까?__\n",
    "\n",
    "* SNS 등에서 광범위한 분량의 텍스트 데이터를 쉽게 얻을 수 있는데, 이 데이터는 소비자들의 개인적, 감성적 반응이 직접 담겨 있을뿐더러 실시간 트렌드를 빠르게 반영하는 데이터이기도 하다.\n",
    "\n",
    "__텍스트 감성분석 접근법을 크게 2가지로 나누면 무엇과 무엇이 있을까?__\n",
    "\n",
    "* 기계학습 기반 접근법과 감성사전 기반 접근법\n",
    "\n",
    "__사전 기반의 감성분석이 기계학습 기반 접근법 대비 가지는 한계점을 2가지__\n",
    "\n",
    "* 분석 대상에 따라 단어의 감성 점수가 달라질 수 있다는 가능성에 대응하기 어렵다.\n",
    "* 단순 긍부정을 넘어서 긍부정의 원인이 되는 대상 속성 기반의 감성 분석이 어렵다.\n",
    "\n",
    "__감성분석 등 텍스트 분류 모델이 다른 데이터분석 업무에 어떤 도움을 줄 수 있을까?__\n",
    "\n",
    "* 일반적인 데이터분석 업무는 범주화가 잘 된 정형데이터를 필요로 하는데, 이런 데이터를 큰 규모로 구축하기 위해서 많은 비용이 들지만, 쉽게 구할 수 있는 비정형데이터인 텍스트에 감성분석 기법을 적용하면 텍스트를 정형데이터로 가공하여 유용한 의사결정 보조자료로 활용할 수 있게 된다.\n",
    "\n",
    "__라벨링 비용이 많이 드는 머신러닝 기반 감성분석의 비용을 절감하면서 정확도를 크게 향상시킬 수 있는 자연어처리 기법에는 무엇이 있을까?__\n",
    "\n",
    "* 단어의 특성을 저차원 벡터값으로 표현할 수 있는 워드 임베딩(word embedding) 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136afb5",
   "metadata": {},
   "source": [
    "## 3. 텍스트 데이터의 특징\n",
    "\n",
    "인공지능 모델을 입력과 출력이 정해진 함수라고 생각해보자. 예를 들어 MNIST 숫자 분류기 모델이라면 이미지 파일을 읽어 들인 매트릭스가 입력이 되고, 이미지 파일에 쓰여 있는 실제 숫자 값이 출력이 되는 함수가 될 것이다.<br>\n",
    "이제 텍스트 문장을 입력으로 받아서 그 의미가 긍정이면 1, 부정이면 0을 출력하는 인공지능 모델을 만든다고 생각해보자. 이 모델을 만들기 위해서는 숫자 분류기를 만들 때는 생각할 필요가 없었던 2가지 문제가 생긴다.\n",
    "\n",
    "* 텍스트를 어떻게 숫자 행렬로 표현할 수 있을까?\n",
    "* 텍스트에는 순서가 중요하다. 입력데이터의 순서를 인공지능 모델에 어떻게 반영해야 할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245e22e",
   "metadata": {},
   "source": [
    "## 4. 텍스트 데이터의 특징 (1) 텍스트를 숫자로 표현하는 방법\n",
    "\n",
    "인공지능 모델의 입력이 될 수 있는 것은 0과 1의 비트로 표현 가능한 숫자만으로 이루어진 매트릭스일뿐이다.<br>\n",
    "아주 단순히, A=0, B=1, …, Z=25 라고 숫자를 임의로 부여한다고 해보자.<br>\n",
    "그러면 의미적으로 A와 B는 1만큼 멀고, A와 Z는 25만큼 멀까? 그렇지 않다. 텍스트의 중요한 특징은 그 자체로는 기호일 뿐이며, 텍스트가 내포하는 의미를 기호가 직접 내포하지 않는다는 점이다.\n",
    "\n",
    "하지만 우리는 우선 단어 사전을 만들어 볼 수는 있다. 우리가 사용하는 국어, 영어 사전에는 단어와 그 의미 설명이 짝지어져 있다.<br>\n",
    "우리가 하려는 것은 단어와 그 **단어의 의미를 나타내는 벡터**를 짝지어 보려고 하는 것이다. 그런데 그 벡터는 어디서 가져올까? 그렇다. 우리는 딥러닝을 통해 그 벡터를 만들어 낼 수 있다.\n",
    "\n",
    "아래와 같이 단 3개의 짧은 문장으로 이루어진 텍스트 데이터를 처리하는 간단한 예제를 생각해보겠다.\n",
    "\n",
    "> _i feel hungry_ <br>\n",
    "> _i eat lunch_ <br>\n",
    "> _now i feel happy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22afef14",
   "metadata": {},
   "source": [
    "우리의 텍스트 데이터로부터 사전을 만들기 위해 모든 문장을 단어 단위로 쪼갠 후에 파이썬 딕셔너리(dict) 자료구조로 표현해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68087c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word={}  # 빈 딕셔너리를 만들어서\n",
    "\n",
    "# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f657958b",
   "metadata": {},
   "source": [
    "단어 10개짜리 작은 딕셔너리가 만들어졌다. 하지만 우리가 가진 텍스트 데이터를 숫자로 바꿔 보려고 하는데, 텍스트를 숫자로 바꾸려면 위의 딕셔너리가 {텍스트:인덱스} 구조여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c42f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2244bd2",
   "metadata": {},
   "source": [
    "이 딕셔너리는 단어를 주면 그 단어의 인덱스를 반환하는 방식으로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16e250",
   "metadata": {},
   "source": [
    "이제 우리가 가진 텍스트 데이터를 숫자로 바꿔 표현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc3dd2",
   "metadata": {},
   "source": [
    "**`get_encoded_sentence`** 함수를 통해 아래와 같이 맵핑된 것을 확인할 수 있다.\n",
    "\n",
    "- **`<BOS>`** -> 1\n",
    "- i -> 3\n",
    "- eat -> 6\n",
    "- lunch -> 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ce532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37162cc4",
   "metadata": {},
   "source": [
    "반대로, encode된 벡터를 decode하여 다시 원래 텍스트 데이터로 복구할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69049426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea85454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c46eb1",
   "metadata": {},
   "source": [
    "여기서 정의된 함수들은 이후 스텝들에서 반복해서 활용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe8c77",
   "metadata": {},
   "source": [
    "## 5. 텍스트 데이터의 특징 (2) Embedding 레이어의 등장\n",
    "\n",
    "텍스트가 숫자로 변환되어 인공지능 모델의 입력으로 사용될 수 있게 되었지만, 이것으로 충분하지는 않다. 'i feel hungry'가 [1, 3, 4, 5]로 변환되었지만 이 벡터는 텍스트에 담긴 언어의 의미와 대응되는 벡터가 아니라 임의로 부여된 단어의 순서에 불과하다. 우리가 하려는 것은 단어와 그 단어의 의미를 나타내는 벡터를 짝짓는 것이었다. 그래서 단어의 의미를 나타내는 벡터를 훈련 가능한 파라미터로 놓고 이를 딥러닝을 통해 학습해서 최적화하게 된다. Tensorflow, Pytorch 등의 딥러닝 프레임워크들은 이러한 의미벡터 파라미터를 구현한 Embedding 레이어를 제공한다.\n",
    "\n",
    "<img src=\"./image/embedding.png\" />\n",
    "<center><b>[텍스트에 담긴 미묘한 어감의 차이를 기계가 이해할 수 있게 하려면 어떻게 해야 할까요?]</b></center>\n",
    "<center>https://wikidocs.net/64779</center>\n",
    "\n",
    "위 그림에서 word_to_index('great')는 1918이다. 그러면 'great'라는 단어의 의미공간상의 워드 벡터(word vector)는 Lookup Table형태로 구성된 Embedding 레이어의 1919번째 벡터가 된다. 위 그림에서는 [1.2, 0.7, 1.9, 1.5]가 된다. Embedding 레이어를 활용하여 이전 스텝의 텍스트 데이터를 워드 벡터 텐서 형태로 다시 표현해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f697ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 코드는 그대로 실행하시면 에러가 발생할 것입니다. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 위 그림과 같이 4차원의 워드 벡터를 가정합니다. \n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다. \n",
    "# list 형태의 sentences는 numpy array로 변환되어야 딥러닝 레이어의 입력이 될 수 있습니다.\n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index))\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fab696",
   "metadata": {},
   "source": [
    "실행해 보니 에러가 발생한다. 왜 그럴까?\n",
    "\n",
    "주의해야 할 점이 있다. Embedding 레이어의 인풋이 되는 문장 벡터는 그 **길이가 일정**해야 한다. raw_inputs의 3개 벡터의 길이는 각각 4, 4, 5이다. <br>\n",
    "Tensorflow에서는 **`keras.preprocessing.sequence.pad_sequences`**라는 편리한 함수를 통해 문장 벡터 뒤에 패딩(**`<PAD>`**)을 추가하여 길이를 일정하게 맞춰주는 기능을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "print(raw_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade22ed8",
   "metadata": {},
   "source": [
    "짧은 문장 뒤쪽이 0으로 채워지는 것을 확인할 수 있다. **`<PAD>`** 가 0에 맵핑되어 있다는 걸 기억하자.\n",
    "\n",
    "그러면 위에 시도했던 **`output = embedding(raw_inputs)`**을 다시 시도해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야 \n",
    "# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index))\n",
    "raw_inputs = keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028b1a6b",
   "metadata": {},
   "source": [
    "__output의 shape=(3, 5, 4)에서 3, 5, 4의 의미는 각각 무엇일까?__\n",
    "* 3은 입력문장 개수, 5는 입력문장의 최대 길이, 4는 워드 벡터의 차원 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb8820",
   "metadata": {},
   "source": [
    "## 6. 시퀀스 데이터를 다루는 RNN\n",
    "\n",
    "텍스트 데이터를 다루는 데 주로 사용되는 딥러닝 모델은 바로 **`Recurrent Neural Netowrk(RNN)`**이다. RNN은 시퀀스(Sequence) 형태의 데이터를 처리하기에 최적인 모델로 알려져 있다.\n",
    "\n",
    "텍스트 데이터도 시퀀스 데이터라는 관점으로 해석할 수 있지만, 시퀀스 데이터의 정의에 가장 잘 어울리는 것은 음성 데이터일 것이다. 시퀀스 데이터란 바로 입력이 시간축을 따라 발생하는 데이터이다. 예를 들어 이전 스텝의 'i feel hungry'라는 문장을 누군가가 초당 한 단어씩, 3초에 걸쳐 이 문장을 발음했다고 하자.\n",
    "\n",
    "> _at time=0s : 듣는이의 귀에 들어온 input='i'_<br>\n",
    "> _at time=1s : 듣는이의 귀에 들어온 input='feel'_<br>\n",
    "> _at time=2s : 듣는이의 귀에 들어온 input='hungry'_\n",
    "\n",
    "time=1s인 시점에서 입력으로 받은 문장은 'i feel' 까지이다. 그다음에 'hungry'가 올지, 'happy'가 올지 알 수 없는 상황이다. RNN은 그런 상황을 묘사하기에 가장 적당한 모델 구조를 가지고 있다. 왜냐하면 RNN은 시간의 흐름에 따라 새롭게 들어오는 입력에 따라 변하는 현재 상태를 묘사하는 state machine으로 설계되었기 때문이다.\n",
    "\n",
    "State가 무엇인지 이해를 돕기 위해 다음 그림을 보면서 질문에 대답해보자.\n",
    "\n",
    "<img src=\"./image/state.png\" />\n",
    "<center><b>[State가 유지된다는 것의 의미]</b></center>\n",
    "<center>https://www.slideshare.net/xguru/ss-16106464</center>\n",
    "\n",
    "__위 그림에서 대화가 stateful한지 stateless한지 결정하는 것은 직원인가, 아니면 손님인가?__\n",
    "\n",
    "* Stateful한 대화에서는 손님이 이전 시점에 어떤 선택을 했는지 직원이 기억을 하지만, Stateless한 대화에서는 직원이 기억하지 못한다. 그래서 손님 스스로 본인이 이전 시점에 했던 선택을 모두 기억하고 있다가 직원에게 매번 새롭게 전달해야 한다. 손님의 이전 주문내역을 기억하는 직원은 stateful하고, 그렇지 못한 직원은 stateless하다.\n",
    "\n",
    "다음 동영상을 통해 RNN의 기본 개념과 설계 구조를 좀 더 구체적으로 확인해보자.\n",
    "\n",
    "__김성훈 교수의 모두의 딥러닝 강좌 12강.RNN__\n",
    "\n",
    "[![김성훈 교수의 모두의 딥러닝 강좌 12강.RNN](http://img.youtube.com/vi/-SHPG_KMUkQ/0.jpg)](https://youtu.be/-SHPG_KMUkQ?t=0s) \n",
    "\n",
    "__RNN의 정의대로라면 t=4 시점의 state h4는 t=4 시점의 input x4와 t=3 시점의 state h3가 결정한다. 그렇다면 h4에는 t<4 이전의 입력 x1, x2, x3의 정보는 반영되지 않는 것일까?__\n",
    "\n",
    "* 그렇지 않다. h4를 결정하는 이전 state h3 안에 x3의 정보가 반영되어 있고, 같은 원리로 이전 시점의 모든 입력의 정보가 현재 상태에 반영될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a1df24",
   "metadata": {},
   "source": [
    "그러면 RNN 모델을 사용하여 이전 스텝의 텍스트 데이터를 처리하는 예제코드를 구현해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe181dac",
   "metadata": {},
   "source": [
    "## 7. 꼭 RNN이어야 할까?\n",
    "\n",
    "텍스트를 처리하기 위해 RNN이 아니라 **`1-D Convolution Neural Network(1-D CNN)`**를 사용할 수도 있다. 우리는 이미지 분류기를 구현하면서 **`2-D CNN`**을 이미 사용해 본 바 있다. 이미지는 시퀀스 데이터가 아니다. 이미지 분류기 모델에는 이미지 전체가 한꺼번에 입력으로 사용된다. <br>\n",
    "그러므로 **`1-D CNN`**은 문장 전체를 한꺼번에 한 방향으로 길이 7짜리 필터로 스캐닝하면서 7단어 이내에서 발견되는 특징을 추출하여 그것으로 문장을 분류하는 방식으로 사용된다. 이 방식도 텍스트를 처리하는 데 RNN 못지않은 효율을 보여준다. 그리고 CNN 계열은 RNN 계열보다 병렬처리가 효율적이기 때문에 학습속도도 훨씬 빠르게 진행된다는 장점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6999cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51cba44",
   "metadata": {},
   "source": [
    "아주 간단히는 **`GlobalMaxPooling1D()`** 레이어 하나만 사용하는 방법도 생각해 볼 수 있다. 이 방식은 전체 문장 중에서 단 하나의 가장 중요한 단어만 피처로 추출하여 그것으로 문장의 긍정/부정을 평가하는 방식이라고 생각할 수 있는데, 의외로 성능이 잘 나올 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d733f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533d511",
   "metadata": {},
   "source": [
    "이 외에도 1-D CNN과 RNN 레이어를 섞어 쓴다거나, FFN(FeedForward Network) 레이어만으로 구성하거나, 혹은 최근 각광받고 있는 Transformer 레이어를 쓰는 등 매우 다양한 시도를 해볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb88ac",
   "metadata": {},
   "source": [
    "## 8. IMDb 영화리뷰 감성분석 (1) IMDB 데이터셋 분석\n",
    "\n",
    "이제 본격적으로 IMDb 영화리뷰 감성분석 태스크에 도전해 보겠다. IMDb Large Movie Dataset은 50000개의 영어로 작성된 영화 리뷰 텍스트로 구성되어 있으며, 긍정은 1, 부정은 0의 라벨이 달려 있다. 2011년 [Learning Word Vectors for Sentiment Analysis](https://aiffelstaticprd.blob.core.windows.net/contents/[https://www.aclweb.org/anthology/P11-1015.pdf](https://www.aclweb.org/anthology/P11-1015.pdf) 논문에서 이 데이터셋을 소개하였다.\n",
    "\n",
    "50000개의 리뷰 중 절반인 25000개가 훈련용 데이터, 나머지 25000개를 테스트용 데이터로 사용하도록 지정되어 있다. 이 데이터셋은 tensorflow Keras 데이터셋 안에 포함되어 있어서 손쉽게 다운로드하여 사용할 수 있다. <br>\n",
    "이후 스텝의 IMDb 데이터셋 처리 코드 중 일부는 Tensorflow 튜토리얼에 언급된 데이터 전처리 로직을 참고하였음을 밝힌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "# IMDb 데이터셋 다운로드 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e477066",
   "metadata": {},
   "source": [
    "**`imdb.load_data()`** 호출 시 단어사전에 등재할 단어의 개수(**`num_words`**)를 10000으로 지정하면, 그 개수만큼의 **`word_to_index`** 딕셔너리까지 생성된 형태로 데이터셋이 생성된다.\n",
    "\n",
    "다운받은 데이터 실제 예시를 확인해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4151539",
   "metadata": {},
   "source": [
    "텍스트 데이터가 아니라 이미 숫자로 encode된 텍스트 데이터를 다운로드받았음을 확인할 수 있다.<br>\n",
    "이미 텍스트가 encode되었으므로 IMDb 데이터셋에는 encode에 사용한 딕셔너리까지 함께 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e687c4",
   "metadata": {},
   "source": [
    "IMDb 데이터셋의 텍스트 인코딩을 위한 **`word_to_index`**, **`index_to_word`**는 아래와 같이 보정되어야 한다. 아래 내용은 Tensorflow 튜토리얼의 가이드를 반영하여 작성하였다.<br>\n",
    "**`word_to_index`**는 IMDb 텍스트 데이터셋의 단어 출현 빈도 기준으로 내림차수 정렬되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac096c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<BOS>\"\n",
    "index_to_word[2] = \"<UNK>\"\n",
    "index_to_word[3] = \"<UNUSED>\"\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238add1",
   "metadata": {},
   "source": [
    "다운받은 데이터셋이 확인되었다. 마지막으로, encode된 텍스트가 정상적으로 decode되는지 확인해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd43519",
   "metadata": {},
   "source": [
    "decode한 문장과 라벨을 비교하여 일치하는지 확인하자.\n",
    "\n",
    "---\n",
    "\n",
    "**`pad_sequences`**를 통해 데이터셋 상의 문장의 길이를 통일하는 것을 잊어서는 안된다. 문장 최대 길이 **`maxlen`**의 값 설정도 전체 모델 성능에 영향을 미치게 된다. 이 길이도 적절한 값을 찾기 위해서는 전체 데이터셋의 분포를 확인해 보는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0af517",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39f232",
   "metadata": {},
   "source": [
    "위의 경우에는 **`maxlen=580`**이 된다.<br>\n",
    "또 한가지 유의해야 하는 것은 padding 방식을 문장 뒤쪽('post')과 앞쪽('pre') 중 어느쪽으로 하느냐에 따라 RNN을 이용한 딥러닝 적용 시 성능 차이가 발생한다는 점이다.<br>\n",
    "두 가지 방식을 한번씩 다 적용해서 RNN을 학습시켜 보면서 그 결과를 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3fc416",
   "metadata": {},
   "source": [
    "__RNN 활용 시 pad_sequences의 padding 방식은 'post'와 'pre' 중 어느 것이 유리할까?__\n",
    "\n",
    "* RNN은 입력데이터가 순차적으로 처리되어, 가장 마지막 입력이 최종 state 값에 가장 영향을 많이 미치게 된다. 그러므로 마지막 입력이 무의미한 padding으로 채워지는 것은 비효율적이다. 따라서 'pre'가 훨씬 유리하며, 10% 이상의 테스트 성능 차이를 보이게 된다.\n",
    "\n",
    "* <나는, 강아지다, 0, 0, 0, 0, 0, 0>에서 '나는'이 마지막 0까지 갈 경우 정보가 손실된다. 그러나 만일 pre를 쓰게 되면 <0 , 0, 0, 0, 0, 0, 나는, 강아지다> 문장의 정보의 손실이 적어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a270295",
   "metadata": {},
   "source": [
    "## 9. IMDb 영화리뷰 감성분석 (2) 딥러닝 모델 설계와 훈련\n",
    "RNN 모델을 직접 설계해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f783382",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff04bc2",
   "metadata": {},
   "source": [
    "(참고)우리가 사용할 수 있는 모델에는 RNN만 있는 것이 아니다. 이전 스텝에서 구현해 본 다양한 모델들이 전부 사용 가능하다.\n",
    "\n",
    "---\n",
    "\n",
    "model 훈련 전에, 훈련용 데이터셋 25000건 중 10000건을 분리하여 검증셋(validation set)으로 사용하도록 한다. 적절한 validation 데이터는 몇 개가 좋을지 고민해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59145985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32ece4",
   "metadata": {},
   "source": [
    "model 학습을 시작해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e248cb0",
   "metadata": {},
   "source": [
    "학습이 끝난 모델을 테스트셋으로 평가해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beaa8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da07aed0",
   "metadata": {},
   "source": [
    "**`model.fit()`** 과정 중의 train/validation loss, accuracy 등이 매 epoch마다 history 변수에 저장되어 있다.<br>\n",
    "이 데이터를 그래프로 그려 보면, 수행했던 딥러닝 학습이 잘 진행되었는지, 오버피팅 혹은 언더피팅하지 않았는지, 성능을 개선할 수 있는 다양한 아이디어를 얻을 수 있는 좋은 자료가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18899191",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fa84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c1d36",
   "metadata": {},
   "source": [
    "Training and validation loss를 그려 보면, 몇 epoch까지의 트레이닝이 적절한지 최적점을 추정해 볼 수 있다. validation loss의 그래프가 train loss와의 이격이 발생하게 되면 더 이상의 트레이닝은 무의미해지기 마련이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad581d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc45e0",
   "metadata": {},
   "source": [
    "마찬가지로 Training and validation accuracy를 그려 보아도 유사한 인사이트를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa70d0",
   "metadata": {},
   "source": [
    "## 10. IMDb 영화리뷰 감성분석 (3) Word2Vec의 적용\n",
    "\n",
    "이전 스텝에서 라벨링 비용이 많이 드는 머신러닝 기반 감성분석의 비용을 절감하면서 정확도를 크게 향상시킬 수 있는 자연어처리 기법으로 단어의 특성을 저차원 벡터값으로 표현할 수 있는 **`워드 임베딩(word embedding)`** 기법이 있다는 언급을 한 바 있다.\n",
    "\n",
    "우리는 이미 이전 스텝에서 워드 임베딩을 사용했다. 사용했던 model의 첫 번째 레이어는 바로 Embedding 레이어였다. 이 레이어는 우리가 가진 사전의 단어 개수 X 워드 벡터 사이즈만큼의 크기를 가진 학습 파라미터였다. 만약 우리의 감성분류 모델이 학습이 잘 되었다면, Embedding 레이어에 학습된 우리의 워드 벡터들도 의미 공간상에 유의미한 형태로 학습되었을 것이다. 한번 확인해보자.\n",
    "\n",
    "이번 스텝부터 워드 벡터 파일을 저장할 디렉토리를 먼저 생성하자. 그리고 워드 벡터를 다루는데 유용한 **`gensim`** 패키지를 설치하자.\n",
    "\n",
    "```bash\n",
    "$ mkdir -p ~/aiffel/sentiment_classification/data \n",
    "$ pip install gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280cd38",
   "metadata": {},
   "source": [
    "**`gensim`**에서 제공하는 패키지를 이용해, 위에 남긴 임베딩 파라미터를 읽어서 word vector로 활용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbf463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09aebc",
   "metadata": {},
   "source": [
    "위와 같이 얻은 워드 벡터를 가지고 재미있는 실험을 해볼 수 있다. 워드 벡터가 의미벡터 공간상에 유의미하게 학습되었는지 확인하는 방법 중에, 단어를 하나 주고 그와 가장 유사한 단어와 그 유사도를 확인하는 방법이 있다. **`gensim`**을 사용하면 아래와 같이 해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41921357",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f18cb",
   "metadata": {},
   "source": [
    "어떤가? love라는 단어와 유사한 다른 단어를 그리 잘 찾았다고 느껴지지는 않는다. 감성분류 태스크를 잠깐 학습한 것만으로 워드 벡터가 유의미하게 학습되기는 어려운 것 같다. 우리가 다룬 정도의 훈련데이터로는 워드 벡터를 정교하게 학습시키기 어렵다.\n",
    "\n",
    "그래서 이번에는 구글에서 제공하는 **`Word2Vec`**이라는 사전학습된(Pretrained) 워드 임베딩 모델을 가져다 활용해 보겠다. Word2Vec은 무려 1억 개의 단어로 구성된 Google News dataset을 바탕으로 학습되었다. 총 300만 개의 단어를 각각 300차원의 벡터로 표현한 것이다. Word2Vec이 학습되는 원리에 대해서는 차후 깊이있게 다루게 될 것이다. 하지만 그렇게 해서 학습된 Word2Vec이라는 것도 실은 방금 우리가 파일에 써본 Embedding Layer와 원리는 동일하다.\n",
    "\n",
    "임베딩의 개념에 대해 아주 잘 정리된 책 **`한국어 임베딩`**의 서론에서 왜 사전학습된 임베딩을 활용하는 것이 유리한지 설명해 주고 있다. 바로 전이학습 때문이다. 관련 내용을 읽어본 후 질문에 답해보자.\n",
    "\n",
    "[한국어 임베딩 서문](https://ratsgo.github.io/natural%20language%20processing/2019/09/12/embedding/)\n",
    "\n",
    "__사전에 학습된 Word2Vec 등의 임베딩 모델을 활용하는 전이학습(Transfer Learning)이 유리한 이유__\n",
    "\n",
    "* 사람도 무언가를 배우기 위해 제로베이스에서 시작하지 않고 자신이 지닌 이전의 경험과 지식을 동원하는 것처럼, 광범위한 데이터를 통해 미리 학습해 놓은 임베딩 속에 녹아 있는 의미, 문법 등의 부가적인 정보를 내가 만들려는 모델이 활용할 수 있는 피처로 활용하는 것이 훨씬 빠르고 정확하게 학습할 수 있는 방법이 된다.\n",
    "\n",
    "그러면 본격적으로 Google의 Word2Vec 모델을 가져와 적용해보자.\n",
    "\n",
    "이 [링크](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)를 클릭하면 무려 1.5GB 이상의 파일을 다운받게 된다. 다운받은 후에는 다음과 같이 진행하자.\n",
    "\n",
    "```bash\n",
    "$ mv ~/Downloads/GoogleNews-vectors-negative300.bin.gz ~/aiffel/sentiment_classification\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f47fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['computer']\n",
    "vector     # 무려 300dim의 워드 벡터입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff864c6e",
   "metadata": {},
   "source": [
    "300dim의 벡터로 이루어진 300만 개의 단어이다. 이 단어 사전을 메모리에 모두 로딩하면 아주 높은 확률로 메모리 에러가 날 것이다. 그래서 **`KeyedVectors.load_word2vec_format`** 메소드로 워드 벡터를 로딩할 때 가장 많이 사용되는 상위 100만 개만 **`limt`**으로 조건을 주어 로딩했다.\n",
    "\n",
    "메모리가 충분하다면 **`limt=None`**으로 하면 300만 개를 모두 로딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a787812a",
   "metadata": {},
   "source": [
    "어떤가? Word2Vec에서 제공하는 워드 임베딩 벡터들끼리는 의미적 유사도가 가까운 것이 서로 가깝게 제대로 학습된 것을 확인할 수 있다. 이제 우리는 이전 스텝에서 학습했던 모델의 임베딩 레이어를 Word2Vec의 것으로 교체하여 다시 학습시켜 볼 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a625a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원 수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57846d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35572d7b",
   "metadata": {},
   "source": [
    "어떤가? Word2Vec을 정상적으로 잘 활용하면 그렇지 않은 경우보다 5% 이상의 성능향상이 발생한다. 적절한 모델 구성, 하이퍼파라미터를 고려하여 감정분석 모델의 성능을 최대한으로 끌어올려 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0f598",
   "metadata": {},
   "source": [
    "## 11. 프로젝트 : 네이버 영화리뷰 감성분석 도전하기\n",
    "\n",
    "이전 스텝까지는 영문 텍스트의 감정분석을 진행해보았다. 그렇다면 이번에는 한국어 텍스트의 감정분석을 진행해 보면 어떨까? 오늘 활용할 데이터셋은 네이버 영화의 댓글을 모아 구성된 [Naver sentiment movie corpus](https://github.com/e9t/nsmc)이다.\n",
    "\n",
    "아래와 같이 다운로드를 진행하자.\n",
    "\n",
    "```bash\n",
    "$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
    "$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
    "$ mv ratings_*.txt ~/aiffel/sentiment_classification/data\n",
    "$ pip install konlpy\n",
    "$ sudo apt-get install git\n",
    "$ sudo apt-get install curl\n",
    "$ bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23365c96",
   "metadata": {},
   "source": [
    "### 1) 데이터 준비와 확인\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5dfeb4",
   "metadata": {},
   "source": [
    "### 2) 데이터로더 구성\n",
    "\n",
    "---\n",
    "\n",
    "실습때 다루었던 IMDB 데이터셋은 텍스트를 가공하여 imdb.data_loader() 메소드를 호출하면 숫자 인덱스로 변환된 텍스트와 word_to_index 딕셔너리까지 친절하게 제공한다. 그러나 이번에 다루게 될 nsmc 데이터셋은 전혀 가공되지 않은 텍스트 파일로 이루어져 있다. 이것을 읽어서 imdb.data_loader()와 동일하게 동작하는 자신만의 **`data_loader`**를 만들어 보는 것으로 시작한다. **`data_loader`** 안에서는 다음을 수행해야 한다.\n",
    "\n",
    "- 데이터의 중복 제거\n",
    "- NaN 결측치 제거\n",
    "- 한국어 토크나이저로 토큰화\n",
    "- 불용어(Stopwords) 제거\n",
    "- 사전**`word_to_index`** 구성\n",
    "- 텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "- X_train, y_train, X_test, y_test, word_to_index 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e76645",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b482937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499759e6",
   "metadata": {},
   "source": [
    "### 3) 모델구성을 위한 데이터 분석 및 가공\n",
    "\n",
    "---\n",
    "\n",
    "- 데이터셋 내 문장 길이 분포\n",
    "- 적절한 최대 문장 길이 지정\n",
    "- keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9758984d",
   "metadata": {},
   "source": [
    "### 4) 모델구성 및 validation set 구성\n",
    "\n",
    "---\n",
    "\n",
    "모델은 3가지 이상 다양하게 구성하여 실험해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22659ddb",
   "metadata": {},
   "source": [
    "### 5) 모델 훈련 개시\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0793cf",
   "metadata": {},
   "source": [
    "### 6) Loss, Accuracy 그래프 시각화\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2f76a",
   "metadata": {},
   "source": [
    "### 7) 학습된 Embedding 레이어 분석\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee78bc",
   "metadata": {},
   "source": [
    "### 8) 한국어 Word2Vec 임베딩 활용하여 성능개선\n",
    "\n",
    "---\n",
    "\n",
    "한국어 Word2Vec은 다음 경로에서 구할 수 있다.\n",
    "\n",
    "- [Pre-trained word vectors of 30+ languages](https://github.com/Kyubyong/wordvectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
