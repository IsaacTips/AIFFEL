{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "absent-field",
   "metadata": {},
   "source": [
    "# 6-7. 프로젝트: 멋진 작사가 만들기\n",
    "## Step 1. 데이터 다운로드\n",
    "---\n",
    "먼저 아래 링크에서 Song Lyrics 데이터를 다운로드하고 저장된 파일을 압축 해제한 후, 모든 txt 파일을 lyrics 폴더를 만들어 그 속에 저장하기.\n",
    "\n",
    "* [Song Lyrics](https://www.kaggle.com/paultimothymooney/poetry/data)\n",
    "\n",
    "아래의 명령어를 실행해도 된다.\n",
    "```\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "$ unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics 폴더에 압축풀기\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-pillow",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽어오기\n",
    "---\n",
    "`glob` 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이하다. `glob` 를 활용하여 모든 `txt` 파일을 읽어온 후, `raw_corpus` 리스트에 문장 단위로 저장!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inappropriate-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['I bought my first key from my baby momma brother', 'I bought my first key', 'Bought my bought my first key']\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-magic",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "level-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I bought my first key from my baby momma brother\n",
      "I bought my first key\n",
      "Bought my bought my first key\n",
      "I bought my first key from my baby momma brother\n",
      "I bought my first key\n",
      "Bought my bought my first key\n",
      "Yeah hustling on my city streets\n",
      "Trying to get a whole key i bought my frist key my first key\n",
      "Inand we was getting em like for twenty five\n",
      "Colombian connect homey we was getting fly\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-tulsa",
   "metadata": {},
   "source": [
    "`preprocess_sentence()` 함수를 사용해서 데이터 정제하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "earlier-state",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "strong-rendering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> i bought my first key from my baby momma brother <end>',\n",
       " '<start> i bought my first key <end>',\n",
       " '<start> bought my bought my first key <end>',\n",
       " '<start> i bought my first key from my baby momma brother <end>',\n",
       " '<start> i bought my first key <end>',\n",
       " '<start> bought my bought my first key <end>',\n",
       " '<start> yeah hustling on my city streets <end>',\n",
       " '<start> trying to get a whole key i bought my frist key my first key <end>',\n",
       " '<start> inand we was getting em like for twenty five <end>',\n",
       " '<start> colombian connect homey we was getting fly <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    if len(sentence.split()) <= 15:\n",
    "        corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-customs",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리\n",
    "---\n",
    "`tokenize()` 함수로 데이터를 Tensor로 변환한 후, `sklearn` 모듈의 `train_test_split()` 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하자. 단어장의 크기는 12,000 이상으로 설정, 총 데이터의 20%를 평가 데이터셋으로 사용하기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "apart-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   5 613 ...   0   0   0]\n",
      " [  2   5 613 ...   0   0   0]\n",
      " [  2 613  13 ...   0   0   0]\n",
      " ...\n",
      " [  2   5  22 ...   0   0   0]\n",
      " [  2   5  22 ...   0   0   0]\n",
      " [  2   5  22 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f9700e23710>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-oregon",
   "metadata": {},
   "source": [
    "단어사전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "southern-entity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-infrared",
   "metadata": {},
   "source": [
    "사전에는 없지만, 텐서 출력부에서 행 뒤쪽에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩(padding)을 채워넣은 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "binding-state",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   5 613  13 251 747  74  13  51 730 557   3   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[  5 613  13 251 747  74  13  51 730 557   3   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-shore",
   "metadata": {},
   "source": [
    "corpus 내의 첫번째 문장에 대해 생성된 소스와 타겟 문장을 확인해 보았다. 소스는 2(`<start>`)에서 시작해서 3(`<end>`)으로 끝난 후 0(`<pad>`)로 채워져 있다. 반면에 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한칸 시프트한 형태를 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-ferry",
   "metadata": {},
   "source": [
    "`sklearn` 모듈의 `train_test_split()` 함수를 사용해 훈련 데이터와 평가 데이터를 분리해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "developed-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input,test_size=0.2,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-solution",
   "metadata": {},
   "source": [
    "잘 분리되었는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "genetic-granny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (134685, 32)\n",
      "Target Train: (134685, 32)\n",
      "Source Train: (33672, 32)\n",
      "Target Train: (33672, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Train:\", enc_val.shape)\n",
    "print(\"Target Train:\", dec_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-colors",
   "metadata": {},
   "source": [
    "이미 데이터셋을 텐서 형태로 생성해 두었으므로, `tf.data.Dataset.from_tensor_slices()` 메소드를 이용해 `tf.data.Dataset`객체를 생성보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "satisfied-farming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 32), (256, 32)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cognitive-dominican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 32), (256, 32)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-account",
   "metadata": {},
   "source": [
    "## Step 5. 인공지능 만들기\n",
    "---\n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 `val_loss` 값을 2.2 수준으로 줄일 수 있는 모델을 설계하자!<br>\n",
    "(Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
    "\n",
    "```\n",
    "#Loss\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-technique",
   "metadata": {},
   "source": [
    "모델은 tf.keras.Model을 Subclassing하는 방식으로 만들 것이다. 모델은 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "grave-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "transparent-delta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 32, 12001), dtype=float32, numpy=\n",
       "array([[[-1.31574212e-04, -5.92743709e-05,  4.28307358e-05, ...,\n",
       "         -1.53623128e-04, -1.16963471e-04, -3.54338022e-06],\n",
       "        [-3.41774401e-04, -2.48245487e-04,  5.42396219e-06, ...,\n",
       "         -1.89827217e-04, -9.76208103e-05,  3.22780630e-04],\n",
       "        [-4.83807846e-04, -1.94802182e-04,  5.74491969e-05, ...,\n",
       "         -2.02980242e-04, -1.95241344e-04,  2.57759937e-04],\n",
       "        ...,\n",
       "        [ 9.62964623e-05, -3.07124713e-03,  3.28716094e-04, ...,\n",
       "          4.89815662e-04,  2.67816847e-03,  1.06990617e-03],\n",
       "        [ 1.14290517e-04, -3.06320586e-03,  2.99625652e-04, ...,\n",
       "          5.09878970e-04,  2.71984166e-03,  1.04257464e-03],\n",
       "        [ 1.29048407e-04, -3.05549148e-03,  2.73137091e-04, ...,\n",
       "          5.27583121e-04,  2.75675976e-03,  1.01917994e-03]],\n",
       "\n",
       "       [[-1.31574212e-04, -5.92743709e-05,  4.28307358e-05, ...,\n",
       "         -1.53623128e-04, -1.16963471e-04, -3.54338022e-06],\n",
       "        [ 6.42408486e-06, -3.10241376e-05,  2.18149027e-04, ...,\n",
       "          2.20186703e-05, -4.65813588e-04,  2.62731657e-04],\n",
       "        [ 7.47716604e-06, -3.53382420e-05,  1.79426017e-04, ...,\n",
       "          2.17367342e-04, -5.35662752e-04,  5.51868055e-04],\n",
       "        ...,\n",
       "        [ 3.11448421e-05, -2.99253268e-03,  6.09872397e-04, ...,\n",
       "          3.66686814e-04,  3.08648683e-03,  1.37952110e-03],\n",
       "        [ 7.12441688e-05, -3.00858938e-03,  5.53959049e-04, ...,\n",
       "          3.99729295e-04,  3.07715079e-03,  1.33417710e-03],\n",
       "        [ 1.03648897e-04, -3.01651750e-03,  5.01389673e-04, ...,\n",
       "          4.31954395e-04,  3.06609622e-03,  1.28849864e-03]],\n",
       "\n",
       "       [[-1.31574212e-04, -5.92743709e-05,  4.28307358e-05, ...,\n",
       "         -1.53623128e-04, -1.16963471e-04, -3.54338022e-06],\n",
       "        [-1.21365214e-04, -1.60564305e-04,  2.00876631e-04, ...,\n",
       "         -3.85220446e-05, -1.88816659e-04, -2.13418243e-04],\n",
       "        [-7.96578024e-05, -2.29853875e-04,  2.89906806e-04, ...,\n",
       "         -1.44571473e-04, -3.49262380e-04, -1.84599179e-04],\n",
       "        ...,\n",
       "        [ 2.19202426e-04, -3.04851402e-03,  1.94456181e-04, ...,\n",
       "          6.48543122e-04,  2.88348435e-03,  9.63738305e-04],\n",
       "        [ 2.18979752e-04, -3.04116006e-03,  1.77904338e-04, ...,\n",
       "          6.43354608e-04,  2.90120672e-03,  9.54513845e-04],\n",
       "        [ 2.18507703e-04, -3.03543336e-03,  1.63372024e-04, ...,\n",
       "          6.38556958e-04,  2.91633350e-03,  9.46957385e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.31574212e-04, -5.92743709e-05,  4.28307358e-05, ...,\n",
       "         -1.53623128e-04, -1.16963471e-04, -3.54338022e-06],\n",
       "        [-1.20725737e-04,  2.74841179e-04, -6.87633437e-05, ...,\n",
       "          2.61453442e-05,  1.11550235e-05,  6.29184724e-05],\n",
       "        [-1.66409547e-04,  4.03765560e-04, -3.80740559e-04, ...,\n",
       "          3.40788829e-05, -8.26623291e-05,  6.12717267e-05],\n",
       "        ...,\n",
       "        [ 3.53466248e-06, -3.29316990e-03,  2.49616191e-04, ...,\n",
       "          2.96886108e-04,  2.54319445e-03,  1.21616095e-03],\n",
       "        [ 2.83116224e-05, -3.25554865e-03,  2.51151476e-04, ...,\n",
       "          3.37815611e-04,  2.60375510e-03,  1.17119087e-03],\n",
       "        [ 4.98586705e-05, -3.21935187e-03,  2.48092663e-04, ...,\n",
       "          3.75823176e-04,  2.65684491e-03,  1.13041466e-03]],\n",
       "\n",
       "       [[-1.31574212e-04, -5.92743709e-05,  4.28307358e-05, ...,\n",
       "         -1.53623128e-04, -1.16963471e-04, -3.54338022e-06],\n",
       "        [ 2.66979878e-05, -1.74431028e-04,  3.12766409e-04, ...,\n",
       "         -1.15648945e-04, -1.46488121e-04,  4.48198298e-05],\n",
       "        [-1.84866934e-04, -1.25247010e-04,  1.92025356e-04, ...,\n",
       "         -4.06744475e-05, -1.52662004e-04,  3.40443454e-04],\n",
       "        ...,\n",
       "        [ 2.15835928e-04, -3.03565292e-03,  1.47371262e-04, ...,\n",
       "          5.84010442e-04,  2.91386805e-03,  9.70577530e-04],\n",
       "        [ 2.17508321e-04, -3.03256814e-03,  1.37654511e-04, ...,\n",
       "          5.86607843e-04,  2.92641181e-03,  9.59053927e-04],\n",
       "        [ 2.18356523e-04, -3.03018000e-03,  1.29163542e-04, ...,\n",
       "          5.88731375e-04,  2.93729850e-03,  9.49820969e-04]],\n",
       "\n",
       "       [[-1.31574212e-04, -5.92743709e-05,  4.28307358e-05, ...,\n",
       "         -1.53623128e-04, -1.16963471e-04, -3.54338022e-06],\n",
       "        [ 1.27876149e-06, -3.03485023e-04,  5.96445170e-04, ...,\n",
       "         -5.30158693e-04, -4.34473739e-04,  2.38949324e-05],\n",
       "        [ 3.12242337e-04, -5.39361848e-04,  9.91622801e-04, ...,\n",
       "         -9.43870342e-04, -5.97180682e-04,  1.48640538e-04],\n",
       "        ...,\n",
       "        [ 2.23468625e-04, -3.10291722e-03,  2.96533894e-04, ...,\n",
       "          5.41337999e-04,  2.88900570e-03,  1.01541658e-03],\n",
       "        [ 2.22645700e-04, -3.08926636e-03,  2.69754033e-04, ...,\n",
       "          5.51332661e-04,  2.90601328e-03,  9.98978969e-04],\n",
       "        [ 2.21465365e-04, -3.07720620e-03,  2.45630072e-04, ...,\n",
       "          5.60221379e-04,  2.92041316e-03,  9.85037419e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in val_dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "waiting-catalyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unexpected-divorce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "526/526 [==============================] - 155s 294ms/step - loss: 1.7052 - val_loss: 1.4714\n",
      "Epoch 2/10\n",
      "526/526 [==============================] - 163s 310ms/step - loss: 1.4158 - val_loss: 1.3882\n",
      "Epoch 3/10\n",
      "526/526 [==============================] - 157s 299ms/step - loss: 1.3432 - val_loss: 1.3373\n",
      "Epoch 4/10\n",
      "526/526 [==============================] - 159s 303ms/step - loss: 1.2851 - val_loss: 1.2994\n",
      "Epoch 5/10\n",
      "526/526 [==============================] - 159s 302ms/step - loss: 1.2341 - val_loss: 1.2661\n",
      "Epoch 6/10\n",
      "526/526 [==============================] - 160s 304ms/step - loss: 1.1878 - val_loss: 1.2405\n",
      "Epoch 7/10\n",
      "526/526 [==============================] - 162s 308ms/step - loss: 1.1456 - val_loss: 1.2186\n",
      "Epoch 8/10\n",
      "526/526 [==============================] - 162s 308ms/step - loss: 1.1062 - val_loss: 1.2003\n",
      "Epoch 9/10\n",
      "526/526 [==============================] - 166s 316ms/step - loss: 1.0698 - val_loss: 1.1845\n",
      "Epoch 10/10\n",
      "526/526 [==============================] - 169s 321ms/step - loss: 1.0352 - val_loss: 1.1728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96a8791610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "false-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#     from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "novel-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                      tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cathedral-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> he s a <unk> , and she s a <unk> <end> \n",
      "<start> i love you , i love you <end> \n",
      "<start> my name is prince <end> \n",
      "<start> i hate to see you <end> \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, tokenizer, init_sentence=\"<start> he\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> I love\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> My name\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> I hate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-refund",
   "metadata": {},
   "source": [
    "### embedding_size = 512 / hidden_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "instant-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lucky-tolerance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 32, 12001), dtype=float32, numpy=\n",
       "array([[[-2.1527823e-04, -1.1601710e-05, -3.8312809e-04, ...,\n",
       "          8.5193838e-05,  4.2805533e-04,  5.7565390e-05],\n",
       "        [-4.8929045e-04,  2.9334857e-05, -6.1145495e-04, ...,\n",
       "          1.2554229e-06,  6.7871076e-04, -3.1141451e-04],\n",
       "        [-2.6729330e-04,  1.6775583e-04, -7.2619913e-04, ...,\n",
       "         -3.6398816e-04,  9.4120624e-04, -5.0453946e-04],\n",
       "        ...,\n",
       "        [ 1.6228271e-03,  2.0656234e-03,  3.5932369e-03, ...,\n",
       "          5.8471859e-03, -3.7177503e-03, -6.2497118e-03],\n",
       "        [ 1.5848646e-03,  2.1218683e-03,  3.5528413e-03, ...,\n",
       "          5.9277397e-03, -3.7573285e-03, -6.2721549e-03],\n",
       "        [ 1.5493408e-03,  2.1746652e-03,  3.5153076e-03, ...,\n",
       "          5.9981872e-03, -3.7892971e-03, -6.2890016e-03]],\n",
       "\n",
       "       [[-2.1527823e-04, -1.1601710e-05, -3.8312809e-04, ...,\n",
       "          8.5193838e-05,  4.2805533e-04,  5.7565390e-05],\n",
       "        [-2.1442720e-04, -3.1306761e-05, -6.4488052e-04, ...,\n",
       "          3.1883875e-04,  9.4351568e-04,  3.2116132e-04],\n",
       "        [-3.9112713e-04,  7.5801050e-05, -5.4224225e-04, ...,\n",
       "          5.4863753e-04,  1.2265355e-03,  1.0172462e-03],\n",
       "        ...,\n",
       "        [ 1.4153632e-03,  2.2573124e-03,  3.4790251e-03, ...,\n",
       "          5.9404075e-03, -3.7802090e-03, -6.2978496e-03],\n",
       "        [ 1.3976032e-03,  2.2956277e-03,  3.4491159e-03, ...,\n",
       "          6.0110828e-03, -3.8104383e-03, -6.3064168e-03],\n",
       "        [ 1.3812971e-03,  2.3318594e-03,  3.4217909e-03, ...,\n",
       "          6.0721310e-03, -3.8344986e-03, -6.3124700e-03]],\n",
       "\n",
       "       [[-2.1527823e-04, -1.1601710e-05, -3.8312809e-04, ...,\n",
       "          8.5193838e-05,  4.2805533e-04,  5.7565390e-05],\n",
       "        [-9.9547775e-05, -1.2208050e-04, -1.1250813e-03, ...,\n",
       "         -1.2520494e-04,  3.8319532e-04,  1.7161510e-04],\n",
       "        [-1.0629812e-04, -2.2907656e-05, -1.3072322e-03, ...,\n",
       "         -3.9531299e-04,  1.9768921e-04,  3.7813144e-05],\n",
       "        ...,\n",
       "        [ 1.4869641e-03,  2.1408517e-03,  3.5568934e-03, ...,\n",
       "          5.9004487e-03, -3.7684885e-03, -6.2619960e-03],\n",
       "        [ 1.4627200e-03,  2.1954242e-03,  3.5205688e-03, ...,\n",
       "          5.9798844e-03, -3.8014832e-03, -6.2776110e-03],\n",
       "        [ 1.4400121e-03,  2.2459165e-03,  3.4864410e-03, ...,\n",
       "          6.0481331e-03, -3.8277081e-03, -6.2895021e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.1527823e-04, -1.1601710e-05, -3.8312809e-04, ...,\n",
       "          8.5193838e-05,  4.2805533e-04,  5.7565390e-05],\n",
       "        [-3.0467688e-04, -5.8919319e-05, -1.5825711e-04, ...,\n",
       "          5.4073450e-04,  7.7308615e-04, -1.6457433e-04],\n",
       "        [-3.0918795e-04,  3.9383678e-05, -3.2775999e-05, ...,\n",
       "          6.8061793e-04,  1.0409500e-03, -8.5790227e-05],\n",
       "        ...,\n",
       "        [ 1.7076029e-03,  1.9251908e-03,  3.6469384e-03, ...,\n",
       "          5.3272401e-03, -3.7171892e-03, -6.1139227e-03],\n",
       "        [ 1.6648256e-03,  1.9617905e-03,  3.6167044e-03, ...,\n",
       "          5.4630861e-03, -3.7679423e-03, -6.1683366e-03],\n",
       "        [ 1.6243795e-03,  2.0024278e-03,  3.5835213e-03, ...,\n",
       "          5.5852816e-03, -3.8090164e-03, -6.2110396e-03]],\n",
       "\n",
       "       [[-2.1527823e-04, -1.1601710e-05, -3.8312809e-04, ...,\n",
       "          8.5193838e-05,  4.2805533e-04,  5.7565390e-05],\n",
       "        [-3.0413037e-04, -4.5125042e-05, -1.8718260e-05, ...,\n",
       "          2.7662027e-04,  6.0309691e-04, -6.6324385e-05],\n",
       "        [-4.4267283e-05, -3.2072808e-04,  1.6120184e-04, ...,\n",
       "          2.1688495e-04, -1.2775516e-04, -4.7230086e-04],\n",
       "        ...,\n",
       "        [ 1.5214760e-03,  1.9456052e-03,  3.5239120e-03, ...,\n",
       "          5.7278518e-03, -3.8108225e-03, -6.2232204e-03],\n",
       "        [ 1.4953635e-03,  2.0175288e-03,  3.4892010e-03, ...,\n",
       "          5.8285277e-03, -3.8382418e-03, -6.2453458e-03],\n",
       "        [ 1.4707494e-03,  2.0848832e-03,  3.4573493e-03, ...,\n",
       "          5.9158094e-03, -3.8592392e-03, -6.2633967e-03]],\n",
       "\n",
       "       [[-2.1527823e-04, -1.1601710e-05, -3.8312809e-04, ...,\n",
       "          8.5193838e-05,  4.2805533e-04,  5.7565390e-05],\n",
       "        [-4.8897247e-05, -2.8062536e-04, -5.7478627e-04, ...,\n",
       "         -7.7836376e-06, -5.8637746e-05, -2.2638103e-04],\n",
       "        [ 3.2956581e-04, -2.6471415e-04, -4.5066371e-04, ...,\n",
       "         -4.6328633e-04, -6.2909874e-04, -5.2588200e-04],\n",
       "        ...,\n",
       "        [ 1.3741117e-03,  2.3025158e-03,  3.4056453e-03, ...,\n",
       "          6.1432300e-03, -3.8521711e-03, -6.3017057e-03],\n",
       "        [ 1.3599186e-03,  2.3424197e-03,  3.3817317e-03, ...,\n",
       "          6.1866292e-03, -3.8664259e-03, -6.3078930e-03],\n",
       "        [ 1.3468652e-03,  2.3790400e-03,  3.3606715e-03, ...,\n",
       "          6.2232586e-03, -3.8772798e-03, -6.3122408e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in val_dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "laden-announcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 33,133,793\n",
      "Trainable params: 33,133,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "micro-hardwood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "526/526 [==============================] - 183s 348ms/step - loss: 1.6743 - val_loss: 1.4500\n",
      "Epoch 2/10\n",
      "526/526 [==============================] - 490s 932ms/step - loss: 1.3854 - val_loss: 1.3553\n",
      "Epoch 3/10\n",
      "526/526 [==============================] - 230s 438ms/step - loss: 1.3035 - val_loss: 1.3023\n",
      "Epoch 4/10\n",
      "526/526 [==============================] - 169s 322ms/step - loss: 1.2378 - val_loss: 1.2578\n",
      "Epoch 5/10\n",
      "526/526 [==============================] - 167s 317ms/step - loss: 1.1802 - val_loss: 1.2242\n",
      "Epoch 6/10\n",
      "526/526 [==============================] - 167s 317ms/step - loss: 1.1275 - val_loss: 1.1970\n",
      "Epoch 7/10\n",
      "526/526 [==============================] - 176s 334ms/step - loss: 1.0790 - val_loss: 1.1743\n",
      "Epoch 8/10\n",
      "526/526 [==============================] - 175s 333ms/step - loss: 1.0339 - val_loss: 1.1569\n",
      "Epoch 9/10\n",
      "526/526 [==============================] - 175s 333ms/step - loss: 0.9914 - val_loss: 1.1409\n",
      "Epoch 10/10\n",
      "526/526 [==============================] - 174s 332ms/step - loss: 0.9508 - val_loss: 1.1278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f969871a850>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "signed-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                      tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "other-million",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> he s got a couple of animals <end> \n",
      "<start> i love you , i love you <end> \n",
      "<start> my name is prince <end> \n",
      "<start> i hate to see you smile <end> \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, tokenizer, init_sentence=\"<start> he\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> I love\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> My name\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> I hate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-pixel",
   "metadata": {},
   "source": [
    "### embedding_size = 256 / hidden_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "automated-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "varying-affiliation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 32, 12001), dtype=float32, numpy=\n",
       "array([[[-3.96370388e-06,  2.18157420e-05,  2.11191262e-04, ...,\n",
       "         -2.07480662e-06,  5.93301920e-05,  1.07609048e-04],\n",
       "        [ 1.82150397e-05, -4.40818621e-06,  3.74529947e-04, ...,\n",
       "         -6.47407505e-05, -3.79609010e-05,  1.23202248e-04],\n",
       "        [-1.73640947e-04, -5.62932910e-05,  3.00176558e-04, ...,\n",
       "          1.30866683e-04, -3.02157889e-04,  3.36859492e-04],\n",
       "        ...,\n",
       "        [ 1.92237785e-03,  2.85950303e-03, -4.06129472e-03, ...,\n",
       "          4.62022232e-04,  1.02168613e-03,  1.16832372e-04],\n",
       "        [ 1.91388722e-03,  2.88365781e-03, -4.11089649e-03, ...,\n",
       "          4.94447653e-04,  1.02099427e-03,  1.20408287e-04],\n",
       "        [ 1.90602208e-03,  2.90388078e-03, -4.15120507e-03, ...,\n",
       "          5.22487331e-04,  1.01961556e-03,  1.23087273e-04]],\n",
       "\n",
       "       [[-3.96370388e-06,  2.18157420e-05,  2.11191262e-04, ...,\n",
       "         -2.07480662e-06,  5.93301920e-05,  1.07609048e-04],\n",
       "        [ 1.24002894e-04,  2.49934150e-04,  3.42073821e-04, ...,\n",
       "          2.92989746e-04, -9.06316563e-05,  6.72030656e-05],\n",
       "        [ 2.79155502e-04,  4.03808459e-04,  3.39922175e-04, ...,\n",
       "          3.22264619e-04, -2.80669396e-04,  1.85604789e-04],\n",
       "        ...,\n",
       "        [ 1.94199383e-03,  2.62332102e-03, -3.58588109e-03, ...,\n",
       "         -1.36932533e-04,  1.08610420e-03,  1.23868478e-04],\n",
       "        [ 1.93525793e-03,  2.68829684e-03, -3.71287134e-03, ...,\n",
       "         -2.90920962e-05,  1.09159178e-03,  1.34244518e-04],\n",
       "        [ 1.92640873e-03,  2.74284440e-03, -3.82063468e-03, ...,\n",
       "          6.74024195e-05,  1.09261705e-03,  1.41243130e-04]],\n",
       "\n",
       "       [[-3.96370388e-06,  2.18157420e-05,  2.11191262e-04, ...,\n",
       "         -2.07480662e-06,  5.93301920e-05,  1.07609048e-04],\n",
       "        [ 7.19197633e-05,  2.18765417e-05,  2.38632812e-04, ...,\n",
       "         -8.47853298e-05, -7.91574857e-05,  8.84619512e-05],\n",
       "        [-1.28385072e-05,  1.56437509e-05,  1.93059910e-04, ...,\n",
       "         -2.12274608e-04, -1.49216619e-04,  3.01972614e-04],\n",
       "        ...,\n",
       "        [ 1.90398144e-03,  2.82600033e-03, -4.15775133e-03, ...,\n",
       "          5.34678053e-04,  1.02444412e-03,  1.09605935e-04],\n",
       "        [ 1.89552642e-03,  2.85307691e-03, -4.18813806e-03, ...,\n",
       "          5.61985071e-04,  1.02472468e-03,  1.19212877e-04],\n",
       "        [ 1.88815477e-03,  2.87610362e-03, -4.21245955e-03, ...,\n",
       "          5.84595604e-04,  1.02388265e-03,  1.26580097e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.96370388e-06,  2.18157420e-05,  2.11191262e-04, ...,\n",
       "         -2.07480662e-06,  5.93301920e-05,  1.07609048e-04],\n",
       "        [-6.40596918e-05,  7.85338343e-05,  4.55345784e-04, ...,\n",
       "          4.91123355e-05,  3.32546537e-04,  2.57114443e-04],\n",
       "        [-2.31786398e-04,  6.32673473e-05,  5.61844441e-04, ...,\n",
       "         -2.08974802e-06,  5.29277837e-04,  3.71150556e-04],\n",
       "        ...,\n",
       "        [ 1.91876141e-03,  2.29657791e-03, -3.22361779e-03, ...,\n",
       "          3.06065253e-04,  8.29877739e-04,  4.26187835e-05],\n",
       "        [ 1.91657501e-03,  2.39767693e-03, -3.40697309e-03, ...,\n",
       "          3.54499731e-04,  8.69068492e-04,  8.14963350e-05],\n",
       "        [ 1.91207358e-03,  2.48632720e-03, -3.56356381e-03, ...,\n",
       "          3.98520002e-04,  9.00803600e-04,  1.11582740e-04]],\n",
       "\n",
       "       [[-3.96370388e-06,  2.18157420e-05,  2.11191262e-04, ...,\n",
       "         -2.07480662e-06,  5.93301920e-05,  1.07609048e-04],\n",
       "        [ 9.22864201e-05,  3.29781818e-04,  4.72108923e-06, ...,\n",
       "          2.04416385e-04, -1.37078314e-04,  3.90574773e-04],\n",
       "        [ 1.09134482e-04,  6.04970672e-04,  3.59918740e-05, ...,\n",
       "          4.62675671e-04, -3.01771070e-04,  5.43990522e-04],\n",
       "        ...,\n",
       "        [ 1.93632685e-03,  2.41232617e-03, -3.69516364e-03, ...,\n",
       "          4.25982944e-05,  1.08644983e-03, -8.56744300e-05],\n",
       "        [ 1.94121222e-03,  2.49929330e-03, -3.80627485e-03, ...,\n",
       "          1.26745450e-04,  1.09476177e-03, -6.09011113e-05],\n",
       "        [ 1.94055086e-03,  2.57432880e-03, -3.90027533e-03, ...,\n",
       "          2.00889408e-04,  1.09823409e-03, -3.87053078e-05]],\n",
       "\n",
       "       [[-3.96370388e-06,  2.18157420e-05,  2.11191262e-04, ...,\n",
       "         -2.07480662e-06,  5.93301920e-05,  1.07609048e-04],\n",
       "        [ 2.10360886e-04, -3.35838966e-04,  1.36157192e-04, ...,\n",
       "         -2.35844946e-05,  1.02896440e-04,  2.21366790e-04],\n",
       "        [ 3.13781784e-04, -4.34622285e-04,  2.07495686e-04, ...,\n",
       "         -1.03777864e-04, -1.10948822e-04,  3.07828130e-04],\n",
       "        ...,\n",
       "        [ 1.92653155e-03,  2.86846515e-03, -4.20113327e-03, ...,\n",
       "          5.17853128e-04,  1.01107277e-03,  1.13099864e-04],\n",
       "        [ 1.91577140e-03,  2.89113238e-03, -4.22231387e-03, ...,\n",
       "          5.42619440e-04,  1.01221178e-03,  1.18171534e-04],\n",
       "        [ 1.90623046e-03,  2.91006546e-03, -4.23916662e-03, ...,\n",
       "          5.63735608e-04,  1.01243670e-03,  1.22329511e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in val_dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                      tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, tokenizer, init_sentence=\"<start> he\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> I love\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> My name\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> I hate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-mobility",
   "metadata": {},
   "source": [
    "## 루브릭\n",
    "\n",
    "__1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?__<br>\n",
    "텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?\n",
    "* 가사 텍스트 생성 모델이 정상적으로 작동하고, 그럴듯한 문장이 생성되었다.\n",
    "\n",
    "__2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?__<br>\n",
    "특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?\n",
    "* 특수문자 제거, 토크나이저 생성, 패딩처리 등 과정이 빠짐없이 진행되었다. 단, 패딩처리가 좀 과하게 된 느낌이다. 해결하려고 해도 해결되지 않았다.\n",
    "\n",
    "__3. 텍스트 생성모델이 안정적으로 학습되었는가?__<br>\n",
    "텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?\n",
    "* 잘은 모르겠지만 처음부터 loss가 2.2보다 낮았다. 솔직히 좀 모르겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-dover",
   "metadata": {},
   "source": [
    "## 느낀점\n",
    "\n",
    "그냥 겁나게 어려웠다. 자연어는 어렵다는 인식이 점점...ㅋㅋ 강해지는거 같다. 이해도 어렵고 생각도 못하고~ㅋㅋㅋ 그래도 그냥 버티고 있다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-handle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
