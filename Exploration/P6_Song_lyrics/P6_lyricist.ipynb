{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e36005d",
   "metadata": {},
   "source": [
    "# 6. 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931a427",
   "metadata": {},
   "source": [
    "## 1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e55a484",
   "metadata": {},
   "source": [
    "### **목차**\n",
    "\n",
    "---\n",
    "\n",
    "* 시퀀스? 스퀀스!\n",
    "* I 다음 am을 쓰면 반 이상은 맞더라\n",
    "* 실습\n",
    "    - 1) 데이터 다듬기\n",
    "    - 2) 인공지능 학습시키기\n",
    "    - 3) 잘 만들어졌는지 평가하기\n",
    "* 프로젝트 : 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa042933",
   "metadata": {},
   "source": [
    "'문장'이란 것은 무엇일까? 사전적 정의는 아래와 같다.\n",
    "\n",
    "> 생각이나 감정을 말과 글로 표현할 때 완결된 내용을 나타내는 최소의 단위.\n",
    "\n",
    "어떤 생각이나 감정을 말과 글로 표현하다... 멋지지만 인공지능에게 알려주기엔 조금 어려워 보인다. 하지만 많은 인공지능들이 이미 놀라운 수준의 작문을 해내고 있다. 아래 웹사이트에서 현존하는 최고의 인공지능 작문가, GPT-2에게 작문을 시켜보자! (현재는 GPT-3 모델도 발표되었다!)\n",
    "\n",
    "* [https://talktotransformer.com/](https://talktotransformer.com/)\n",
    "\n",
    "어떤가? 아직 한국어 작문 실력은 조금 어색하다. 아직 한국어 데이터를 충분히 학습한 것 같지는 않아 보인다. 그러나 GPT-2가 처음 나왔을 때엔 세간에 충격적인 인상을 남겼다.\n",
    "\n",
    "* [뛰어난 '문장 생성 인공지능'을 숨겨야만 하는 이유는?](https://decenter.kr/NewsView/1VFGQMBBXZ/GZ02)\n",
    "\n",
    "GPT-2 이전에도 작문을 할 수 있는 딥러닝 모델은 존재했다. 그러나 생성한 문장 길이가 일정 이상이 되면 주제의 일관성이 흐트러지면서 어색함이 드러나곤 했다. 그러나 GPT-2는 무려 신문기사 1편 정도의 길이의 글을 작문하면서 주제나 논리의 일관성을 어느정도 유지했다는 점에서 놀라움을 주었다. 2019년 2월, GPT-2를 발표한 OpenAI에서 문장 생성 모델의 오남용이 가져올 위험 때문에 해당 모델을 비공개하기로 하면서 위와 같은 기사들이 한동안 세간의 이슈가 된 바 있다. 그로부터 1년 여 후 2020년 5월에 OpenAI에서는 다시 GPT-2를 이전보다 훨씬 큰 규모로 발전시킨 GPT-3를 발표해서 다시한번 충격을 주었다. GPT-3가 만들어낸 텍스트는 그저 논리적 일관성을 유지하는 수준을 넘어서서 사람이 쓴 것과 구분이 안될 정도의 자연스러움을 보여 주었기 때문이다.\n",
    "\n",
    "왜 문장을 생성하는 인공지능이 이토록 충격을 주는 것일까? 인공지능의 대명사와 같은 알파고 같은 모델도 있지만, 일반인들이 흔히 `인공지능`이라고 할 때 흔히 떠올리는 것은 바로 인간과 자연어로 대화 가능한 인공지능이기 때문이다. 만약 인공지능이 사람의 언어에서 생각과 의도와 감정을 읽어 내고, 그 의미를 이해하며, 적절한 말을 만들어내서 인간의 질문에 대답할 수 있다면 그야말로 `지능을 가진 기계`라는 특이점에 이르렀다고 할 수 있기 때문이다.**\n",
    "\n",
    "오늘은 인공지능이 문장을 이해하는 방식과 작문을 가르치는 법을 배울 것이다.\n",
    "\n",
    "__작문 아이디어__\n",
    "\n",
    "* 구텐베르크 프로젝트의 방대한 소설 데이터를 활용해서 소설 쓰는 인공지능을 만들어 보는건 어떨까? \n",
    "* 인터넷에 많이 검색되는 자기소개서를 크롤링해서 자기소개서 대신 써주는 인공지능을 만들어 볼수도 있다. \n",
    "* 논문 데이터를 많이 모아서 그럴듯한 인공지능 논문을 써보는 시도도 재미있을 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26dfb0a",
   "metadata": {},
   "source": [
    "## 2. 시퀀스? 스퀀스!\n",
    "\n",
    "**시퀀스(Sequence)**란 무엇일까?\n",
    "\n",
    "<img src=\"./image/film.jpg\" />\n",
    "\n",
    "생각보다 시퀀스는 널리 사용되고 있다. 영화 분야에서도, 전기 분야에서도 쓰인다. 게다가 학창시절에 배운 수열을 영어로 시퀀스라고 한다.\n",
    "\n",
    "문장은 당연하고, 주가, 날짜, 심지어 드라마까지... 아주 많은 유형이 시퀀스 데이터에 포함된다. 그리고 그 데이터들을 \"Sequential\" 하다고 표현한다. 도대체 기준이 뭘까?\n",
    "\n",
    "* [파이썬 프로그래밍 입문서 (가제) - 5.2. 시퀀스](https://python.bakyeono.net/chapter-5-2.html)\n",
    "\n",
    "__[1, 2, 3, 100, -57.6] 은 시퀀스 데이터일까?__\n",
    "\n",
    "* 부를 수 있다. \n",
    "* 시퀀스 데이터는 나열된 데이터를 의미하므로, 각 요소들이 동일한 속성을 띌 필요가 없으며, 어떤 기준에 따라 정렬되어 있지 않아도 된다.\n",
    "\n",
    "__일상 속에서 찾아볼 수 있는 시퀀스 데이터를 두 가지__\n",
    "\n",
    "* 신문기사, 시, 소설 등 우리 주변의 모든 텍스트는 전부 시퀀스 데이터이다. \n",
    "* 텍스트가 아니더라도, 월별 상품 판매량 변화, 일별 주식 시황 데이터 등의 시계열 수치 데이터도 시퀀스 데이터로 볼 수 있다.\n",
    "\n",
    "시퀀스 데이터가 곧 각 요소들의 연관성을 의미하는 것은 아니지만, 우리(인공지능)가 예측을 하려면 __어느 정도는 연관성이 있어줘야 한다.__ 예를 들어, **`[ 18.01.01, 18.01.02, 18.01.03, ? ]`** 의 \"?\" 부분을 맞추기 위해선 정답이 **`18.01.04`** 여야 한다. 정답이 \"오리\"라면 난감하다는 것이다.\n",
    "\n",
    "문장을 구성하는 각 단어들은 문법이라는 규칙을 따라 배열되어 있다. 다만 이 **문법**이란 놈이 괘씸하다는 것을 교육과정을 통해 배웠다. 그런 의미에서 문장이라는 시퀀스 데이터는 꽤나 어렵다. 이런 문법을 인공지능이 그대로 배워서 문장 데이터를 예측할 리가 만무하니, 좀 더 단순한 접근 방법을 취해야한다. 바로 **통계에 기반한 방법**이다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e706a",
   "metadata": {},
   "source": [
    "## **3. I 다음 am을 쓰면 반 이상은 맞더라**\n",
    "\n",
    "**`나는 밥을 [ ]`** 에서 빈 칸에 들어갈 말이 **`먹는다`**라는 것을 우리는 큰 고민 없이 알 수 있다. **`밥`**은 통계적으로 **`먹`**이기 때문이다. **`알바생이 커피를 [ ]`** 라면 아마도 **`만든다`**가 정답일 것이다. 알바생이 **`커피`**를 마실 수도 있지만, 통계적으론 **`만드`**이기 때문이다.\n",
    "\n",
    "인공지능이 글을 이해하게 하는 방식도 위와 같다. 어떤 문법적인 원리를 통해서가 아니고, __수많은 글을 읽게 함으로써__ **`나는`** , **`밥을`**, 그 다음이 **`먹는다`** 라는 사실을 알게 하는 것이다. 그런 이유에서 __많은 데이터가 곧 좋은 결과__를 만들어낸다. 단어를 적재적소에 활용하는 능력이 발달된다고 할 수 있다.\n",
    "\n",
    "이 방식을 가장 잘 처리하는 인공지능 중 하나가 __순환신경망(RNN)__ 이다.\n",
    "\n",
    "<img src=\"./image/rnn.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51717bc1",
   "metadata": {},
   "source": [
    "위는 순환신경망의 작동 방법을 가장 단순하게 표현한 예시이다. 앞에서 **`먹었다`** 를 만드는 법은 배웠지만, 가장 첫 시작인 **`나는`** 은 어떻게 만들어야 할요?\n",
    "\n",
    "이는 **`<start>`** 라는 특수한 토큰을 맨 앞에 추가해주므로써 해결할 수 있다. 인공지능에게 \"자, 이제 어떤 문장이든 생성해봐!\" 라는 사인을 주는 셈이다. **`<start>`** 를 입력으로 받은 순환신경망은 다음 단어로 **`나는`** 을 생성하고, __생성한 단어를 다시 입력으로 사용__한다. 이 순환적인 특성을 살려 순환신경망이라고 이름을 붙인 것이다.\n",
    "\n",
    "그렇게 순차적으로 **`밥을 먹었다`** 까지 생성하고나면, 인공지능은 \"다 만들었어!\" 라는 사인으로 **`<end>`** 라는 특수한 토큰을 생성한다. 즉, 우리는 **`<start>`** 가 문장의 시작에 더해진 입력 데이터(문제지)와, **`<end>`** 가 문장의 끝에 더해진 출력 데이터(답안지)가 필요하며, 이는 **문장 데이터만 있으면 만들어낼 수 있다**는 것 또한 알 수 있다.\n",
    "\n",
    "위 과정을 **파이썬**으로는 아래와 같이 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bb7140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T15:48:37.268396Z",
     "start_time": "2021-05-28T15:48:37.261134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 문장: &lt;start&gt; 나는 밥을 먹었다 \n",
      "Target 문장:  나는 밥을 먹었다 &lt;end&gt;\n"
     ]
    }
   ],
   "source": [
    "sentence = \" 나는 밥을 먹었다 \"\n",
    "\n",
    "source_sentence = \"&lt;start&gt;\" + sentence\n",
    "target_sentence = sentence + \"&lt;end&gt;\"\n",
    "\n",
    "print(\"Source 문장:\", source_sentence)\n",
    "print(\"Target 문장:\", target_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078abe4",
   "metadata": {},
   "source": [
    "### 언어 모델 (Language Model)\n",
    "---\n",
    "\n",
    "`나는`, `밥을`, `먹었다` 를 순차적으로 생성할 때, `밥을` 다음이 `먹었다` 인 것은 쉽게 알 수 있다. 하지만 `나는` 다음이 `밥을` 인 것은 조금 억지처럼 느껴질 수 있다. 실제로 동작하는 방식도, `밥을` 을 만드는 것은 순전히 운이다. 우리가 의도한다고 나오는 것이 아니다.\n",
    "\n",
    "이걸 좀 더 확률적으로 표현해 보겠다. '나는 밥을' 다음에 '먹었다' 가 나올 확률을 KaTeX parse error: Expected 'EOF', got '먹' at position 3: p(먹̲었다 | 나는, 밥을) 이라고 하자. 그렇다면 이 확률은 '나는' 뒤에 '밥이' 가 나올 확률인 KaTeX parse error: Expected 'EOF', got '밥' at position 3: p(밥̲을|나는) 보다는 높게 나올 것이다. 아마 KaTeX parse error: Expected 'EOF', got '먹' at position 3: p(먹̲었다 | 나는, 밥을, 맛있…의 확률값은 더 높아질 것이다.<br>\n",
    "어떤 문구 뒤에 다음 단어가 나올 확률이 높다는 것은 그 다음 단어가 나오는 것이 보다 자연스럽다는 뜻이 된다. 그렇다면 '나는' 뒤에 '밥을'이 나오는 것이 자연스럽지 않다는 뜻일까? 그것은 아니다. '나는' 뒤에 올 수 있는 자연스러운 단어의 경우의 수가 워낙 많다 보니 불확실성이 높을 뿐이다.\n",
    "\n",
    "n-1개의 단어 시퀀스 $w\\_1, \\cdots, w\\_{n-1}$가 주어졌을 때, n번째 단어 $w\\_n$ 으로 무엇이 올지를 예측하는 확률 모델을 **언어 모델(Language Model)**이라고 부른다. 파라미터 $\\theta$로 모델링하는 언어 모델을 다음과 같이 표현할 수 있다.\n",
    "\n",
    "$$P(w\\_n | w\\_1, ..., w\\_{n-1};\\theta )$$\n",
    "\n",
    "잠깐 스크롤을 올려 RNN의 개념도를 잠깐 다시 보면, 정확히 $w\\_1, \\cdots, w\\_{n-1}$가 주어졌을 때, n번째 단어 $w\\_n$ 으로 무엇이 올지 예측하는 구조를 가지고 있음을 알아챌 수 있을 것이다. 이런 언어 모델을 어떻게 학습시킬 수 있을까? 간단하다. 어떤 텍스트도 언어 모델의 학습 데이터가 될 수 있다. n-1번째까지의 단어 시퀀스가 x_train이 되고 n번째 단어가 y_train이 되는 데이터셋은 무궁무진하게 만들 수 있기 때문이다. 이렇게 학습된 언어 모델을 학습 모드가 아닌 테스트 모드로 가동하면 어떤 일이 벌어질까? 그렇다, 이 모델은 일정한 단어 시퀀스가 주어진다면 다음 단어, 그 다음 단어를 계속해서 예측해 낼 것이다. 이게 바로 텍스트 생성이고 작문이다. __잘 학습된 언어 모델은 훌륭한 문장 생성기__로 동작하게 된다.\n",
    "\n",
    "이전 스텝에서 소개했던 GPT-2 같은 문장 생성기도 언어 모델의 한 종류에 불과하다. 딥러닝 모델의 구조나 파라미터 사이즈, 학습데이터의 양 등이 특별할 뿐, 기본적인 원리는 오늘 우리가 만들게 될 언어 모델과 전혀 다를게 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcfa9dd",
   "metadata": {},
   "source": [
    "## 4. 실습 (1) 데이터 다듬기\n",
    "지금부터는 실습을 해볼 것이다. 이번 실습에서는 연극의 대사를 학습해서 스스로 연극 대사 문장을 생성해내는 언어 모델 인공지능을 만들 예정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7fdc3",
   "metadata": {},
   "source": [
    "### **데이터 다운로드**\n",
    "\n",
    "---\n",
    "\n",
    "그럼 첫 번째 단계는 무엇일까? 인공지능의 시작은 언제나 그렇듯 데이터이다.\n",
    "\n",
    "다음과 같이 작업 디렉토리를 설정해주자.\n",
    "\n",
    "```bash\n",
    "$ mkdir -p ~/aiffel/lyricist/data\n",
    "$ mkdir -p ~/aiffel/lyricist/models\n",
    "```\n",
    "\n",
    "그리고 이번 실습에서 사용할 데이터를 다운로드받아 작업 디렉토리로 옮겨 주자. 이번 실습에서는 텐서플로우(TensorFlow)가 제공하는 셰익스피어의 연극 대본을 사용할 것이다.\n",
    "\n",
    "```bash\n",
    "$ wget https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "$ mv shakespeare.txt ~/aiffel/lyricist/data\n",
    "```\n",
    "\n",
    "1Mb 남짓한 텍스트 파일이 받아졌다. GPT-2를 학습시킬 때의 100Gb 가까이 되는 수준과는 아주 거리가 멀지만, 멋진 셰익스피어 같은 극작가 언어 모델이 탄생하길 기대된다.\n",
    "\n",
    "우선 실습에 사용할 라이브러리를 불러오자. 그리고 방금 다운받은 파일의 내용을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf8b426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "import os\n",
    "\n",
    "# 파일을 읽기모드로 열어 봅니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()   # 텍스트를 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "\n",
    "print(raw_corpus[:9])    # 앞에서부터 10라인만 화면에 출력해 볼까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b7bb7",
   "metadata": {},
   "source": [
    "경로를 따라 데이터를 읽어오면 데이터가 어떻게 생겼는지 눈으로 확인할 수 있다. 완벽한 연극 대본이다. 하지만 우린 **문장(대사)**만을 원하므로 화자 이름이나 공백뿐인 정보는 필요가 없다. 우리가 만들 언어 모델은 연극 대사를 만들어 내는 모델이기 때문이다.<br>\n",
    "1차 필터링을 할 필요가 있다. 데이터의 형태를 자세히 살피며 필터를 구상해보자.\n",
    "\n",
    "<img src=\"./image/data.png\" />\n",
    "<center>[▲ shakespeare.txt 파일의 일부분]</center>\n",
    "\n",
    "모든 문장을 하나하나 검사한다고 가정하자. 우리가 원치 않는 문장은 __화자가 표기된 문장(0, 3, 6)__, 그리고 __공백인 문장(2, 5, 9)__ 이다. 화자가 표기된 문장은 문장의 끝이 `:` 로 끝나게 되어 있다. 일반적으로 대사가 `:` 로 끝나는 일은 없을테니, `:` 를 기준으로 문장을 제외시켜도 괜찮을 것 같다. 그리고 공백인 문장은 길이를 검사하여 길이가 0이라면 제외를 시키자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be094c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d30f6",
   "metadata": {},
   "source": [
    "우리가 원하는 문장만 성공적으로 출력된다.\n",
    "\n",
    "텍스트 분류 모델에서 많이 보신 것처럼 텍스트 생성 모델에도 단어 사전을 만들게 된다. 그렇다면 문장을 일정한 기준으로 쪼개야 할것이다. 그 과정을 __토큰화(Tokenize)__ 라고 한다.\n",
    "\n",
    "가장 심플한 방법은 띄어쓰기를 기준으로 나누는 방법이고, 우리도 그 방법을 사용할 것이다. 하지만 약간의 문제가 있을 수 있다. 몇 가지 문제 케이스를 살펴보자.\n",
    "\n",
    "1. Hi, my name is John. *(\"Hi,\" \"my\", …, \"john.\" 으로 분리됨) - 문장부호\n",
    "2. First, open the first chapter. *(First와 first를 다른 단어로 인식) - 대소문자\n",
    "3. He is a ten-year-old boy. *(ten-year-old를 한 단어로 인식) - 특수문자\n",
    "\n",
    "\"1.\" 을 막기 위해 __문장 부호 양쪽에 공백을 추가__ 하고, \"2.\" 를 막기 위해 __모든 문자들을 소문자로 변환__하고. \"3.\"을 막기 위해 __특수문자들은 모두 제거__하여 해결할 수 있다.\n",
    "\n",
    "이런 전처리를 위해 정규표현식(Regex)을 이용한 필터링이 유용하게 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2cdc320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b09ab9",
   "metadata": {},
   "source": [
    "지저분한 문장을 넣어도 예쁘게 변환해주는 정제 함수가 완성되었다! 보너스로 이전 스텝에서 배운 **`<start>`** **`<end>`** 도 추가했다.\n",
    "\n",
    "그러면 우리가 구축해야 할 데이터셋은 어떤 모양이 될까?\n",
    "\n",
    "이전 스텝에서 봤던 예를 떠올려 보자.\n",
    "\n",
    "```text\n",
    "언어 모델의 입력 문장 :  <start> 나는 밥을 먹었다\n",
    "언어 모델의 출력 문장 : 나는 밥을 먹었다 <end>\n",
    "```\n",
    "\n",
    "자연어처리 분야에서 모델의 입력이 되는 문장을 **소스 문장(Source Sentence)**, 정답 역할을 하게 될 모델의 출력 문장을 **타겟 문장(Target Sentence)**라고 관례적으로 부른다. 각각 X_train, y_train 에 해당한다고 할 수 있다.\n",
    "\n",
    "그렇다면 우리는 위에서 만든 정제 함수를 통해 만든 데이터셋에서 토큰화를 진행한 후 끝 단어 **`<end>`**를 없애면 소스 문장, 첫 단어 **`<start>`**를 없애면 타겟 문장이 된다. 이 정제 함수를 활용해서 아래와 같이 정제 데이터를 구축하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd440e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e324c6e",
   "metadata": {},
   "source": [
    "새로운 언어를 배우는 상상을 해보자. 영어를 전혀 모르던 그 때로 돌아가서, 다시 영어를 배우려면 어떻게 해야 할까? **영한사전** 을 허리춤에 끼고 문장 속 단어를 하나하나 찾아가며 **한국어 해석** 을 적을 것이다. 이 아이디어는 인공지능에게도 똑같이 적용된다. **배우고자 하는 언어** 를 **모국어로 표현** 을 해야 공부를 할 수 있다.\n",
    "\n",
    "인공지능의 모국어라면 단연 **숫자**일 것이다. 우리는 가르칠 언어(데이터)를 숫자로 변환해서 인공지능에게 줄 것이다. 이에 필요한 것은 **사전**이다.\n",
    "\n",
    "텐서플로우는 자연어 처리를 위한 여러 가지 모듈을 제공하는데, 우리도 그 모듈을 십분 활용할 것이다. 아래에서 활용하게 될 **`tf.keras.preprocessing.text.Tokenizer`** 패키지는 정제된 데이터를 토큰화하고, 단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며, 데이터를 숫자로 변환까지 한 방에 해준다. 이 과정을 **벡터화(vectorize)** 라 하며, 숫자로 변환된 데이터를 **텐서(tensor)** 라고 칭한다. 우리가 사용하는 텐서플로우로 만든 모델의 입출력 데이터는 실제로는 모두 이런 텐서로 변환되어 처리되는 것이다.\n",
    "\n",
    "* [Tensor란 무엇인가](https://rekt77.tistory.com/102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce52335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f40134d7090>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc5fc8",
   "metadata": {},
   "source": [
    "생성된 텐서 데이터를 3번째 행, 10번째 열까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc95c3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837bb26a",
   "metadata": {},
   "source": [
    "텐서 데이터는 모두 정수로 이루어져 있다. 이 숫자는 다름 아니라, tokenizer에 구축된 단어 사전의 인덱스이다. 단어 사전이 어떻게 구축되었는지 아래와 같이 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd816ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180d43b",
   "metadata": {},
   "source": [
    "2번 인덱스가 바로 **`<start>`**였다. 왜 모든 행이 2로 시작하는지 이해할 수 있겠다.\n",
    "\n",
    "이제 생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게 하겠다. 이 과정도*텐서플로우*가 제공하는 모듈을 사용할 것이니, 어떻게 사용하는지만 눈여겨 보자.\n",
    "\n",
    "텐서 출력부에서 행 뒤쪽에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩(padding)을 채워넣은 것이다. 사전에는 없지만 0은 바로 패딩 문자 **`<pad>`**가 될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51cf0d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a1bd3",
   "metadata": {},
   "source": [
    "corpus 내의 첫번째 문장에 대해 생성된 소스와 타겟 문장을 확인해 보았다. 예상대로 소스는 2(**`<start>`**)에서 시작해서 3(**`<end>`**)으로 끝난 후 0(**`<pad>`**)로 채워져 있다. 하지만 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한칸 시프트한 형태를 가지고 있다.\n",
    "\n",
    "마지막으로 우리는 데이터셋 객체를 생성할 것이다. 그동안 우리는 model.fit(x_train, y_train, ...) 형태로 Numpy Array 데이터셋을 생성하여 model에 제공하는 형태의 학습을 많이 진행해 왔다. 그러나 텐서플로우를 활용할 경우 텐서로 생성된 데이터를 이용해 **`tf.data.Dataset`**객체를 생성하는 방법을 흔히 사용한다. **`tf.data.Dataset`**객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의기능을 제공하므로 꼭 사용법을 알아 두기를 권한다. 우리는 이미 데이터셋을 텐서 형태로 생성해 두었으므로, **`tf.data.Dataset.from_tensor_slices()`** 메소드를 이용해 **`tf.data.Dataset`**객체를 생성할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c701e85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b55a9a",
   "metadata": {},
   "source": [
    "데이터셋 생성 과정\n",
    "* 정규표현식을 이용한 corpus 생성\n",
    "* `tf.keras.preprocessing.text.Tokenizer`를 이용해 corpus를 텐서로 변환\n",
    "* `tf.data.Dataset.from_tensor_slices()`를 이용해 corpus 텐서를 `tf.data.Dataset`객체로 변환\n",
    "\n",
    "**`dataset`**을 얻음으로써 데이터 다듬기 과정은 끝났다. tf.data.Dataset에서 제공하는 **`shuffle()`**, **`batch()`** 등 다양한 데이터셋 관련 기능을 손쉽게 이용할 수 있게 되었다.\n",
    "\n",
    "이 모든 일련의 과정을 텐서플로우에서의 __데이터 전처리__ 라 칭한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343cbac",
   "metadata": {},
   "source": [
    "## 5. 실습 (2) 인공지능 학습시키기\n",
    "\n",
    "우리가 인공지능이라고 부르는 것은 인공신경망이자 딥러닝 네트워크이자 이번 코스에선 순환신경망이기도 하고...... 너무 많은 이름이 같은 의미를 담고 있다. 따라서 지금부터 우리가 만들고자 하는 인공지능을 모델(model)이라고 칭하겠다. 실제로도 다들 모델이라고 한다.\n",
    "\n",
    "우리가 만들 모델의 구조도는 아래와 같다.\n",
    "\n",
    "<img src=\"./image/model.png\" />\n",
    "\n",
    "우리가 만들 모델은 tf.keras.Model을 Subclassing하는 방식으로 만들 것이다. 위 그림에서 설명한 것처럼 우리가 만들 모델에는 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다.\n",
    "\n",
    "각 레이어의 기능을 확실히 이해하는 것은 나중에 하고, 지금은 구조도에 설명된 정도의 간단한 이해만 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "066e3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc8ed2",
   "metadata": {},
   "source": [
    "텍스트 분류 모델을 다루어 보셨다면 Embedding 레이어의 역할에 대해서는 낯설지 않을 것이다. 우리 입력 텐서에는 단어 사전의 인덱스가 들어 있다. Embedding 레이어는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다. 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용된다.\n",
    "\n",
    "위 코드에서 **`embedding_size`** 는 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기이다. 만약 그 크기가 2라면 예를 들어\n",
    "\n",
    "* 차갑다: [0.0, 1.0]\n",
    "* 뜨겁다: [1.0, 0.0]\n",
    "* 미지근하다: [0.5, 0.5]\n",
    "\n",
    "정도의 구분이 가능하다. 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만, 그만큼 충분한 데이터가 주어지지 않으면 **오히려 혼란만을 야기**할 수 있다. 이번 실습에서는 256이 적당해 보인다.\n",
    "\n",
    "LSTM 레이어의 hidden state 의 차원수인 **`hidden_size`** 도 같은 맥락이다. **`hidden_size`** 는 모델에 얼마나 많은 일꾼을 둘 것인가? 로 이해해도 크게 엇나가지 않는다. 그 일꾼들은 모두 같은 데이터를 보고 각자의 생각을 가지는데, 역시 충분한 데이터가 주어지면 올바른 결정을 내리겠지만 그렇지 않으면 **배가 산으로 갈 뿐** 이다. 이번 실습에는 **`1024`**가 적당해보인다.\n",
    "\n",
    "---\n",
    "\n",
    "model은 아직 제대로 build되지 않았다. model.compile()을 호출한 적도 없고, 아직 model의 입력 텐서가 무엇인지 제대로 지정해 주지도 않았기 때문이다. <br>\n",
    "그런 경우 아래와 같이 model에 데이터를 아주 조금 태워 보는 것도 방법이다. model의 input shape가 결정되면서 model.build()가 자동으로 호출된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d31a501d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-1.86524630e-04, -9.23087573e-05, -4.21900550e-05, ...,\n",
       "          2.66193703e-04, -2.14425425e-04,  2.81080633e-04],\n",
       "        [-2.88872223e-04, -3.09472933e-04, -1.79014587e-05, ...,\n",
       "          3.52602015e-04, -4.72354732e-04,  4.43382654e-04],\n",
       "        [-6.83938270e-04, -3.76970042e-04,  8.66845294e-05, ...,\n",
       "          8.19710898e-04, -6.22885767e-04,  7.66251818e-04],\n",
       "        ...,\n",
       "        [-1.74506230e-03, -4.47066443e-04, -1.35725946e-03, ...,\n",
       "          2.07451731e-03, -2.67890352e-03,  1.89307588e-03],\n",
       "        [-2.11501704e-03, -7.07514235e-04, -1.71606313e-03, ...,\n",
       "          2.19977484e-03, -3.07550118e-03,  2.72542797e-03],\n",
       "        [-2.45350902e-03, -9.77066113e-04, -2.01623607e-03, ...,\n",
       "          2.29292526e-03, -3.44800157e-03,  3.49156791e-03]],\n",
       "\n",
       "       [[-1.86524630e-04, -9.23087573e-05, -4.21900550e-05, ...,\n",
       "          2.66193703e-04, -2.14425425e-04,  2.81080633e-04],\n",
       "        [-2.07824734e-04, -4.98488953e-04, -1.26760584e-04, ...,\n",
       "          1.41564495e-04, -4.24255768e-06,  5.58265543e-04],\n",
       "        [ 3.05756203e-06, -6.73075265e-04,  1.66950573e-04, ...,\n",
       "          3.92056332e-04,  4.01360856e-04,  1.09491101e-03],\n",
       "        ...,\n",
       "        [-1.98780699e-03, -1.03093346e-03, -2.63464544e-03, ...,\n",
       "          2.29657977e-03, -3.44177126e-03,  4.93217027e-03],\n",
       "        [-2.35033967e-03, -1.31956965e-03, -2.82166153e-03, ...,\n",
       "          2.32612295e-03, -3.79141490e-03,  5.47434716e-03],\n",
       "        [-2.66613532e-03, -1.57917757e-03, -2.96930037e-03, ...,\n",
       "          2.33603059e-03, -4.11127508e-03,  5.93560701e-03]],\n",
       "\n",
       "       [[-1.86524630e-04, -9.23087573e-05, -4.21900550e-05, ...,\n",
       "          2.66193703e-04, -2.14425425e-04,  2.81080633e-04],\n",
       "        [-3.92292888e-04, -4.63302393e-04, -3.41832987e-04, ...,\n",
       "          5.92035125e-04, -6.92265632e-04,  7.67693855e-04],\n",
       "        [-4.41192358e-04, -6.19898085e-04, -4.61286021e-04, ...,\n",
       "          7.69695209e-04, -1.13201758e-03,  8.11478822e-04],\n",
       "        ...,\n",
       "        [-1.12784351e-03, -9.23821121e-04, -2.19980557e-03, ...,\n",
       "          2.11966573e-03, -2.56797089e-03,  3.33075575e-03],\n",
       "        [-1.55296584e-03, -1.23904226e-03, -2.48832651e-03, ...,\n",
       "          2.18281639e-03, -2.99945311e-03,  4.08030907e-03],\n",
       "        [-1.93940708e-03, -1.52710325e-03, -2.71458086e-03, ...,\n",
       "          2.22108699e-03, -3.40509508e-03,  4.73815110e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.86524630e-04, -9.23087573e-05, -4.21900550e-05, ...,\n",
       "          2.66193703e-04, -2.14425425e-04,  2.81080633e-04],\n",
       "        [ 5.63722415e-06, -9.29697781e-05,  7.26605504e-05, ...,\n",
       "          5.40129957e-04, -3.92677786e-04,  4.85772529e-04],\n",
       "        [ 1.64487585e-06, -2.37033411e-04,  5.17321052e-04, ...,\n",
       "          6.42466242e-04,  2.57186857e-05,  3.83284787e-04],\n",
       "        ...,\n",
       "        [-2.17122678e-03, -1.42237288e-03, -2.01113382e-03, ...,\n",
       "          3.28500988e-03, -3.34407296e-03,  5.01029799e-03],\n",
       "        [-2.46763020e-03, -1.63469184e-03, -2.25681416e-03, ...,\n",
       "          3.20111820e-03, -3.74285597e-03,  5.54406736e-03],\n",
       "        [-2.72568851e-03, -1.83225737e-03, -2.45429110e-03, ...,\n",
       "          3.10471421e-03, -4.10678191e-03,  5.99745102e-03]],\n",
       "\n",
       "       [[-1.86524630e-04, -9.23087573e-05, -4.21900550e-05, ...,\n",
       "          2.66193703e-04, -2.14425425e-04,  2.81080633e-04],\n",
       "        [-2.29339385e-05, -3.62397521e-04,  6.73163304e-05, ...,\n",
       "          1.39803553e-04, -2.39379733e-04,  1.27163250e-04],\n",
       "        [ 1.98006455e-04, -4.44672653e-04,  6.76999116e-05, ...,\n",
       "         -4.49965992e-05, -9.71464324e-05,  1.91223051e-04],\n",
       "        ...,\n",
       "        [-3.10872402e-03, -1.49959873e-03, -3.36027145e-03, ...,\n",
       "          2.34995899e-03, -4.52460721e-03,  6.67743804e-03],\n",
       "        [-3.28207761e-03, -1.73369842e-03, -3.39066004e-03, ...,\n",
       "          2.35997583e-03, -4.79390146e-03,  6.94363797e-03],\n",
       "        [-3.42223025e-03, -1.94027426e-03, -3.40177864e-03, ...,\n",
       "          2.36127712e-03, -5.03868051e-03,  7.16527319e-03]],\n",
       "\n",
       "       [[-1.86524630e-04, -9.23087573e-05, -4.21900550e-05, ...,\n",
       "          2.66193703e-04, -2.14425425e-04,  2.81080633e-04],\n",
       "        [ 6.91438690e-05,  1.61438977e-04, -1.08311426e-04, ...,\n",
       "          4.98746114e-04, -5.58598986e-05,  2.72372708e-04],\n",
       "        [ 2.07224090e-04,  4.80623712e-04, -4.47347527e-04, ...,\n",
       "          4.16729075e-04,  2.92018758e-05,  1.57218910e-05],\n",
       "        ...,\n",
       "        [-2.37348140e-03,  1.83047217e-04, -1.97451981e-03, ...,\n",
       "          1.96211855e-03, -3.42252618e-03,  4.42002993e-03],\n",
       "        [-2.65177176e-03, -1.49611631e-04, -2.26074597e-03, ...,\n",
       "          2.09775101e-03, -3.74613306e-03,  5.00887446e-03],\n",
       "        [-2.89556547e-03, -4.81919822e-04, -2.49600224e-03, ...,\n",
       "          2.19000783e-03, -4.04196326e-03,  5.52371098e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a3533",
   "metadata": {},
   "source": [
    "모델의 최종 출력 텐서 shape를 유심히 보면 **`shape=(256, 20, 7001)`**임을 알 수 있다. 7001은 Dense 레이어의 출력 차원수이다. 7001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링해야 하기 때문이다. <br>\n",
    "256은 이전 스텝에서 지정한 배치 사이즈이다. **`dataset.take(1)`**를 통해서 1개의 배치, 즉 256개의 문장 데이터를 가져온 것이다.\n",
    "\n",
    "그렇다면 20은 무엇을 의미할까? 비밀은 바로 **`tf.keras.layers.LSTM(hidden_size, return_sequences=True)`**로 호출한 LSTM 레이어에서 **`return_sequences=True`**이라고 지정한 부분에 있다. 즉, LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미이다. <br>\n",
    "만약 **`return_sequences=False`**였다면 LSTM 레이어는 1개의 벡터만 출력했을 것이다. 그런데 문제는, 우리의 모델은 입력 데이터의 시퀀스 길이가 얼마인지 모른다는 점이다. 모델을 만들면서 알려준 적도 없다. 그럼 20은 언제 알게된 것일까? 그렇다. 데이터를 입력받으면서 비로소 알게 된 것이다. 우리 데이터셋의 max_len이 20으로 맞춰져 있었던 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb020aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2985c",
   "metadata": {},
   "source": [
    "이제 드디어 **`model.summary()`**를 호출할 수 있게 되었다. 그런데 호출해 보니 그동안 많이 보았던 것과는 다른 점이 있다. 우리가 궁금했던 Output Shape를 정확하게 알려주지 않는다. 바로 위에서 설명한 이유 때문이다. 우리의 모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape를 특정할 수 없는 것이다. 하지만 모델의 파라미터 사이즈는 측정된다. 대략 22million 정도 된다. 참고로 서두에 소개했던 GPT-2의 파라미터 사이즈는, 1.5billion이다. 우리 모델의 100배까지는 안되더라도 수십배가 넘는다. GPT-3의 파라미터 사이즈는 GPT-2의 100배이다...\n",
    "\n",
    "---\n",
    "\n",
    "이제 모델이 학습할 준비가 완료되었다. 아래 코드를 실행해 모델을 학습시켜보자!\n",
    "\n",
    "> 학습엔 15분 정도 소요된다.(GPU 환경 기준). 혹시라도 학습에 지나치게 많은 시간이 소요된다면 tf.test.is_gpu_available() 소스를 실행해 텐서플로우가 GPU를 잘 사용하고 있는지 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41093689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 26s 277ms/step - loss: 3.4911\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 25s 269ms/step - loss: 2.8075\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 25s 270ms/step - loss: 2.7143\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 25s 271ms/step - loss: 2.6158\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 25s 270ms/step - loss: 2.5482\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 25s 271ms/step - loss: 2.4963\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 25s 273ms/step - loss: 2.4408\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 26s 278ms/step - loss: 2.3915\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 25s 274ms/step - loss: 2.3446\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 25s 274ms/step - loss: 2.3021\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 26s 274ms/step - loss: 2.2609\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 26s 275ms/step - loss: 2.2178\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 26s 275ms/step - loss: 2.1760\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 26s 274ms/step - loss: 2.1349\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 26s 276ms/step - loss: 2.0956\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 26s 276ms/step - loss: 2.0544\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 26s 276ms/step - loss: 2.0155\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 26s 280ms/step - loss: 1.9752\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 27s 286ms/step - loss: 1.9354\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 1.8958\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 26s 277ms/step - loss: 1.8581\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 26s 275ms/step - loss: 1.8169\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 26s 275ms/step - loss: 1.7761\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 26s 275ms/step - loss: 1.7389\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 26s 275ms/step - loss: 1.7015\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 26s 275ms/step - loss: 1.6596\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 25s 266ms/step - loss: 1.6195\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 13s 142ms/step - loss: 1.5806\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.5416\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.5025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4013c48090>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07899aa",
   "metadata": {},
   "source": [
    "Loss는 모델이 오답을 만들고 있는 정도라고 생각해도 좋다(그렇다고 Loss가 1일 때 99%를 맞추고 있다는 의미는 아니다). 오답률이 감소하고 있으니 **학습이 잘 진행되고 있다** 고 해석할 수 있다.\n",
    "\n",
    "학습이 완료되었다면 이제 모델을 평가해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f584f69e",
   "metadata": {},
   "source": [
    "## 6. 실습 (3) 잘 만들어졌는지 평가하기\n",
    "\n",
    "모델이 작문을 잘하는지 컴퓨터 알고리즘이 평가하는 것은 무리가 있다. 만약에 그게 가능했다면 지금껏 해온 독후감 숙제를 컴퓨터가 채점했을 것이다. 따라서 작문 모델을 평가하는 가장 확실한 방법은 **작문을 시켜보고 직접 평가**하는 것이다. 아래 **`generate_text`** 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1f4d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb6be2b",
   "metadata": {},
   "source": [
    "텍스트를 생성하는 함수 안을 들여다보면 while문이 하나 자리잡고 있는 것을 볼 수 있다. 왜 그럴까요?\n",
    "\n",
    "학습 단계에서 우리는 이런 while 문이 필요없었다. 소스 문장과 타겟 문장이 있었고, 우리는 소스 문장을 모델에 입력해서 나온 결과를 타겟 문장과 직접 비교하면 그만이었다. <br>\n",
    "그러나 텍스트를 실제로 생성해야 하는 시점에서, 우리에게는 2가지가 없다. 하나는 타겟 문장이다. 또하나는 무엇이냐 하면, 소스 문장이다. 생각해 보면 우리는 텍스트 생성 태스크를 위해 테스트 데이터셋을 따로 생성한 적이 없다.\n",
    "\n",
    "generate_text() 함수에서 **`init_sentence`**를 인자로 받고는 있다. 이렇게 받은 인자를 일단 텐서로 만들고 있다. 디폴트로는 **`<start>`** 단어 하나만 받는다.\n",
    "\n",
    "* while의 첫번째 루프에서 test_tensor에 **`<start>`** 하나만 들어갔다고 하자. 우리의 모델이 출력으로 7001개의 단어 중 **`A`**를 골랐다고 하자.\n",
    "* while의 두번째 루프에서 test_tensor에는 **`<start> A`**가 들어간다. 그래서 우리의 모델이 그다음 **`B`**를 골랐다고 하자.\n",
    "* while의 세번째 루프에서 test_tensor에는 **`<start> A B`**가 들어간다. 그래서..... (이하 후략)\n",
    "\n",
    "---\n",
    "\n",
    "그럼 실제로 위 문장 생성 함수를 실행해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf14fbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she is not lolling on a lewd day bed , <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f69f5",
   "metadata": {},
   "source": [
    "제법 멋진 문장을 생성해냈다. 위 함수의 **`init_sentence`** 를 바꿔가며 이런저런 실험을 해보자 단, **`<start>`**를 빼먹지는 않도록 하자. \n",
    "\n",
    "지금은 짧은 한 줄짜리 문장을 생성하지만, 더 공부하면 최고의 작문 모델 GPT-2를 이용해 더욱 멋진 문장 생성 프로젝트를 할 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4e827",
   "metadata": {},
   "source": [
    "## 7. 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203766b6",
   "metadata": {},
   "source": [
    "### **Step 1. 데이터 다운로드**\n",
    "\n",
    "---\n",
    "\n",
    "먼저 아래 링크에서 Song Lyrics 데이터를 다운로드하자. 저장된 파일을 압축 해제한 후, 모든 **`txt`** 파일을 **`lyrics`** 폴더를 만들어 그 속에 저장하자.\n",
    "\n",
    "\n",
    "* [Song Lyrics](https://www.kaggle.com/paultimothymooney/poetry/data)\n",
    "\n",
    "아래의 명령어를 실행해도 된다.\n",
    "\n",
    "```bash\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "$ unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics 폴더에 압축풀기\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1dae5",
   "metadata": {},
   "source": [
    "### **Step 2. 데이터 읽어오기**\n",
    "\n",
    "---\n",
    "\n",
    "**`glob`** 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이하다. **`glob`** 를 활용하여 모든 **`txt`** 파일을 읽어온 후, **`raw_corpus`** 리스트에 문장 단위로 저장하도록 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093636a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d6bac",
   "metadata": {},
   "source": [
    "### **Step 3. 데이터 정제**\n",
    "\n",
    "---\n",
    "\n",
    "**앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하자!**\n",
    "\n",
    "**`preprocess_sentence()`** 함수를 만든 것을 기억할 것이다. 이를 활용해 데이터를 정제하도록 하겠다.\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거한다. 너무 긴 문장은 노래가사 작사하기에 어울리지 않을수도 있다.<br> \n",
    "그래서 이번에는 문장을 **토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기**를 권한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae56b1a",
   "metadata": {},
   "source": [
    "### **Step 4. 평가 데이터셋 분리**\n",
    "\n",
    "---\n",
    "\n",
    "**훈련 데이터와 평가 데이터를 분리하자!**\n",
    "\n",
    "**`tokenize()`** 함수로 데이터를 Tensor로 변환한 후, **`sklearn`** 모듈의 **`train_test_split()`** 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠다. **단어장의 크기는 12,000 이상**으로 설정하자! **총 데이터의 20%**를 평가 데이터셋으로 사용하자!\n",
    "\n",
    "```python\n",
    "enc_train, enc_val, dec_train, dec_val = <코드 작성>\n",
    "```\n",
    "\n",
    "여기까지 올바르게 진행했을 경우, 아래 실행 결과를 확인할 수 있다.\n",
    "\n",
    "```python\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "```\n",
    "\n",
    "```python\n",
    "out:\n",
    "\n",
    "Source Train: (124960, 14)\n",
    "Target Train: (124960, 14)\n",
    "```\n",
    "\n",
    "만약 결과가 다르다면 천천히 과정을 다시 살펴 동일한 결과를 얻도록 하자! 만약 학습데이터 갯수가 124960보다 크다면 위 Step 3.의 데이터 정제 과정을 다시한번 검토해 보시기를 권한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85cf86",
   "metadata": {},
   "source": [
    "### **Step 5. 인공지능 만들기**\n",
    "\n",
    "---\n",
    "\n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 **`val_loss`** 값을 2.2 수준으로 줄일 수 있는 모델을 설계하자! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
    "\n",
    "```python\n",
    "#Loss\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "```\n",
    "\n",
    "데이터가 커서 훈련하는 데 시간이 제법 걸릴 것이다.\n",
    "\n",
    "```python\n",
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
